# Lesson 2: Matrices

## Introduction

Matrices are 2D arrays that represent collections of vectors, transformations, and entire datasets. Understanding matrices is crucial for machine learning since your data is stored as matrices and models perform matrix operations.

## What is a Matrix?

A **matrix** is a rectangular array of numbers arranged in rows and columns:

$$A = \begin{bmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23}
\end{bmatrix}$$

This is a $2 \times 3$ matrix (2 rows, 3 columns).

### In Machine Learning

- **Each row**: A data sample (observation)
- **Each column**: A feature
- **Entire matrix**: Your dataset!

**Example:** A dataset of 3 houses with 4 features:

$$X = \begin{bmatrix}
1500 & 3 & 2 & 2005 \\
2000 & 4 & 3 & 2010 \\
1200 & 2 & 1 & 1995
\end{bmatrix}$$

Rows = houses, Columns = [sqft, bedrooms, bathrooms, year_built]

## Creating Matrices in NumPy

```python
import numpy as np

# Create a matrix
A = np.array([[1, 2, 3],
              [4, 5, 6]])
print(A.shape)  # (2, 3) - 2 rows, 3 columns

# Special matrices
zeros = np.zeros((3, 4))       # 3x4 matrix of zeros
ones = np.ones((2, 2))         # 2x2 matrix of ones
identity = np.eye(3)           # 3x3 identity matrix
random = np.random.rand(2, 3)  # 2x3 random matrix

# From a dataset
data = [[1500, 3, 2],
        [2000, 4, 3],
        [1200, 2, 1]]
X = np.array(data)
```

## Types of Matrices

### Square Matrix
Equal number of rows and columns ($n \times n$):

$$A = \begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}$$

### Identity Matrix
Square matrix with 1s on diagonal, 0s elsewhere:

$$I = \begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}$$

```python
I = np.eye(3)
```

**Property:** $A \cdot I = I \cdot A = A$ (identity for multiplication)

### Diagonal Matrix
Non-zero elements only on the diagonal:

$$D = \begin{bmatrix}
d_1 & 0 & 0 \\
0 & d_2 & 0 \\
0 & 0 & d_3
\end{bmatrix}$$

```python
D = np.diag([1, 2, 3])
```

### Symmetric Matrix
Equal to its transpose ($A = A^T$):

$$A = \begin{bmatrix}
1 & 2 & 3 \\
2 & 5 & 6 \\
3 & 6 & 9
\end{bmatrix}$$

**ML Application:** Covariance matrices are symmetric.

### Sparse Matrix
Most elements are zero:

$$S = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 0 & 2 & 0 \\
0 & 0 & 0 & 3
\end{bmatrix}$$

**ML Application:** Text data (document-term matrices) are often sparse.

## Matrix Operations

### Transpose

Flip rows and columns:

$$A = \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix} \quad \Rightarrow \quad A^T = \begin{bmatrix}
1 & 4 \\
2 & 5 \\
3 & 6
\end{bmatrix}$$

```python
A = np.array([[1, 2, 3],
              [4, 5, 6]])
A_T = A.T  # or np.transpose(A)
```

**Properties:**
- $(A^T)^T = A$
- $(A + B)^T = A^T + B^T$
- $(AB)^T = B^T A^T$

### Addition and Subtraction

Element-wise operations (matrices must have same shape):

$$A + B = \begin{bmatrix}
a_{11} + b_{11} & a_{12} + b_{12} \\
a_{21} + b_{21} & a_{22} + b_{22}
\end{bmatrix}$$

```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = A + B  # [[6, 8], [10, 12]]
```

### Scalar Multiplication

Multiply every element by a scalar:

$$c \cdot A = \begin{bmatrix}
c \cdot a_{11} & c \cdot a_{12} \\
c \cdot a_{21} & c \cdot a_{22}
\end{bmatrix}$$

```python
A = np.array([[1, 2], [3, 4]])
B = 2 * A  # [[2, 4], [6, 8]]
```

## Matrix-Vector Multiplication

This is fundamental to understanding how ML models make predictions!

$$A\vec{x} = \begin{bmatrix}
a_{11} & a_{12} \\
a_{21} & a_{22}
\end{bmatrix} \begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} = \begin{bmatrix}
a_{11}x_1 + a_{12}x_2 \\
a_{21}x_1 + a_{22}x_2
\end{bmatrix}$$

Each element of the result is the **dot product** of a matrix row with the vector.

```python
A = np.array([[1, 2],
              [3, 4]])
x = np.array([5, 6])

y = A @ x  # [17, 39]
# or
y = np.dot(A, x)

# Manual computation:
# y[0] = 1*5 + 2*6 = 17
# y[1] = 3*5 + 4*6 = 39
```

**ML Application:** Linear model prediction:

$$\hat{y} = Wx + b$$

where $W$ is a weight matrix, $x$ is input features, $b$ is bias.

## Matrix-Matrix Multiplication

Multiplying two matrices combines their transformations:

$$C = AB$$

where $C_{ij} = \sum_{k} A_{ik} B_{kj}$

**Rule:** For $A_{m \times n}$ and $B_{n \times p}$, result is $C_{m \times p}$

!!! warning "Dimension Compatibility"
    Number of columns in $A$ must equal number of rows in $B$!

```python
A = np.array([[1, 2],
              [3, 4]])  # 2x2
B = np.array([[5, 6],
              [7, 8]])  # 2x2

C = A @ B  # 2x2
# [[19, 22],
#  [43, 50]]

# Element-by-element:
# C[0,0] = 1*5 + 2*7 = 19
# C[0,1] = 1*6 + 2*8 = 22
# C[1,0] = 3*5 + 4*7 = 43
# C[1,1] = 3*6 + 4*8 = 50
```

**Properties:**
- **Not commutative:** $AB \neq BA$ (usually)
- **Associative:** $(AB)C = A(BC)$
- **Distributive:** $A(B + C) = AB + AC$

**ML Application:** Stacking neural network layers:

$$output = W_3(W_2(W_1 x))$$

## Practical Example: Dataset Operations

```python
import numpy as np

# Dataset: 4 samples, 3 features
X = np.array([[1, 2, 3],
              [4, 5, 6],
              [7, 8, 9],
              [10, 11, 12]])

print(f"Dataset shape: {X.shape}")  # (4, 3)

# Mean of each feature (column)
feature_means = X.mean(axis=0)  # [5.5, 6.5, 7.5]

# Mean of each sample (row)
sample_means = X.mean(axis=1)   # [2, 5, 8, 11]

# Center the data (subtract mean)
X_centered = X - feature_means

# Get specific samples and features
first_sample = X[0, :]      # [1, 2, 3]
second_feature = X[:, 1]    # [2, 5, 8, 11]
subset = X[0:2, 1:3]        # [[2, 3], [5, 6]]
```

## Visualization

Matrices can transform vectors:

```python
import matplotlib.pyplot as plt
import numpy as np

# Original vector
v = np.array([1, 1])

# Transformation matrices
rotation = np.array([[0, -1],
                      [1, 0]])  # 90Â° rotation
scaling = np.array([[2, 0],
                     [0, 0.5]])  # Scale x by 2, y by 0.5
shear = np.array([[1, 0.5],
                   [0, 1]])  # Shear transformation

# Apply transformations
v_rot = rotation @ v
v_scale = scaling @ v
v_shear = shear @ v

# Plot
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

for ax, v_transformed, title in zip(axes,
                                     [v_rot, v_scale, v_shear],
                                     ['Rotation', 'Scaling', 'Shear']):
    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,
              color='blue', width=0.01, label='Original')
    ax.quiver(0, 0, v_transformed[0], v_transformed[1],
              angles='xy', scale_units='xy', scale=1,
              color='red', width=0.01, label='Transformed')
    ax.set_xlim(-2, 3)
    ax.set_ylim(-2, 3)
    ax.set_aspect('equal')
    ax.grid(True)
    ax.legend()
    ax.set_title(title)

plt.tight_layout()
plt.show()
```

## Key Takeaways

!!! success "Important Concepts"
    - Matrices represent datasets (rows = samples, columns = features)
    - Transpose flips rows and columns
    - Matrix multiplication combines transformations
    - Matrix-vector product is how models make predictions
    - Matrix dimensions must be compatible for multiplication

## Next Steps

[Next: Lesson 3 - Advanced Matrix Operations](03-matrix-operations.md){ .md-button .md-button--primary }

[Back: Lesson 1 - Vectors](01-vectors.md){ .md-button }

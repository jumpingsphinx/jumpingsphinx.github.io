{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to ML101","text":""},{"location":"#machine-learning-fundamentals","title":"Machine Learning Fundamentals","text":"<p>ML101 is a comprehensive, fully interactive course designed to teach you machine learning from the ground up. Whether you're a beginner or looking to solidify your understanding of ML fundamentals, this course provides a structured path through essential concepts, algorithms, and implementations.</p>"},{"location":"#learn-entirely-in-your-browser","title":"\u2728 Learn Entirely in Your Browser","text":"<p>Start coding immediately - no installation required!</p> <p>ML101 offers a unique learning experience:</p> <ul> <li>\ud83d\ude80 Interactive Code Examples: Run Python code directly on this website - click \"\u25b6 Run Code\" to execute</li> <li>\u2601\ufe0f Cloud-Based Exercises: One-click access to Google Colab for hands-on practice</li> <li>\ud83d\udcbb Zero Setup: Works on any device - Windows, Mac, Linux, even Chromebooks</li> <li>\ud83c\udf93 Progressive Learning: From guided examples to open-ended challenges</li> </ul> <p>Try It Now!</p> <p>Here's a taste of what you'll learn. Click the \u25b6 Run Code button below to execute Python code right in your browser:</p> <pre><code>import numpy as np\n\n# Machine Learning in action!\ndata = np.array([1, 2, 3, 4, 5])\nmean = np.mean(data)\nstd = np.std(data)\n\nprint(f\"Data: {data}\")\nprint(f\"Mean: {mean}\")\nprint(f\"Standard Deviation: {std:.2f}\")\nprint(\"\\n\u2713 This is real Python running in your browser!\")\n</code></pre> <p>No installation. No configuration. Just learning.</p>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>This course is organized into four progressive modules, each building on the previous one:</p>"},{"location":"#module-1-linear-algebra-basics","title":"Module 1: Linear Algebra Basics","text":"<p>Foundation for Understanding ML</p> <p>Linear algebra is the mathematical foundation of machine learning. In this module, you'll learn:</p> <ul> <li>Vector operations and their geometric interpretations</li> <li>Matrix operations and transformations</li> <li>Eigenvalues, eigenvectors, and matrix decompositions</li> <li>Principal Component Analysis (PCA) and dimensionality reduction</li> </ul> <p>Time Commitment: 4-6 hours</p> <p>Start Module 1</p>"},{"location":"#module-2-regression-algorithms","title":"Module 2: Regression Algorithms","text":"<p>Predicting Continuous Values</p> <p>Build your first machine learning models with regression techniques:</p> <ul> <li>Linear regression from scratch and with scikit-learn</li> <li>Gradient descent and optimization strategies</li> <li>Logistic regression for classification problems</li> <li>Regularization techniques (L1, L2, Elastic Net)</li> </ul> <p>Time Commitment: 6-8 hours</p> <p>Start Module 2</p>"},{"location":"#module-3-tree-based-algorithms","title":"Module 3: Tree-Based Algorithms","text":"<p>Decision Trees and Ensemble Methods</p> <p>Master powerful tree-based algorithms used in industry:</p> <ul> <li>Decision tree fundamentals and splitting criteria</li> <li>Random Forest for robust predictions</li> <li>Gradient boosting and its variants</li> <li>XGBoost for high-performance ML</li> </ul> <p>Time Commitment: 6-8 hours</p> <p>Start Module 3</p>"},{"location":"#module-4-neural-networks","title":"Module 4: Neural Networks","text":"<p>Deep Learning Fundamentals</p> <p>Build neural networks from scratch and with modern frameworks:</p> <ul> <li>Perceptron and activation functions</li> <li>Feedforward networks and backpropagation</li> <li>NumPy implementation to understand internals</li> <li>PyTorch for modern deep learning applications</li> </ul> <p>Time Commitment: 8-10 hours</p> <p>Start Module 4</p>"},{"location":"#how-this-course-works","title":"How This Course Works","text":""},{"location":"#1-interactive-lessons","title":"1. Interactive Lessons","text":"<p>Each module contains detailed lesson pages with theory, mathematical explanations, and runnable code examples. Click the \"\u25b6 Run Code\" button to execute Python directly in your browser. No setup required!</p>"},{"location":"#2-cloud-based-exercises","title":"2. Cloud-Based Exercises","text":"<p>Every module includes hands-on Jupyter notebook exercises. Click the \"Open in Colab\" badge to launch exercises in Google Colab - a free cloud environment with all libraries pre-installed. Just sign in with your Google account and start coding!</p>"},{"location":"#3-review-solutions","title":"3. Review Solutions","text":"<p>After attempting exercises, review the solution notebooks. These include not just the code, but explanations of the approach and alternative solutions. Solutions are also available in Colab with one click.</p>"},{"location":"#4-experiment-and-explore","title":"4. Experiment and Explore","text":"<p>Modify parameters, try different datasets, and experiment with variations. All in your browser - save your work to Google Drive and access it anywhere.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":""},{"location":"#required-knowledge","title":"Required Knowledge","text":"<ul> <li>Python programming: Comfortable with Python syntax, functions, and basic data structures</li> <li>Basic mathematics: High school algebra and basic calculus concepts</li> </ul>"},{"location":"#technical-requirements","title":"Technical Requirements","text":"<p>For Browser-Only Learning (Path 1): - \u2705 A modern web browser (Chrome, Firefox, Safari, Edge) - \u2705 Internet connection - \u2705 Google account (free) for saving Colab notebooks</p> <p>For Local Development (Paths 2 &amp; 3): - Python 3.9 or higher - 4GB RAM minimum - Code editor or IDE (VS Code, PyCharm, or Jupyter Lab)</p> <p>New to Python?</p> <p>Check out our Python Refresher before starting. You can run all the examples directly in your browser!</p>"},{"location":"#course-philosophy","title":"Course Philosophy","text":""},{"location":"#learn-by-doing","title":"Learn by Doing","text":"<p>Theory is important, but implementation cements understanding. You'll write code for nearly every concept you learn.</p>"},{"location":"#build-from-scratch","title":"Build from Scratch","text":"<p>Before using libraries, you'll implement algorithms from scratch using NumPy. This builds deep understanding of how things work under the hood.</p>"},{"location":"#use-modern-tools","title":"Use Modern Tools","text":"<p>After understanding the fundamentals, you'll use industry-standard libraries like scikit-learn and PyTorch.</p>"},{"location":"#progressive-complexity","title":"Progressive Complexity","text":"<p>Start with heavy guidance and clear instructions. As you progress, exercises become more open-ended, preparing you for real-world problems.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to begin your machine learning journey?</p> <ol> <li>Set up your environment</li> <li>Review the learning path</li> <li>Start with Module 1: Linear Algebra</li> </ol> <p>Get Started</p>"},{"location":"#need-help","title":"Need Help?","text":"<ul> <li>Questions about content? Open an issue on GitHub</li> <li>Found a bug? Submit a pull request or issue</li> <li>Want to contribute? See our contribution guidelines</li> </ul>"},{"location":"#license","title":"License","text":"<p>This course is open source and available under the MIT License. Feel free to use, share, and adapt the materials.</p> <p>Happy Learning!</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide will help you set up your environment and get ready to start learning machine learning fundamentals.</p>"},{"location":"getting-started/#choose-your-learning-path","title":"Choose Your Learning Path","text":"<p>ML101 offers three ways to learn, designed for different needs and skill levels:</p>"},{"location":"getting-started/#path-1-browser-only-recommended-for-beginners","title":"\ud83c\udf10 Path 1: Browser-Only (Recommended for Beginners)","text":"<p>Best for: Complete beginners, quick start, no installation hassles</p> <p>What you get: - \u2705 Start learning immediately - zero setup required - \u2705 Run Python code directly in your browser - \u2705 Use Google Colab for exercises (free cloud notebooks with GPU) - \u2705 Works on any device (Windows, Mac, Linux, even Chromebooks!)</p> <p>How it works: 1. Browse the lessons on this website 2. Click \"\u25b6 Run Code\" buttons to execute examples in your browser 3. Click \"Open in Colab\" badges in exercises to complete them in the cloud 4. No installation, no configuration needed!</p> <p>Start Learning Now</p>"},{"location":"getting-started/#path-2-hybrid-balance-of-convenience-and-control","title":"\ud83d\udcbb Path 2: Hybrid (Balance of Convenience and Control)","text":"<p>Best for: Learners who want both browser interactivity and local development</p> <p>What you get: - \u2705 Interactive code examples in browser (like Path 1) - \u2705 Local Jupyter Lab for deeper exploration - \u2705 Full control over your environment - \u2705 Work offline once set up</p> <p>Setup: Follow the Local Installation instructions below.</p>"},{"location":"getting-started/#path-3-fully-local-maximum-control","title":"\ud83d\udd27 Path 3: Fully Local (Maximum Control)","text":"<p>Best for: Advanced users, offline work, custom environments</p> <p>What you get: - \u2705 Complete local development environment - \u2705 No dependency on cloud services - \u2705 Full customization of tools and libraries - \u2705 Best performance for large experiments</p> <p>Setup: Follow the Local Installation instructions below.</p> <p>Not sure which path to choose?</p> <p>Start with Path 1 (Browser-Only)! You can always switch to local development later. Many learners complete the entire course using just their browser and Colab.</p>"},{"location":"getting-started/#local-installation-paths-2-3","title":"Local Installation (Paths 2 &amp; 3)","text":"<p>If you chose Path 2 (Hybrid) or Path 3 (Fully Local), follow these installation steps:</p>"},{"location":"getting-started/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>Operating System: Windows 10/11, macOS 10.14+, or Linux</li> <li>Python: Version 3.9 or higher</li> <li>RAM: 4GB minimum (8GB recommended)</li> <li>Disk Space: 2GB for dependencies and datasets</li> </ul>"},{"location":"getting-started/#recommended-setup","title":"Recommended Setup","text":"<ul> <li>RAM: 8GB or more for faster computations</li> <li>Python: Version 3.10 or 3.11 for best compatibility</li> <li>GPU: Not required, but helpful for Module 4 (Neural Networks)</li> </ul>"},{"location":"getting-started/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/#step-1-verify-python-installation","title":"Step 1: Verify Python Installation","text":"<p>First, check if Python is installed and which version you have:</p> <pre><code>python --version\n</code></pre> <p>or</p> <pre><code>python3 --version\n</code></pre> <p>You should see Python 3.9 or higher. If not, download Python.</p> <p>Python Version</p> <p>Make sure you have Python 3.9 or higher. Older versions may not be compatible with all dependencies.</p>"},{"location":"getting-started/#step-2-clone-the-repository","title":"Step 2: Clone the Repository","text":"<p>Clone the ML101 repository to your local machine:</p> <pre><code>git clone https://github.com/jumpingsphinx/ML101.git\ncd ML101\n</code></pre> <p>If you don't have Git installed, you can download it here or download the repository as a ZIP file from GitHub.</p>"},{"location":"getting-started/#step-3-create-a-virtual-environment","title":"Step 3: Create a Virtual Environment","text":"<p>Creating a virtual environment keeps your ML101 dependencies isolated from other Python projects.</p> <p>On Windows: <pre><code>python -m venv venv\nvenv\\Scripts\\activate\n</code></pre></p> <p>On macOS/Linux: <pre><code>python3 -m venv venv\nsource venv/bin/activate\n</code></pre></p> <p>You should see <code>(venv)</code> appear at the beginning of your command prompt, indicating the virtual environment is active.</p> <p>Virtual Environments</p> <p>Always activate your virtual environment before working on ML101 projects. This ensures you're using the correct package versions.</p>"},{"location":"getting-started/#step-4-install-dependencies","title":"Step 4: Install Dependencies","text":"<p>With your virtual environment activated, install all required packages:</p> <pre><code>pip install --upgrade pip\npip install -r requirements.txt\n</code></pre> <p>This will install: - NumPy, SciPy, and Pandas for numerical computing - scikit-learn for machine learning algorithms - PyTorch for deep learning (Module 4) - Matplotlib and Seaborn for visualization - Jupyter Lab for interactive notebooks</p> <p>The installation may take 5-10 minutes depending on your internet connection.</p>"},{"location":"getting-started/#step-5-verify-installation","title":"Step 5: Verify Installation","text":"<p>Let's verify everything is installed correctly:</p> <pre><code>python -c \"import numpy, pandas, sklearn, torch, matplotlib; print('All packages installed successfully!')\"\n</code></pre> <p>If you see \"All packages installed successfully!\" you're ready to go!</p>"},{"location":"getting-started/#step-6-launch-jupyter-lab","title":"Step 6: Launch Jupyter Lab","text":"<p>Start Jupyter Lab to access the interactive notebooks:</p> <pre><code>jupyter lab\n</code></pre> <p>This will open Jupyter Lab in your web browser (usually at <code>http://localhost:8888</code>).</p> <p>First Time Using Jupyter?</p> <p>Jupyter Lab is an interactive development environment for notebooks. You can write code, see results immediately, and mix code with explanatory text.</p>"},{"location":"getting-started/#navigating-the-repository","title":"Navigating the Repository","text":"<p>Once Jupyter Lab is open, you'll see the file browser. Here's what each folder contains:</p> <pre><code>ML101/\n\u251c\u2500\u2500 notebooks/              \u2190 Your exercises are here!\n\u2502   \u251c\u2500\u2500 module1-linear-algebra/\n\u2502   \u251c\u2500\u2500 module2-regression/\n\u2502   \u251c\u2500\u2500 module3-trees/\n\u2502   \u2514\u2500\u2500 module4-neural-networks/\n\u251c\u2500\u2500 docs/                   \u2190 Documentation source files\n\u251c\u2500\u2500 src/                    \u2190 Utility functions (optional)\n\u2514\u2500\u2500 tests/                  \u2190 Tests for utilities\n</code></pre>"},{"location":"getting-started/#your-learning-workflow","title":"Your Learning Workflow","text":"<p>Here's the recommended way to work through each module:</p> <ol> <li> <p>Read the Lesson: Visit the documentation page for the lesson you're working on (this site!)</p> </li> <li> <p>Open the Exercise Notebook:</p> </li> <li>Navigate to <code>notebooks/module-X/</code> in Jupyter Lab</li> <li> <p>Open the exercise notebook (e.g., <code>exercise1-vectors.ipynb</code>)</p> </li> <li> <p>Complete the Exercises:</p> </li> <li>Read the instructions carefully</li> <li>Write code in the provided cells</li> <li> <p>Run cells to see results (Shift + Enter)</p> </li> <li> <p>Check Your Work:</p> </li> <li>Try to complete exercises without looking at solutions</li> <li>Use the hints if you get stuck</li> <li> <p>Verify your output matches expected results</p> </li> <li> <p>Review Solutions:</p> </li> <li>Navigate to the <code>solutions/</code> subfolder</li> <li>Compare your approach with the solution</li> <li> <p>Understand alternative methods</p> </li> <li> <p>Experiment:</p> </li> <li>Modify parameters and see what happens</li> <li>Try the code on different inputs</li> <li>Answer the reflection questions</li> </ol>"},{"location":"getting-started/#jupyter-lab-quick-reference","title":"Jupyter Lab Quick Reference","text":""},{"location":"getting-started/#essential-keyboard-shortcuts","title":"Essential Keyboard Shortcuts","text":"Action Shortcut Run cell and advance <code>Shift + Enter</code> Run cell in place <code>Ctrl + Enter</code> Insert cell above <code>A</code> (in command mode) Insert cell below <code>B</code> (in command mode) Delete cell <code>DD</code> (press D twice, in command mode) Undo cell deletion <code>Z</code> (in command mode) Change to markdown <code>M</code> (in command mode) Change to code <code>Y</code> (in command mode)"},{"location":"getting-started/#command-mode-vs-edit-mode","title":"Command Mode vs Edit Mode","text":"<ul> <li>Edit Mode: Press <code>Enter</code> to edit a cell's content</li> <li>Command Mode: Press <code>Esc</code> to navigate between cells</li> </ul>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/#import-errors","title":"Import Errors","text":"<p>If you get \"ModuleNotFoundError\": <pre><code># Make sure virtual environment is activated\n# Re-run pip install\npip install -r requirements.txt\n</code></pre></p>"},{"location":"getting-started/#jupyter-lab-wont-start","title":"Jupyter Lab Won't Start","text":"<p>Try: <pre><code>pip install --upgrade jupyterlab\njupyter lab --port=8889  # Try a different port\n</code></pre></p>"},{"location":"getting-started/#pytorch-installation-issues","title":"PyTorch Installation Issues","text":"<p>If PyTorch doesn't install properly:</p> <p>CPU-only version (smaller, faster download): <pre><code>pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p> <p>Check PyTorch website for your specific platform.</p>"},{"location":"getting-started/#memory-issues","title":"Memory Issues","text":"<p>If you encounter memory errors: - Close other applications - Restart Jupyter Lab - Work with smaller datasets initially - Consider using Google Colab (free cloud notebooks)</p>"},{"location":"getting-started/#using-google-colab-path-1-2","title":"Using Google Colab (Path 1 &amp; 2)","text":"<p>Google Colab provides free cloud notebooks with GPU access - perfect for running exercises without local installation.</p>"},{"location":"getting-started/#quick-start-with-colab","title":"Quick Start with Colab","text":"<p>Every exercise notebook in ML101 has a \"Open in Colab\" badge at the top. Just click it!</p> <p></p> <p>Benefits: - \u2705 No installation required - \u2705 Free GPU access (helpful for Module 4) - \u2705 Pre-installed ML libraries (NumPy, pandas, scikit-learn, matplotlib) - \u2705 Save your work to Google Drive - \u2705 Share notebooks easily</p> <p>Additional Setup (if needed):</p> <p>Some exercises may require XGBoost (Module 3). Install it in a Colab cell: <pre><code>!pip install xgboost\n</code></pre></p> <p>Colab Tip</p> <p>Sign in with your Google account to save your work. Your notebooks will be saved in Google Drive under \"Colab Notebooks\".</p>"},{"location":"getting-started/#development-dependencies-optional","title":"Development Dependencies (Optional)","text":"<p>If you want to build the documentation locally or contribute to the project:</p> <pre><code>pip install -r requirements-dev.txt\n</code></pre> <p>This installs MkDocs and development tools. You can then build the documentation:</p> <pre><code>mkdocs serve\n</code></pre> <p>Visit <code>http://127.0.0.1:8000</code> to see the documentation site locally.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<p>Now that you're set up, it's time to start learning!</p> <ol> <li>Review the Learning Path for tips on how to approach the course</li> <li>Start with Module 1: Linear Algebra Basics</li> <li>Open your first exercise notebook in Jupyter Lab</li> </ol> <p>View Learning Path Start Module 1</p> <p>Having issues? Open an issue on GitHub and we'll help you out!</p>"},{"location":"learning-path/","title":"Learning Path","text":"<p>This guide provides recommendations for how to approach ML101 to get the most out of your learning experience.</p>"},{"location":"learning-path/#recommended-study-sequence","title":"Recommended Study Sequence","text":""},{"location":"learning-path/#follow-the-module-order","title":"Follow the Module Order","text":"<p>The modules are designed to build on each other:</p> <pre><code>graph LR\n    A[Module 1&lt;br/&gt;Linear Algebra] --&gt; B[Module 2&lt;br/&gt;Regression]\n    B --&gt; C[Module 3&lt;br/&gt;Trees]\n    C --&gt; D[Module 4&lt;br/&gt;Neural Networks]\n</code></pre> <p>Sequential Learning</p> <p>Each module assumes knowledge from previous modules. Skipping ahead may result in missing important foundational concepts.</p>"},{"location":"learning-path/#study-approach","title":"Study Approach","text":""},{"location":"learning-path/#for-each-module","title":"For Each Module","text":""},{"location":"learning-path/#1-read-the-module-overview-15-minutes","title":"1. Read the Module Overview (15 minutes)","text":"<ul> <li>Understand what you'll learn</li> <li>Review prerequisites</li> <li>Set expectations for time commitment</li> </ul>"},{"location":"learning-path/#2-complete-each-lesson-30-45-minutes-per-lesson","title":"2. Complete Each Lesson (30-45 minutes per lesson)","text":"<ul> <li>Read the lesson carefully on this documentation site</li> <li>Take notes on key concepts</li> <li>Run any inline code examples</li> <li>Ensure you understand before moving on</li> </ul>"},{"location":"learning-path/#3-work-through-exercises-1-2-hours-per-exercise","title":"3. Work Through Exercises (1-2 hours per exercise)","text":"<ul> <li>Open the exercise notebook in Jupyter Lab</li> <li>Attempt exercises independently first</li> <li>Use hints only when stuck</li> <li>Don't rush - understanding is more important than speed</li> </ul>"},{"location":"learning-path/#4-review-solutions-30-minutes-per-exercise","title":"4. Review Solutions (30 minutes per exercise)","text":"<ul> <li>Compare your solution with the provided one</li> <li>Note any differences in approach</li> <li>Understand why the solution works</li> <li>Learn alternative methods</li> </ul>"},{"location":"learning-path/#5-experiment-and-explore-30-60-minutes","title":"5. Experiment and Explore (30-60 minutes)","text":"<ul> <li>Modify parameters and observe effects</li> <li>Try different inputs</li> <li>Test edge cases</li> <li>Answer reflection questions</li> </ul>"},{"location":"learning-path/#6-take-breaks","title":"6. Take Breaks","text":"<ul> <li>Step away after completing an exercise</li> <li>Let concepts sink in</li> <li>Return refreshed for the next topic</li> </ul>"},{"location":"learning-path/#time-commitment","title":"Time Commitment","text":""},{"location":"learning-path/#estimated-time-per-module","title":"Estimated Time per Module","text":"Module Lessons Exercises Total Time Module 1: Linear Algebra 2-3 hours 2-3 hours 4-6 hours Module 2: Regression 3-4 hours 3-4 hours 6-8 hours Module 3: Trees 3-4 hours 3-4 hours 6-8 hours Module 4: Neural Networks 4-5 hours 4-5 hours 8-10 hours <p>Total Course: 24-32 hours</p>"},{"location":"learning-path/#recommended-pace","title":"Recommended Pace","text":"<p>Choose a pace that works for you:</p> <p>Intensive (2-3 weeks): - Study 10-15 hours per week - Complete 1-2 lessons per day - Full module per week</p> <p>Moderate (1-2 months): - Study 5-8 hours per week - Complete 2-3 lessons per week - One module every 2 weeks</p> <p>Relaxed (2-3 months): - Study 2-4 hours per week - Complete 1 lesson per week - One module per month</p> <p>Quality Over Speed</p> <p>It's better to deeply understand one concept than to rush through many. Take your time!</p>"},{"location":"learning-path/#learning-strategies","title":"Learning Strategies","text":""},{"location":"learning-path/#active-learning","title":"Active Learning","text":"<p>Don't Just Read - Do: - Type out code examples (don't copy-paste) - Predict output before running cells - Explain concepts aloud or in writing - Create your own examples</p>"},{"location":"learning-path/#incremental-understanding","title":"Incremental Understanding","text":"<p>Build Layer by Layer: - Master basics before advanced topics - Revisit earlier material when needed - Connect new concepts to what you know - Look for patterns across modules</p>"},{"location":"learning-path/#practical-application","title":"Practical Application","text":"<p>Think About Real Use Cases: - How would you use this at work? - What problems could this solve? - Where have you seen this in action? - Can you apply this to your own data?</p>"},{"location":"learning-path/#tips-for-success","title":"Tips for Success","text":""},{"location":"learning-path/#when-youre-stuck","title":"When You're Stuck","text":"<ol> <li>Re-read the lesson - The answer is often there</li> <li>Check the hints - They provide helpful direction</li> <li>Search online - Stack Overflow, documentation, tutorials</li> <li>Take a break - Fresh perspective often helps</li> <li>Review prerequisites - Make sure fundamentals are solid</li> <li>Ask for help - Open an issue on GitHub</li> </ol>"},{"location":"learning-path/#effective-note-taking","title":"Effective Note-Taking","text":"<p>Keep a learning journal: - Key concepts: What are the main ideas? - Examples: Concrete instances of concepts - Questions: Things you don't understand yet - Applications: Where could you use this? - Connections: How does this relate to other topics?</p>"},{"location":"learning-path/#code-organization","title":"Code Organization","text":"<p>As you work through exercises: - Comment your code: Explain your reasoning - Use meaningful variable names: <code>learning_rate</code> not <code>lr</code> - Test incrementally: Run cells frequently - Save your work: Notebooks save automatically, but save manually too - Experiment in copies: Duplicate notebooks before major changes</p>"},{"location":"learning-path/#dealing-with-challenges","title":"Dealing with Challenges","text":""},{"location":"learning-path/#concept-is-confusing","title":"Concept Is Confusing","text":"<ul> <li>Break it into smaller pieces</li> <li>Draw diagrams or flowcharts</li> <li>Find alternative explanations online</li> <li>Implement a simple version first</li> <li>Discuss with others (forums, study groups)</li> </ul>"},{"location":"learning-path/#math-is-intimidating","title":"Math Is Intimidating","text":"<ul> <li>Focus on intuition first, formulas second</li> <li>Use visualizations</li> <li>Relate to concrete examples</li> <li>Check the Math Primer</li> <li>Remember: understanding &gt; memorization</li> </ul>"},{"location":"learning-path/#code-isnt-working","title":"Code Isn't Working","text":"<ol> <li>Read the error message carefully</li> <li>Check for typos</li> <li>Verify data types and shapes</li> <li>Print intermediate values</li> <li>Simplify to smallest failing case</li> <li>Compare with working examples</li> </ol>"},{"location":"learning-path/#losing-motivation","title":"Losing Motivation","text":"<ul> <li>Review what you've already learned (you've made progress!)</li> <li>Set small, achievable goals</li> <li>Join a study group or find an accountability partner</li> <li>Take a day off and come back fresh</li> <li>Remember your \"why\" - why did you start?</li> </ul>"},{"location":"learning-path/#beyond-the-course","title":"Beyond the Course","text":""},{"location":"learning-path/#after-completing-ml101","title":"After Completing ML101","text":"<p>Practice More: - Kaggle competitions (start with Getting Started competitions) - Personal projects with your own data - Contribute to open-source ML projects</p> <p>Deepen Knowledge: - Implement papers from scratch - Take advanced courses (Deep Learning Specialization, Fast.ai) - Read ML textbooks (Pattern Recognition and Machine Learning, Deep Learning Book)</p> <p>Build a Portfolio: - Document your projects on GitHub - Write blog posts explaining concepts - Create visualizations and tutorials - Share on LinkedIn or personal website</p> <p>Stay Current: - Follow ML researchers on Twitter/X - Read papers on ArXiv - Listen to ML podcasts (TWiML, The Batch) - Attend local ML meetups or conferences</p>"},{"location":"learning-path/#study-group-recommendations","title":"Study Group Recommendations","text":"<p>Learning with others can be highly effective:</p> <p>Weekly Study Group: - Meet weekly to discuss a module - Work through exercises together - Explain concepts to each other - Share different approaches</p> <p>Online Communities: - Reddit: r/MachineLearning, r/learnmachinelearning - Discord servers for ML learners - Twitter/X ML community - LinkedIn ML groups</p> <p>Teaching Others: - Start a blog about your learning - Create YouTube videos explaining concepts - Mentor beginners once you've progressed - Answer questions on Stack Overflow</p> <p>The Feynman Technique</p> <p>\"If you can't explain it simply, you don't understand it well enough.\" Teaching is one of the best ways to learn.</p>"},{"location":"learning-path/#module-specific-tips","title":"Module-Specific Tips","text":""},{"location":"learning-path/#module-1-linear-algebra","title":"Module 1: Linear Algebra","text":"<ul> <li>Visualize everything: Use matplotlib to plot vectors and matrices</li> <li>Connect to geometry: Think about transformations visually</li> <li>Practice by hand first: Do small examples manually before coding</li> <li>Use NumPy broadcasting: Learn how it works early</li> </ul>"},{"location":"learning-path/#module-2-regression","title":"Module 2: Regression","text":"<ul> <li>Plot your data: Always visualize before modeling</li> <li>Watch gradient descent: Animate the optimization process</li> <li>Experiment with learning rates: See what happens when too high/low</li> <li>Understand overfitting: This concept appears everywhere</li> </ul>"},{"location":"learning-path/#module-3-trees","title":"Module 3: Trees","text":"<ul> <li>Visualize trees: Use sklearn's tree plotting</li> <li>Compare algorithms: See how different models perform</li> <li>Feature importance: Understand what drives predictions</li> <li>Try hyperparameter tuning: See the impact of different settings</li> </ul>"},{"location":"learning-path/#module-4-neural-networks","title":"Module 4: Neural Networks","text":"<ul> <li>Start simple: Single neuron before complex architectures</li> <li>Implement from scratch first: Don't jump to PyTorch immediately</li> <li>Understand dimensions: Print shapes constantly</li> <li>Monitor training: Watch loss curves, check for overfitting</li> </ul>"},{"location":"learning-path/#assessment-and-progress-tracking","title":"Assessment and Progress Tracking","text":""},{"location":"learning-path/#self-assessment","title":"Self-Assessment","text":"<p>After each module, ask yourself: - [ ] Can I explain key concepts without notes? - [ ] Did I complete all exercises? - [ ] Do I understand the solutions? - [ ] Can I modify code for different scenarios? - [ ] Could I teach this to someone else?</p>"},{"location":"learning-path/#keep-a-progress-log","title":"Keep a Progress Log","text":"<p>Track your journey: <pre><code>## Week 1\n- Completed Module 1 Lessons 1-2\n- Struggled with eigenvalues but figured it out\n- Need to review matrix multiplication\n\n## Week 2\n- Finished Module 1\n- PCA exercise was challenging but rewarding\n- Ready for Module 2\n</code></pre></p>"},{"location":"learning-path/#resources-for-different-learning-styles","title":"Resources for Different Learning Styles","text":""},{"location":"learning-path/#visual-learners","title":"Visual Learners","text":"<ul> <li>Watch 3Blue1Brown videos on linear algebra and neural networks</li> <li>Create mind maps of concepts</li> <li>Use plotting extensively in notebooks</li> <li>Draw architectures and data flows</li> </ul>"},{"location":"learning-path/#auditory-learners","title":"Auditory Learners","text":"<ul> <li>Explain concepts aloud</li> <li>Use text-to-speech for reading lessons</li> <li>Join study group discussions</li> <li>Listen to ML podcasts</li> </ul>"},{"location":"learning-path/#kinesthetic-learners","title":"Kinesthetic Learners","text":"<ul> <li>Type all code (no copy-paste)</li> <li>Build physical models of concepts</li> <li>Take breaks to move around</li> <li>Implement variations of exercises</li> </ul>"},{"location":"learning-path/#readingwriting-learners","title":"Reading/Writing Learners","text":"<ul> <li>Take detailed notes</li> <li>Write blog posts about concepts</li> <li>Create cheat sheets</li> <li>Annotate code extensively</li> </ul>"},{"location":"learning-path/#final-advice","title":"Final Advice","text":""},{"location":"learning-path/#remember-these-principles","title":"Remember These Principles","text":"<ol> <li>Consistency &gt; Intensity: 30 minutes daily beats 3 hours once a week</li> <li>Understanding &gt; Speed: Deep learning takes time</li> <li>Practice &gt; Theory: Build things to cement knowledge</li> <li>Questions &gt; Answers: Asking good questions drives learning</li> <li>Progress &gt; Perfection: You'll make mistakes - that's how you learn</li> </ol>"},{"location":"learning-path/#youve-got-this","title":"You've Got This!","text":"<p>Machine learning can seem overwhelming at first, but with consistent effort and the right approach, you'll be amazed at how much you can learn. Thousands of people have successfully taught themselves ML, and you can too.</p> <p>Ready to start?</p> <p>Begin Module 1</p> <p>Questions about the learning path? Open an issue on GitHub.</p>"},{"location":"module1-linear-algebra/","title":"Module 1: Linear Algebra Basics","text":""},{"location":"module1-linear-algebra/#overview","title":"Overview","text":"<p>Linear algebra is the mathematical foundation of machine learning. Understanding vectors, matrices, and their operations is essential for grasping how ML algorithms work under the hood. This module will give you the mathematical tools you need for the rest of the course.</p>"},{"location":"module1-linear-algebra/#why-linear-algebra-matters-in-machine-learning","title":"Why Linear Algebra Matters in Machine Learning","text":"<p>In machine learning: - Data is represented as vectors and matrices: Each data sample is a vector, and datasets are matrices - Models perform transformations: Linear algebra describes how inputs become outputs - Optimization uses gradients: Derivatives are computed using matrix operations - Dimensionality reduction: Techniques like PCA rely on eigenvalue decomposition - Neural networks: Weights are matrices, and forward/backward propagation uses matrix multiplication</p> <p>Why Study This?</p> <p>\"You can't truly understand machine learning without understanding linear algebra. Every ML algorithm performs operations on vectors and matrices.\"</p>"},{"location":"module1-linear-algebra/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ul> <li>\u2705 Understand vectors geometrically and algebraically</li> <li>\u2705 Perform vector operations (addition, scaling, dot product)</li> <li>\u2705 Work with matrices and matrix operations</li> <li>\u2705 Compute matrix-vector and matrix-matrix products</li> <li>\u2705 Understand eigenvalues and eigenvectors</li> <li>\u2705 Apply PCA for dimensionality reduction</li> <li>\u2705 Implement these concepts using NumPy</li> </ul>"},{"location":"module1-linear-algebra/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python basics: Variables, functions, loops</li> <li>High school math: Basic algebra and geometry</li> <li>NumPy installation: Required for all exercises</li> </ul>"},{"location":"module1-linear-algebra/#module-structure","title":"Module Structure","text":""},{"location":"module1-linear-algebra/#lesson-1-vectors","title":"Lesson 1: Vectors","text":"<p>Time: 45 minutes</p> <p>Learn the fundamentals of vectors - the building blocks of linear algebra.</p> <ul> <li>What are vectors?</li> <li>Vector operations: addition, scaling, dot product</li> <li>Vector norms (L1, L2, infinity)</li> <li>Geometric interpretation</li> <li>Implementation with NumPy</li> </ul> <p>Start Lesson 1</p>"},{"location":"module1-linear-algebra/#lesson-2-matrices","title":"Lesson 2: Matrices","text":"<p>Time: 45 minutes</p> <p>Understand matrices and how they represent data and transformations.</p> <ul> <li>Matrix fundamentals and notation</li> <li>Types of matrices (identity, diagonal, symmetric)</li> <li>Matrix operations (addition, multiplication, transpose)</li> <li>Matrix-vector multiplication</li> <li>Implementation with NumPy</li> </ul> <p>Start Lesson 2</p>"},{"location":"module1-linear-algebra/#lesson-3-advanced-matrix-operations","title":"Lesson 3: Advanced Matrix Operations","text":"<p>Time: 60 minutes</p> <p>Dive deeper into matrix operations used in ML algorithms.</p> <ul> <li>Matrix inverse and pseudo-inverse</li> <li>Determinants and their meaning</li> <li>Rank and linear independence</li> <li>Matrix decompositions (LU, QR)</li> <li>Solving systems of equations</li> </ul> <p>Start Lesson 3</p>"},{"location":"module1-linear-algebra/#lesson-4-eigenvalues-and-eigenvectors","title":"Lesson 4: Eigenvalues and Eigenvectors","text":"<p>Time: 60 minutes</p> <p>Master eigenvalues and eigenvectors, crucial for PCA and many ML algorithms.</p> <ul> <li>Eigenvalue and eigenvector intuition</li> <li>Computing eigenvalues</li> <li>Eigendecomposition</li> <li>Singular Value Decomposition (SVD)</li> <li>Principal Component Analysis (PCA)</li> </ul> <p>Start Lesson 4</p>"},{"location":"module1-linear-algebra/#exercises","title":"Exercises","text":"<p>Time: 3-4 hours</p> <p>Apply what you've learned through hands-on Jupyter notebook exercises.</p> <ul> <li>Exercise 1: Vector operations with NumPy</li> <li>Exercise 2: Matrix manipulations and transformations</li> <li>Exercise 3: PCA implementation from scratch</li> </ul> <p>View Exercises</p>"},{"location":"module1-linear-algebra/#key-concepts","title":"Key Concepts","text":"<p>Throughout this module, you'll encounter these fundamental concepts:</p> Concept Description ML Application Vector Ordered list of numbers Data samples, features Matrix 2D array of numbers Datasets, weight matrices Dot Product Sum of element-wise products Similarity measures Matrix Multiplication Combine transformations Neural network layers Eigenvalues Scale factors for eigenvectors PCA, stability analysis Eigenvectors Special directions preserved by transformation Principal components"},{"location":"module1-linear-algebra/#what-youll-build","title":"What You'll Build","text":"<p>By the end of this module, you'll implement:</p> <ol> <li>Vector Operations Library: Functions for vector math</li> <li>Matrix Transformation Visualizer: See how matrices transform space</li> <li>PCA from Scratch: Dimensionality reduction without sklearn</li> </ol>"},{"location":"module1-linear-algebra/#tips-for-success","title":"Tips for Success","text":"<p>Visualization is Key</p> <p>Linear algebra becomes much clearer when you visualize it. Use matplotlib to plot vectors, transformations, and data projections.</p> <p>Start Small</p> <p>Begin with 2D examples before moving to higher dimensions. It's easier to visualize and debug.</p> <p>NumPy Broadcasting</p> <p>Learn how NumPy broadcasting works early. It will save you from writing many loops.</p> <p>Common Pitfall</p> <p>Matrix multiplication is not commutative: <code>A @ B \u2260 B @ A</code>. Always pay attention to order and dimensions.</p>"},{"location":"module1-linear-algebra/#estimated-time","title":"Estimated Time","text":"<ul> <li>Reading lessons: 3-4 hours</li> <li>Completing exercises: 3-4 hours</li> <li>Total: 6-8 hours</li> </ul> <p>Take your time! Understanding this material deeply will make the rest of the course much easier.</p>"},{"location":"module1-linear-algebra/#resources","title":"Resources","text":""},{"location":"module1-linear-algebra/#recommended-videos","title":"Recommended Videos","text":"<ul> <li>3Blue1Brown - Essence of Linear Algebra - Excellent visual explanations</li> <li>Khan Academy - Linear Algebra - Comprehensive tutorials</li> </ul>"},{"location":"module1-linear-algebra/#additional-reading","title":"Additional Reading","text":"<ul> <li>Linear Algebra Review (Stanford CS229)</li> <li>NumPy for MATLAB Users</li> </ul>"},{"location":"module1-linear-algebra/#ready-to-start","title":"Ready to Start?","text":"<p>Let's begin with the fundamentals of vectors!</p> <p>Start Lesson 1: Vectors</p> <p>Questions? Open an issue on GitHub.</p>"},{"location":"module1-linear-algebra/01-vectors/","title":"Lesson 1: Vectors","text":""},{"location":"module1-linear-algebra/01-vectors/#introduction","title":"Introduction","text":"<p>Vectors are fundamental building blocks in machine learning. Every data sample you work with is represented as a vector, and understanding how to manipulate vectors is essential for understanding ML algorithms.</p>"},{"location":"module1-linear-algebra/01-vectors/#what-is-a-vector","title":"What is a Vector?","text":"<p>A vector is an ordered list of numbers. You can think of vectors in two complementary ways:</p>"},{"location":"module1-linear-algebra/01-vectors/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>A vector represents a point in space or a direction with magnitude. For example, in 2D:</p> \\[\\vec{v} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}\\] <p>This vector points from the origin (0, 0) to the point (3, 2).</p>"},{"location":"module1-linear-algebra/01-vectors/#algebraic-interpretation","title":"Algebraic Interpretation","text":"<p>A vector is simply a collection of numbers in a specific order. In machine learning: - Each number represents a feature - The vector represents a data sample</p> <p>Example: A house might be represented as: $\\(\\vec{house} = \\begin{bmatrix} 1500 \\\\ 3 \\\\ 2 \\\\ 250000 \\end{bmatrix}\\)$</p> <p>where features are: [square feet, bedrooms, bathrooms, price]</p>"},{"location":"module1-linear-algebra/01-vectors/#vector-notation","title":"Vector Notation","text":"<p>Vectors can be written in different ways:</p> <ul> <li> <p>Column vector (most common in ML): $\\(\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\)$</p> </li> <li> <p>Row vector: $\\(\\vec{v}^T = \\begin{bmatrix} v_1 &amp; v_2 &amp; \\cdots &amp; v_n \\end{bmatrix}\\)$</p> </li> <li> <p>Python/NumPy: <pre><code>v = np.array([v1, v2, ..., vn])\n</code></pre></p> </li> </ul>"},{"location":"module1-linear-algebra/01-vectors/#creating-vectors-in-numpy","title":"Creating Vectors in NumPy","text":"<p>Try It Yourself</p> <p>Click the \u25b6 Run Code button below to execute Python code directly in your browser - no installation needed!</p> <pre><code>import numpy as np\n\n# Create a vector\nv = np.array([1, 2, 3, 4])\nprint(f\"Vector: {v}\")\nprint(f\"Shape: {v.shape}\")\n\n# Create a column vector (2D array)\nv_col = np.array([[1], [2], [3], [4]])\nprint(f\"\\nColumn vector shape: {v_col.shape}\")\n\n# Create specific vectors\nzeros = np.zeros(5)\nones = np.ones(3)\nprint(f\"\\nZeros: {zeros}\")\nprint(f\"Ones: {ones}\")\n</code></pre> <p>Try modifying the code: Change the numbers in the vector and run it again!</p> <p>1D vs 2D Arrays</p> <p>In NumPy, <code>np.array([1, 2, 3])</code> creates a 1D array with shape <code>(3,)</code>, while <code>np.array([[1], [2], [3]])</code> creates a 2D column vector with shape <code>(3, 1)</code>. Both work for most operations, but be aware of the difference.</p>"},{"location":"module1-linear-algebra/01-vectors/#vector-operations","title":"Vector Operations","text":""},{"location":"module1-linear-algebra/01-vectors/#1-vector-addition","title":"1. Vector Addition","text":"<p>Add vectors element-wise:</p> \\[\\vec{a} + \\vec{b} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} = \\begin{bmatrix} a_1 + b_1 \\\\ a_2 + b_2 \\end{bmatrix}\\] <p>Geometric interpretation: Place vectors tip-to-tail.</p> <pre><code>a = np.array([1, 2])\nb = np.array([3, 1])\nc = a + b  # [4, 3]\n</code></pre> <p>ML Application: Combining features or model updates.</p>"},{"location":"module1-linear-algebra/01-vectors/#2-scalar-multiplication","title":"2. Scalar Multiplication","text":"<p>Multiply each element by a scalar:</p> \\[c \\cdot \\vec{v} = c \\cdot \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} c \\cdot v_1 \\\\ c \\cdot v_2 \\end{bmatrix}\\] <p>Geometric interpretation: Scales the vector (stretches or shrinks).</p> <pre><code>v = np.array([1, 2, 3])\nscaled = 2 * v  # [2, 4, 6]\n</code></pre> <p>ML Application: Learning rate scaling in gradient descent.</p>"},{"location":"module1-linear-algebra/01-vectors/#3-dot-product-inner-product","title":"3. Dot Product (Inner Product)","text":"<p>The dot product combines two vectors into a single number:</p> \\[\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i \\cdot b_i = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n\\] <pre><code>a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Three ways to compute dot product\ndot1 = np.dot(a, b)      # 32\ndot2 = a @ b             # 32 (@ is matrix multiplication operator)\ndot3 = (a * b).sum()     # 32 (element-wise multiply, then sum)\n</code></pre> <p>Geometric interpretation:</p> \\[\\vec{a} \\cdot \\vec{b} = \\|\\vec{a}\\| \\|\\vec{b}\\| \\cos(\\theta)\\] <p>where \\(\\theta\\) is the angle between vectors.</p> <ul> <li>If \\(\\vec{a} \\cdot \\vec{b} &gt; 0\\): vectors point in similar directions</li> <li>If \\(\\vec{a} \\cdot \\vec{b} = 0\\): vectors are perpendicular (orthogonal)</li> <li>If \\(\\vec{a} \\cdot \\vec{b} &lt; 0\\): vectors point in opposite directions</li> </ul> <p>ML Application: - Measuring similarity between data samples - Computing neural network activations: \\(\\vec{w} \\cdot \\vec{x} + b\\)</p>"},{"location":"module1-linear-algebra/01-vectors/#4-element-wise-operations","title":"4. Element-wise Operations","text":"<p>Multiply or divide elements one-by-one (Hadamard product):</p> <pre><code>a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\nelementwise_mult = a * b  # [4, 10, 18]\nelementwise_div = a / b   # [0.25, 0.4, 0.5]\n</code></pre> <p>Dot Product vs Element-wise Multiplication</p> <ul> <li><code>np.dot(a, b)</code> or <code>a @ b</code>: dot product \u2192 single number</li> <li><code>a * b</code>: element-wise multiplication \u2192 vector</li> </ul>"},{"location":"module1-linear-algebra/01-vectors/#vector-norms","title":"Vector Norms","text":"<p>A norm measures the \"size\" or \"length\" of a vector.</p>"},{"location":"module1-linear-algebra/01-vectors/#l2-norm-euclidean-norm","title":"L2 Norm (Euclidean Norm)","text":"<p>The most common norm - the straight-line distance:</p> \\[\\|\\vec{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\\] <pre><code>v = np.array([3, 4])\nl2_norm = np.linalg.norm(v)  # 5.0\n# Or manually:\nl2_norm_manual = np.sqrt((v ** 2).sum())  # 5.0\n</code></pre>"},{"location":"module1-linear-algebra/01-vectors/#l1-norm-manhattan-norm","title":"L1 Norm (Manhattan Norm)","text":"<p>Sum of absolute values:</p> \\[\\|\\vec{v}\\|_1 = |v_1| + |v_2| + \\cdots + |v_n|\\] <pre><code>v = np.array([3, -4])\nl1_norm = np.linalg.norm(v, ord=1)  # 7.0\n# Or manually:\nl1_norm_manual = np.abs(v).sum()  # 7.0\n</code></pre>"},{"location":"module1-linear-algebra/01-vectors/#l-norm-maximum-norm","title":"L\u221e Norm (Maximum Norm)","text":"<p>The largest absolute value:</p> \\[\\|\\vec{v}\\|_\\infty = \\max(|v_1|, |v_2|, \\ldots, |v_n|)\\] <pre><code>v = np.array([3, -7, 2])\nlinf_norm = np.linalg.norm(v, ord=np.inf)  # 7.0\n# Or manually:\nlinf_norm_manual = np.abs(v).max()  # 7.0\n</code></pre> <p>ML Applications: - L2 norm: Euclidean distance, L2 regularization - L1 norm: Manhattan distance, L1 regularization (encourages sparsity) - L\u221e norm: Measuring worst-case error</p>"},{"location":"module1-linear-algebra/01-vectors/#unit-vectors-and-normalization","title":"Unit Vectors and Normalization","text":"<p>A unit vector has length 1. Normalizing a vector means converting it to a unit vector:</p> \\[\\hat{v} = \\frac{\\vec{v}}{\\|\\vec{v}\\|}\\] <pre><code>v = np.array([3, 4])\nv_normalized = v / np.linalg.norm(v)  # [0.6, 0.8]\nprint(np.linalg.norm(v_normalized))   # 1.0\n</code></pre> <p>ML Application: Feature normalization to ensure all features have similar scales.</p>"},{"location":"module1-linear-algebra/01-vectors/#distance-between-vectors","title":"Distance Between Vectors","text":""},{"location":"module1-linear-algebra/01-vectors/#euclidean-distance","title":"Euclidean Distance","text":"\\[d(\\vec{a}, \\vec{b}) = \\|\\vec{a} - \\vec{b}\\|_2 = \\sqrt{\\sum_{i=1}^{n} (a_i - b_i)^2}\\] <pre><code>a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\ndistance = np.linalg.norm(a - b)  # 5.196\n</code></pre> <p>ML Application: K-nearest neighbors, clustering algorithms.</p>"},{"location":"module1-linear-algebra/01-vectors/#cosine-similarity","title":"Cosine Similarity","text":"<p>Measures angle between vectors (independent of magnitude):</p> \\[\\text{similarity} = \\frac{\\vec{a} \\cdot \\vec{b}}{\\|\\vec{a}\\| \\|\\vec{b}\\|}\\] \\[\\text{similarity} \\in [-1, 1]\\] <pre><code>a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\ncosine_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\nprint(cosine_sim)  # 0.9746 (very similar direction)\n</code></pre> <p>ML Application: Text similarity, recommendation systems.</p>"},{"location":"module1-linear-algebra/01-vectors/#practical-example-working-with-real-data","title":"Practical Example: Working with Real Data","text":"<p>Let's represent and compare houses:</p> <pre><code>import numpy as np\n\n# House features: [square_feet, bedrooms, bathrooms]\nhouse1 = np.array([1500, 3, 2])\nhouse2 = np.array([2000, 4, 3])\nhouse3 = np.array([1200, 2, 1])\n\n# Normalize features (mean 0, std 1)\nhouses = np.array([house1, house2, house3])\nmean = houses.mean(axis=0)\nstd = houses.std(axis=0)\n\nhouse1_norm = (house1 - mean) / std\nhouse2_norm = (house2 - mean) / std\nhouse3_norm = (house3 - mean) / std\n\n# Find which house is most similar to house1\ndist_1_2 = np.linalg.norm(house1_norm - house2_norm)\ndist_1_3 = np.linalg.norm(house1_norm - house3_norm)\n\nprint(f\"Distance from house1 to house2: {dist_1_2:.2f}\")\nprint(f\"Distance from house1 to house3: {dist_1_3:.2f}\")\n\nif dist_1_2 &lt; dist_1_3:\n    print(\"House 2 is more similar to house 1\")\nelse:\n    print(\"House 3 is more similar to house 1\")\n</code></pre>"},{"location":"module1-linear-algebra/01-vectors/#visualization","title":"Visualization","text":"<p>Visualizing vectors helps build geometric intuition:</p> <pre><code>import matplotlib.pyplot as plt\n\n# Create vectors\nv1 = np.array([3, 2])\nv2 = np.array([1, 3])\nv_sum = v1 + v2\n\n# Plot\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot vectors from origin\nax.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1,\n          color='red', width=0.006, label='v1')\nax.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1,\n          color='blue', width=0.006, label='v2')\nax.quiver(0, 0, v_sum[0], v_sum[1], angles='xy', scale_units='xy', scale=1,\n          color='green', width=0.006, label='v1 + v2')\n\n# Formatting\nax.set_xlim(-1, 5)\nax.set_ylim(-1, 6)\nax.set_aspect('equal')\nax.grid(True)\nax.axhline(y=0, color='k', linewidth=0.5)\nax.axvline(x=0, color='k', linewidth=0.5)\nax.legend()\nax.set_title('Vector Addition')\nplt.show()\n</code></pre>"},{"location":"module1-linear-algebra/01-vectors/#key-takeaways","title":"Key Takeaways","text":"<p>Important Concepts</p> <ul> <li>Vectors represent both data samples and directions in space</li> <li>Vector addition combines vectors element-wise</li> <li>Dot product measures similarity and projection</li> <li>Norms measure vector magnitude (L1, L2, L\u221e)</li> <li>Normalization creates unit vectors for fair comparison</li> <li>Distance metrics (Euclidean, cosine) measure similarity</li> </ul>"},{"location":"module1-linear-algebra/01-vectors/#common-patterns-in-ml","title":"Common Patterns in ML","text":"Operation ML Use Case <code>x + y</code> Combining features, model updates <code>np.dot(w, x)</code> Linear model prediction: \\(y = w^T x + b\\) <code>np.linalg.norm(x)</code> Regularization penalty <code>x / np.linalg.norm(x)</code> Feature normalization <code>np.linalg.norm(x - y)</code> Distance for clustering, KNN"},{"location":"module1-linear-algebra/01-vectors/#practice-problems","title":"Practice Problems","text":"<p>Before moving to the exercises, try these quick problems:</p> <ol> <li>Create a vector <code>v = [1, 2, 3, 4]</code> and compute its L2 norm</li> <li>Normalize this vector to unit length</li> <li>Create two vectors and compute their dot product</li> <li>Plot two 2D vectors and their sum</li> </ol>"},{"location":"module1-linear-algebra/01-vectors/#next-steps","title":"Next Steps","text":"<p>Now that you understand vectors, let's move on to matrices - collections of vectors that represent entire datasets!</p> <p>Next: Lesson 2 - Matrices</p> <p>Or complete the exercises first</p> <p>Questions? Open an issue on GitHub.</p>"},{"location":"module1-linear-algebra/02-matrices/","title":"Lesson 2: Matrices","text":""},{"location":"module1-linear-algebra/02-matrices/#introduction","title":"Introduction","text":"<p>Matrices are 2D arrays that represent collections of vectors, transformations, and entire datasets. Understanding matrices is crucial for machine learning since your data is stored as matrices and models perform matrix operations.</p>"},{"location":"module1-linear-algebra/02-matrices/#what-is-a-matrix","title":"What is a Matrix?","text":"<p>A matrix is a rectangular array of numbers arranged in rows and columns:</p> \\[A = \\begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\end{bmatrix}\\] <p>This is a \\(2 \\times 3\\) matrix (2 rows, 3 columns).</p>"},{"location":"module1-linear-algebra/02-matrices/#in-machine-learning","title":"In Machine Learning","text":"<ul> <li>Each row: A data sample (observation)</li> <li>Each column: A feature</li> <li>Entire matrix: Your dataset!</li> </ul> <p>Example: A dataset of 3 houses with 4 features:</p> \\[X = \\begin{bmatrix} 1500 &amp; 3 &amp; 2 &amp; 2005 \\\\ 2000 &amp; 4 &amp; 3 &amp; 2010 \\\\ 1200 &amp; 2 &amp; 1 &amp; 1995 \\end{bmatrix}\\] <p>Rows = houses, Columns = [sqft, bedrooms, bathrooms, year_built]</p>"},{"location":"module1-linear-algebra/02-matrices/#creating-matrices-in-numpy","title":"Creating Matrices in NumPy","text":"<p>Try It Yourself</p> <p>Run this code to see how matrices are created in NumPy!</p> <pre><code>import numpy as np\n\n# Create a matrix\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nprint(f\"Matrix A:\\n{A}\")\nprint(f\"Shape: {A.shape}\")  # (2, 3) - 2 rows, 3 columns\n\n# Special matrices\nzeros = np.zeros((3, 4))       # 3x4 matrix of zeros\nones = np.ones((2, 2))         # 2x2 matrix of ones\nidentity = np.eye(3)           # 3x3 identity matrix\n\nprint(f\"\\nZeros matrix:\\n{zeros}\")\nprint(f\"\\nIdentity matrix:\\n{identity}\")\n\n# From a dataset - houses example\ndata = [[1500, 3, 2],\n        [2000, 4, 3],\n        [1200, 2, 1]]\nX = np.array(data)\nprint(f\"\\nHouse dataset:\\n{X}\")\nprint(f\"Features: [sqft, bedrooms, bathrooms]\")\n</code></pre> <p>Try modifying: Change the matrix dimensions or create your own dataset!</p>"},{"location":"module1-linear-algebra/02-matrices/#types-of-matrices","title":"Types of Matrices","text":""},{"location":"module1-linear-algebra/02-matrices/#square-matrix","title":"Square Matrix","text":"<p>Equal number of rows and columns (\\(n \\times n\\)):</p> \\[A = \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}\\]"},{"location":"module1-linear-algebra/02-matrices/#identity-matrix","title":"Identity Matrix","text":"<p>Square matrix with 1s on diagonal, 0s elsewhere:</p> \\[I = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}\\] <pre><code>I = np.eye(3)\n</code></pre> <p>Property: \\(A \\cdot I = I \\cdot A = A\\) (identity for multiplication)</p>"},{"location":"module1-linear-algebra/02-matrices/#diagonal-matrix","title":"Diagonal Matrix","text":"<p>Non-zero elements only on the diagonal:</p> \\[D = \\begin{bmatrix} d_1 &amp; 0 &amp; 0 \\\\ 0 &amp; d_2 &amp; 0 \\\\ 0 &amp; 0 &amp; d_3 \\end{bmatrix}\\] <pre><code>D = np.diag([1, 2, 3])\n</code></pre>"},{"location":"module1-linear-algebra/02-matrices/#symmetric-matrix","title":"Symmetric Matrix","text":"<p>Equal to its transpose (\\(A = A^T\\)):</p> \\[A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 5 &amp; 6 \\\\ 3 &amp; 6 &amp; 9 \\end{bmatrix}\\] <p>ML Application: Covariance matrices are symmetric.</p>"},{"location":"module1-linear-algebra/02-matrices/#sparse-matrix","title":"Sparse Matrix","text":"<p>Most elements are zero:</p> \\[S = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 3 \\end{bmatrix}\\] <p>ML Application: Text data (document-term matrices) are often sparse.</p>"},{"location":"module1-linear-algebra/02-matrices/#matrix-operations","title":"Matrix Operations","text":""},{"location":"module1-linear-algebra/02-matrices/#transpose","title":"Transpose","text":"<p>Flip rows and columns:</p> \\[A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\quad \\Rightarrow \\quad A^T = \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix}\\] <pre><code>A = np.array([[1, 2, 3],\n              [4, 5, 6]])\nA_T = A.T  # or np.transpose(A)\n</code></pre> <p>Properties: - \\((A^T)^T = A\\) - \\((A + B)^T = A^T + B^T\\) - \\((AB)^T = B^T A^T\\)</p>"},{"location":"module1-linear-algebra/02-matrices/#addition-and-subtraction","title":"Addition and Subtraction","text":"<p>Element-wise operations (matrices must have same shape):</p> \\[A + B = \\begin{bmatrix} a_{11} + b_{11} &amp; a_{12} + b_{12} \\\\ a_{21} + b_{21} &amp; a_{22} + b_{22} \\end{bmatrix}\\] <pre><code>A = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\nC = A + B  # [[6, 8], [10, 12]]\n</code></pre>"},{"location":"module1-linear-algebra/02-matrices/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Multiply every element by a scalar:</p> \\[c \\cdot A = \\begin{bmatrix} c \\cdot a_{11} &amp; c \\cdot a_{12} \\\\ c \\cdot a_{21} &amp; c \\cdot a_{22} \\end{bmatrix}\\] <pre><code>A = np.array([[1, 2], [3, 4]])\nB = 2 * A  # [[2, 4], [6, 8]]\n</code></pre>"},{"location":"module1-linear-algebra/02-matrices/#matrix-vector-multiplication","title":"Matrix-Vector Multiplication","text":"<p>This is fundamental to understanding how ML models make predictions!</p> \\[A\\vec{x} = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} a_{11}x_1 + a_{12}x_2 \\\\ a_{21}x_1 + a_{22}x_2 \\end{bmatrix}\\] <p>Each element of the result is the dot product of a matrix row with the vector.</p> <p>ML Application: Making Predictions</p> <p>This is exactly how a linear model makes predictions! Try it:</p> <pre><code>import numpy as np\n\n# Weight matrix (model parameters)\nW = np.array([[0.5, 0.3],\n              [0.2, 0.7]])\n\n# Input features (e.g., house: [size_normalized, age_normalized])\nx = np.array([0.8, 0.6])\n\n# Make prediction: y = W @ x\ny = W @ x\nprint(f\"Weight matrix W:\\n{W}\")\nprint(f\"\\nInput features x: {x}\")\nprint(f\"\\nPrediction y = W @ x: {y}\")\nprint(f\"\\nManual computation:\")\nprint(f\"  y[0] = {W[0,0]}*{x[0]} + {W[0,1]}*{x[1]} = {W[0,0]*x[0] + W[0,1]*x[1]:.2f}\")\nprint(f\"  y[1] = {W[1,0]}*{x[0]} + {W[1,1]}*{x[1]} = {W[1,0]*x[0] + W[1,1]*x[1]:.2f}\")\n</code></pre> <p>Mathematical form: \\(\\hat{y} = Wx + b\\)</p> <p>where \\(W\\) is a weight matrix, \\(x\\) is input features, \\(b\\) is bias.</p>"},{"location":"module1-linear-algebra/02-matrices/#matrix-matrix-multiplication","title":"Matrix-Matrix Multiplication","text":"<p>Multiplying two matrices combines their transformations:</p> \\[C = AB\\] <p>where \\(C_{ij} = \\sum_{k} A_{ik} B_{kj}\\)</p> <p>Rule: For \\(A_{m \\times n}\\) and \\(B_{n \\times p}\\), result is \\(C_{m \\times p}\\)</p> <p>Dimension Compatibility</p> <p>Number of columns in \\(A\\) must equal number of rows in \\(B\\)!</p> <pre><code>A = np.array([[1, 2],\n              [3, 4]])  # 2x2\nB = np.array([[5, 6],\n              [7, 8]])  # 2x2\n\nC = A @ B  # 2x2\n# [[19, 22],\n#  [43, 50]]\n\n# Element-by-element:\n# C[0,0] = 1*5 + 2*7 = 19\n# C[0,1] = 1*6 + 2*8 = 22\n# C[1,0] = 3*5 + 4*7 = 43\n# C[1,1] = 3*6 + 4*8 = 50\n</code></pre> <p>Properties: - Not commutative: \\(AB \\neq BA\\) (usually) - Associative: \\((AB)C = A(BC)\\) - Distributive: \\(A(B + C) = AB + AC\\)</p> <p>ML Application: Stacking neural network layers:</p> \\[output = W_3(W_2(W_1 x))\\]"},{"location":"module1-linear-algebra/02-matrices/#practical-example-dataset-operations","title":"Practical Example: Dataset Operations","text":"<p>Working with Real Data</p> <p>This is what you'll do in every ML project - manipulate datasets!</p> <pre><code>import numpy as np\n\n# Dataset: 4 samples, 3 features\nX = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9],\n              [10, 11, 12]])\n\nprint(f\"Dataset shape: {X.shape}\")  # (4, 3)\nprint(f\"Dataset:\\n{X}\\n\")\n\n# Mean of each feature (column)\nfeature_means = X.mean(axis=0)\nprint(f\"Feature means: {feature_means}\")\n\n# Mean of each sample (row)\nsample_means = X.mean(axis=1)\nprint(f\"Sample means: {sample_means}\")\n\n# Center the data (subtract mean) - important preprocessing!\nX_centered = X - feature_means\nprint(f\"\\nCentered data:\\n{X_centered}\")\n\n# Get specific samples and features\nfirst_sample = X[0, :]      # First row\nsecond_feature = X[:, 1]    # Second column\nsubset = X[0:2, 1:3]        # First 2 rows, columns 1-2\n\nprint(f\"\\nFirst sample: {first_sample}\")\nprint(f\"Second feature (all rows): {second_feature}\")\nprint(f\"Subset [0:2, 1:3]:\\n{subset}\")\n</code></pre> <p>Try it: Change the indexing to extract different parts of the dataset!</p>"},{"location":"module1-linear-algebra/02-matrices/#visualization","title":"Visualization","text":"<p>Matrices can transform vectors:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\n# Original vector\nv = np.array([1, 1])\n\n# Transformation matrices\nrotation = np.array([[0, -1],\n                      [1, 0]])  # 90\u00b0 rotation\nscaling = np.array([[2, 0],\n                     [0, 0.5]])  # Scale x by 2, y by 0.5\nshear = np.array([[1, 0.5],\n                   [0, 1]])  # Shear transformation\n\n# Apply transformations\nv_rot = rotation @ v\nv_scale = scaling @ v\nv_shear = shear @ v\n\n# Plot\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor ax, v_transformed, title in zip(axes,\n                                     [v_rot, v_scale, v_shear],\n                                     ['Rotation', 'Scaling', 'Shear']):\n    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n              color='blue', width=0.01, label='Original')\n    ax.quiver(0, 0, v_transformed[0], v_transformed[1],\n              angles='xy', scale_units='xy', scale=1,\n              color='red', width=0.01, label='Transformed')\n    ax.set_xlim(-2, 3)\n    ax.set_ylim(-2, 3)\n    ax.set_aspect('equal')\n    ax.grid(True)\n    ax.legend()\n    ax.set_title(title)\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"module1-linear-algebra/02-matrices/#key-takeaways","title":"Key Takeaways","text":"<p>Important Concepts</p> <ul> <li>Matrices represent datasets (rows = samples, columns = features)</li> <li>Transpose flips rows and columns</li> <li>Matrix multiplication combines transformations</li> <li>Matrix-vector product is how models make predictions</li> <li>Matrix dimensions must be compatible for multiplication</li> </ul>"},{"location":"module1-linear-algebra/02-matrices/#next-steps","title":"Next Steps","text":"<p>Next: Lesson 3 - Advanced Matrix Operations</p> <p>Back: Lesson 1 - Vectors</p>"},{"location":"module1-linear-algebra/03-matrix-operations/","title":"Lesson 3: Advanced Matrix Operations","text":""},{"location":"module1-linear-algebra/03-matrix-operations/#introduction","title":"Introduction","text":"<p>This lesson covers advanced matrix operations essential for machine learning: inverses, determinants, rank, and matrix decompositions.</p>"},{"location":"module1-linear-algebra/03-matrix-operations/#matrix-inverse","title":"Matrix Inverse","text":"<p>The inverse of a matrix \\(A\\) is denoted \\(A^{-1}\\) and satisfies:</p> \\[AA^{-1} = A^{-1}A = I\\]"},{"location":"module1-linear-algebra/03-matrix-operations/#computing-inverse-in-numpy","title":"Computing Inverse in NumPy","text":"<pre><code>import numpy as np\n\nA = np.array([[1, 2],\n              [3, 4]])\nA_inv = np.linalg.inv(A)\n\n# Verify: A @ A_inv should be identity\nprint(A @ A_inv)\n# [[1, 0],\n#  [0, 1]]\n</code></pre> <p>ML Application: Solving normal equations in linear regression:</p> \\[\\hat{w} = (X^TX)^{-1}X^Ty\\]"},{"location":"module1-linear-algebra/03-matrix-operations/#determinant","title":"Determinant","text":"<p>The determinant measures how much a matrix scales space:</p> <pre><code>A = np.array([[1, 2],\n              [3, 4]])\ndet_A = np.linalg.det(A)  # -2.0\n</code></pre> <p>Properties: - If \\(\\det(A) = 0\\), matrix is singular (not invertible) - If \\(|\\det(A)| &gt; 1\\), transformation expands space - If \\(|\\det(A)| &lt; 1\\), transformation contracts space</p>"},{"location":"module1-linear-algebra/03-matrix-operations/#rank-and-linear-independence","title":"Rank and Linear Independence","text":"<p>The rank is the number of linearly independent rows/columns:</p> <pre><code>A = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\nrank = np.linalg.matrix_rank(A)  # 2 (not full rank!)\n</code></pre> <p>ML Application: Identifying redundant features.</p>"},{"location":"module1-linear-algebra/03-matrix-operations/#matrix-decompositions","title":"Matrix Decompositions","text":""},{"location":"module1-linear-algebra/03-matrix-operations/#lu-decomposition","title":"LU Decomposition","text":"<p>Decomposes \\(A\\) into lower and upper triangular matrices:</p> \\[A = LU\\] <pre><code>from scipy.linalg import lu\n\nA = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9]])\nP, L, U = lu(A)\n</code></pre>"},{"location":"module1-linear-algebra/03-matrix-operations/#qr-decomposition","title":"QR Decomposition","text":"<p>Decomposes \\(A\\) into orthogonal and upper triangular matrices:</p> \\[A = QR\\] <pre><code>A = np.array([[1, 2],\n              [3, 4],\n              [5, 6]])\nQ, R = np.linalg.qr(A)\n</code></pre> <p>ML Application: Gram-Schmidt process, solving least squares.</p>"},{"location":"module1-linear-algebra/03-matrix-operations/#solving-systems-of-linear-equations","title":"Solving Systems of Linear Equations","text":""},{"location":"module1-linear-algebra/03-matrix-operations/#the-problem","title":"The Problem","text":"<p>Solve \\(Ax = b\\) for \\(x\\). This is the foundation of linear regression!</p> <p>ML in Action: Solving for Model Parameters</p> <p>This demonstrates how linear regression finds the best-fit parameters:</p> <pre><code>import numpy as np\n\n# System: 3x + y = 9\n#         x + 2y = 8\n# Solution: x=2, y=3\n\nA = np.array([[3, 1],\n              [1, 2]])\nb = np.array([9, 8])\n\n# Method 1: Using solve (most efficient)\nx_solve = np.linalg.solve(A, b)\nprint(f\"System of equations:\")\nprint(f\"  3x + y = 9\")\nprint(f\"  x + 2y = 8\")\nprint(f\"\\nSolution using np.linalg.solve: {x_solve}\")\n\n# Method 2: Using inverse (less efficient)\nA_inv = np.linalg.inv(A)\nx_inv = A_inv @ b\nprint(f\"Solution using inverse: {x_inv}\")\n\n# Verify the solution\nverification = A @ x_solve\nprint(f\"\\nVerification: A @ x = {verification}\")\nprint(f\"Expected b = {b}\")\nprint(f\"Match: {np.allclose(verification, b)}\")\n\n# This is what linear regression does!\nprint(f\"\\n\ud83d\udca1 Linear regression solves: (X^T X)w = X^T y\")\n</code></pre> <p>ML Application: This is exactly what linear regression does to find optimal weights!</p>"},{"location":"module1-linear-algebra/03-matrix-operations/#pseudo-inverse-moore-penrose-inverse","title":"Pseudo-Inverse (Moore-Penrose Inverse)","text":"<p>For non-square or singular matrices:</p> <pre><code>A = np.array([[1, 2],\n              [3, 4],\n              [5, 6]])  # 3x2 matrix (not square)\n\nA_pinv = np.linalg.pinv(A)  # 2x3 pseudo-inverse\n\n# Property: A @ A_pinv @ A \u2248 A\n</code></pre> <p>ML Application: Solving overdetermined systems in linear regression.</p>"},{"location":"module1-linear-algebra/03-matrix-operations/#key-takeaways","title":"Key Takeaways","text":"<p>Important Concepts</p> <ul> <li>Matrix inverse: \\(AA^{-1} = I\\) (when it exists)</li> <li>Determinant: Measures scaling factor of transformation</li> <li>Rank: Number of linearly independent rows/columns</li> <li>Decompositions: LU, QR for efficient computation</li> <li>Solving \\(Ax = b\\) is central to many ML algorithms</li> </ul>"},{"location":"module1-linear-algebra/03-matrix-operations/#next-steps","title":"Next Steps","text":"<p>Next: Lesson 4 - Eigenvalues &amp; Eigenvectors</p> <p>Back: Lesson 2 - Matrices</p>"},{"location":"module1-linear-algebra/04-eigenvalues/","title":"Lesson 4: Eigenvalues and Eigenvectors","text":""},{"location":"module1-linear-algebra/04-eigenvalues/#introduction","title":"Introduction","text":"<p>Eigenvalues and eigenvectors are among the most important concepts in linear algebra for machine learning. They're the foundation of Principal Component Analysis (PCA), one of the most widely used dimensionality reduction techniques.</p>"},{"location":"module1-linear-algebra/04-eigenvalues/#what-are-eigenvalues-and-eigenvectors","title":"What Are Eigenvalues and Eigenvectors?","text":"<p>For a square matrix \\(A\\), an eigenvector \\(\\vec{v}\\) and its corresponding eigenvalue \\(\\lambda\\) satisfy:</p> \\[A\\vec{v} = \\lambda\\vec{v}\\]"},{"location":"module1-linear-algebra/04-eigenvalues/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>When you multiply a matrix by its eigenvector, the vector only gets scaled (not rotated). The eigenvalue is the scaling factor.</p> <pre><code>import numpy as np\n\nA = np.array([[2, 0],\n              [0, 3]])\nv = np.array([1, 0])  # Eigenvector\n\nresult = A @ v  # [2, 0] = 2 * [1, 0]\n# v is scaled by eigenvalue \u03bb = 2\n</code></pre>"},{"location":"module1-linear-algebra/04-eigenvalues/#computing-eigenvalues-and-eigenvectors","title":"Computing Eigenvalues and Eigenvectors","text":"<pre><code>A = np.array([[4, 2],\n              [1, 3]])\n\n# Compute eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\nprint(\"Eigenvalues:\", eigenvalues)  # [5, 2]\nprint(\"Eigenvectors:\\n\", eigenvectors)\n\n# Verify for first eigenvector\nv1 = eigenvectors[:, 0]\nlambda1 = eigenvalues[0]\n\nprint(\"A @ v1:\", A @ v1)\nprint(\"lambda1 * v1:\", lambda1 * v1)\n# They should be equal!\n</code></pre>"},{"location":"module1-linear-algebra/04-eigenvalues/#properties-of-eigenvalues","title":"Properties of Eigenvalues","text":"<ol> <li>Sum of eigenvalues = Trace (sum of diagonal elements)</li> <li>Product of eigenvalues = Determinant</li> <li>Symmetric matrices have real eigenvalues</li> <li>Symmetric matrices have orthogonal eigenvectors</li> </ol>"},{"location":"module1-linear-algebra/04-eigenvalues/#eigendecomposition","title":"Eigendecomposition","text":"<p>For a diagonalizable matrix \\(A\\):</p> \\[A = Q\\Lambda Q^{-1}\\] <p>where: - \\(Q\\): Matrix of eigenvectors (columns) - \\(\\Lambda\\): Diagonal matrix of eigenvalues</p> <pre><code>A = np.array([[4, 2],\n              [1, 3]])\n\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Reconstruct A\nLambda = np.diag(eigenvalues)\nA_reconstructed = eigenvectors @ Lambda @ np.linalg.inv(eigenvectors)\n\nprint(np.allclose(A, A_reconstructed))  # True\n</code></pre>"},{"location":"module1-linear-algebra/04-eigenvalues/#singular-value-decomposition-svd","title":"Singular Value Decomposition (SVD)","text":"<p>SVD works for any matrix (not just square):</p> \\[A = U\\Sigma V^T\\] <p>where: - \\(U\\): Left singular vectors (eigenvectors of \\(AA^T\\)) - \\(\\Sigma\\): Singular values (square roots of eigenvalues) - \\(V^T\\): Right singular vectors (eigenvectors of \\(A^TA\\))</p> <pre><code>A = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9],\n              [10, 11, 12]])  # 4x3 matrix\n\nU, S, VT = np.linalg.svd(A, full_matrices=False)\n\nprint(U.shape)   # (4, 3)\nprint(S.shape)   # (3,)\nprint(VT.shape)  # (3, 3)\n\n# Reconstruct A\nSigma = np.diag(S)\nA_reconstructed = U @ Sigma @ VT\nprint(np.allclose(A, A_reconstructed))  # True\n</code></pre>"},{"location":"module1-linear-algebra/04-eigenvalues/#principal-component-analysis-pca","title":"Principal Component Analysis (PCA)","text":"<p>PCA uses eigenvalues and eigenvectors to find the most important directions in your data.</p>"},{"location":"module1-linear-algebra/04-eigenvalues/#the-goal","title":"The Goal","text":"<p>Reduce dimensions while preserving maximum variance: - Original: 100 features - Reduced: 10 features (principal components) - Retain 95% of information!</p>"},{"location":"module1-linear-algebra/04-eigenvalues/#how-pca-works","title":"How PCA Works","text":"<ol> <li>Center the data (subtract mean)</li> <li>Compute covariance matrix: \\(C = \\frac{1}{n}X^TX\\)</li> <li>Find eigenvectors of covariance matrix</li> <li>Project data onto top eigenvectors</li> </ol> <pre><code>import numpy as np\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n# Load data\niris = load_iris()\nX = iris.data  # 150 samples, 4 features\n\n# Step 1: Center the data\nX_centered = X - X.mean(axis=0)\n\n# Step 2: Compute covariance matrix\ncov_matrix = (X_centered.T @ X_centered) / (X.shape[0] - 1)\n\n# Step 3: Compute eigenvectors and eigenvalues\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n# Sort by eigenvalue (descending)\nidx = eigenvalues.argsort()[::-1]\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\n# Step 4: Project onto first 2 principal components\nPC1 = eigenvectors[:, 0]\nPC2 = eigenvectors[:, 1]\n\nX_pca = X_centered @ np.column_stack([PC1, PC2])\n\n# Visualize\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1],\n                      c=iris.target, cmap='viridis')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\nplt.title('PCA of Iris Dataset')\nplt.colorbar(scatter, label='Species')\nplt.grid(True)\nplt.show()\n\n# Explained variance\nexplained_variance = eigenvalues / eigenvalues.sum()\nprint(\"Explained variance ratio:\", explained_variance)\n# [0.92, 0.05, 0.02, 0.005]\n# First PC captures 92% of variance!\n</code></pre>"},{"location":"module1-linear-algebra/04-eigenvalues/#using-sklearns-pca","title":"Using sklearn's PCA","text":"<pre><code>from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\nprint(\"Components:\\n\", pca.components_)\n</code></pre>"},{"location":"module1-linear-algebra/04-eigenvalues/#when-to-use-pca","title":"When to Use PCA","text":"<p>Use PCA when: - \u2705 You have too many features (curse of dimensionality) - \u2705 Features are correlated - \u2705 You need visualization (reduce to 2D/3D) - \u2705 You want to speed up training - \u2705 You need noise reduction</p> <p>Don't use PCA when: - \u274c Features are already uncorrelated - \u274c You need interpretable features (PCA creates combinations) - \u274c You have very few features - \u274c Non-linear relationships matter (use kernel PCA instead)</p>"},{"location":"module1-linear-algebra/04-eigenvalues/#key-takeaways","title":"Key Takeaways","text":"<p>Important Concepts</p> <ul> <li>Eigenvectors are special directions preserved by matrix transformation</li> <li>Eigenvalues measure scaling along eigenvector directions</li> <li>SVD decomposes any matrix into singular vectors and values</li> <li>PCA finds directions of maximum variance using eigenvectors</li> <li>PCA is fundamental for dimensionality reduction</li> </ul>"},{"location":"module1-linear-algebra/04-eigenvalues/#practical-tips","title":"Practical Tips","text":"<p>PCA Best Practices</p> <ul> <li>Always standardize features before PCA (zero mean, unit variance)</li> <li>Choose number of components based on explained variance</li> <li>Visualize explained variance with a scree plot</li> <li>Remember: PCA is sensitive to outliers</li> </ul>"},{"location":"module1-linear-algebra/04-eigenvalues/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've completed the linear algebra lessons. Now it's time to practice what you've learned.</p> <p>Complete the Exercises</p> <p>Back: Lesson 3 - Advanced Matrix Operations</p>"},{"location":"module1-linear-algebra/exercises/","title":"Module 1 Exercises: Linear Algebra","text":""},{"location":"module1-linear-algebra/exercises/#overview","title":"Overview","text":"<p>Time to put your linear algebra knowledge into practice! These exercises will help you build intuition and coding skills with vectors, matrices, and eigenvalue decomposition.</p>"},{"location":"module1-linear-algebra/exercises/#before-you-start","title":"Before You Start","text":""},{"location":"module1-linear-algebra/exercises/#setup","title":"Setup","text":"<ol> <li> <p>Open Jupyter Lab:    <pre><code>jupyter lab\n</code></pre></p> </li> <li> <p>Navigate to <code>notebooks/module1-linear-algebra/</code></p> </li> <li> <p>Start with <code>exercise1-vectors.ipynb</code></p> </li> </ol>"},{"location":"module1-linear-algebra/exercises/#exercise-format","title":"Exercise Format","text":"<p>Each exercise includes: - Learning objectives: What you'll practice - Background: Quick concept review - Tasks: Step-by-step implementation - Hints: Help when you're stuck - Expected output: What your results should look like - Reflection questions: Deepen your understanding</p>"},{"location":"module1-linear-algebra/exercises/#tips-for-success","title":"Tips for Success","text":"<p>Best Practices</p> <ul> <li>Try first, then look at hints: Struggling helps learning</li> <li>Run cells frequently: Test as you go</li> <li>Print shapes: Use <code>.shape</code> to debug dimension mismatches</li> <li>Visualize: Use matplotlib to see what's happening</li> <li>Check solutions only after trying: Compare your approach</li> </ul>"},{"location":"module1-linear-algebra/exercises/#exercise-1-vector-operations-with-numpy","title":"Exercise 1: Vector Operations with NumPy","text":"<p>Time: 1-1.5 hours</p>"},{"location":"module1-linear-algebra/exercises/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Create and manipulate vectors with NumPy</li> <li>Implement vector operations (addition, scaling, dot product)</li> <li>Compute vector norms (L1, L2, infinity)</li> <li>Calculate distances and similarities between vectors</li> <li>Visualize vectors in 2D</li> </ul>"},{"location":"module1-linear-algebra/exercises/#topics-covered","title":"Topics Covered","text":"<ul> <li>Vector creation and indexing</li> <li>Vector arithmetic operations</li> <li>Dot product and its applications</li> <li>Vector norms and normalization</li> <li>Euclidean distance</li> <li>Cosine similarity</li> <li>Vector visualization</li> </ul>"},{"location":"module1-linear-algebra/exercises/#key-functions","title":"Key Functions","text":"<pre><code>np.array(), np.dot(), np.linalg.norm(),\nnp.zeros(), np.ones(), np.random.rand()\n</code></pre>"},{"location":"module1-linear-algebra/exercises/#what-youll-build","title":"What You'll Build","text":"<ol> <li>Vector calculator: Functions for basic vector operations</li> <li>Similarity finder: Find most similar vectors in a dataset</li> <li>Visualizer: Plot vectors and their operations</li> </ol>"},{"location":"module1-linear-algebra/exercises/#exercise-2-matrix-manipulations-and-transformations","title":"Exercise 2: Matrix Manipulations and Transformations","text":"<p>Coming Soon</p> <p>This exercise is currently being developed. Check back soon!</p> <p>Time: 1.5-2 hours</p>"},{"location":"module1-linear-algebra/exercises/#what-youll-learn_1","title":"What You'll Learn","text":"<ul> <li>Create and manipulate matrices with NumPy</li> <li>Perform matrix operations (transpose, multiplication)</li> <li>Apply matrix transformations to vectors</li> <li>Work with real datasets as matrices</li> <li>Solve systems of linear equations</li> </ul>"},{"location":"module1-linear-algebra/exercises/#topics-covered_1","title":"Topics Covered","text":"<ul> <li>Matrix creation and indexing</li> <li>Matrix transpose and properties</li> <li>Matrix-vector multiplication</li> <li>Matrix-matrix multiplication</li> <li>Matrix transformations (rotation, scaling, shear)</li> <li>Dataset operations (centering, normalizing)</li> <li>Solving linear systems</li> </ul>"},{"location":"module1-linear-algebra/exercises/#key-functions_1","title":"Key Functions","text":"<pre><code>np.array(), A.T, A @ B, np.linalg.inv(),\nnp.linalg.solve(), np.linalg.det()\n</code></pre>"},{"location":"module1-linear-algebra/exercises/#what-youll-build_1","title":"What You'll Build","text":"<ol> <li>Transformation visualizer: See how matrices transform 2D space</li> <li>Dataset processor: Center and normalize real data</li> <li>Linear system solver: Solve \\(Ax = b\\) problems</li> </ol>"},{"location":"module1-linear-algebra/exercises/#exercise-3-pca-implementation-from-scratch","title":"Exercise 3: PCA Implementation from Scratch","text":"<p>Coming Soon</p> <p>This exercise is currently being developed. Check back soon!</p> <p>Time: 2-3 hours</p>"},{"location":"module1-linear-algebra/exercises/#what-youll-learn_2","title":"What You'll Learn","text":"<ul> <li>Implement PCA from scratch using NumPy</li> <li>Compute and interpret eigenvalues and eigenvectors</li> <li>Reduce dimensionality of real datasets</li> <li>Visualize principal components</li> <li>Compare with sklearn's PCA implementation</li> </ul>"},{"location":"module1-linear-algebra/exercises/#topics-covered_2","title":"Topics Covered","text":"<ul> <li>Data centering and standardization</li> <li>Covariance matrix computation</li> <li>Eigenvalue decomposition</li> <li>Selecting principal components</li> <li>Data projection onto PCs</li> <li>Explained variance analysis</li> <li>PCA visualization (2D and 3D)</li> </ul>"},{"location":"module1-linear-algebra/exercises/#key-functions_2","title":"Key Functions","text":"<pre><code>np.cov(), np.linalg.eig(), np.linalg.svd(),\nsklearn.decomposition.PCA\n</code></pre>"},{"location":"module1-linear-algebra/exercises/#what-youll-build_2","title":"What You'll Build","text":"<ol> <li>PCA from scratch: Complete implementation without sklearn</li> <li>Dimensionality reducer: Reduce high-dimensional data to 2D/3D</li> <li>Variance analyzer: Determine optimal number of components</li> <li>Comparison tool: Validate against sklearn's PCA</li> </ol>"},{"location":"module1-linear-algebra/exercises/#datasets-used","title":"Datasets Used","text":"<ul> <li>Iris dataset (4D \u2192 2D)</li> <li>Digits dataset (64D \u2192 2D for visualization)</li> <li>Custom synthetic data</li> </ul>"},{"location":"module1-linear-algebra/exercises/#solutions","title":"Solutions","text":"<p>After completing each exercise, review the solutions to: - Compare your approach with the reference implementation - Learn alternative methods - Understand best practices - Debug any issues</p> <p>Solutions Coming Soon</p> <p>Solution notebooks will be added as exercises are completed. Exercise 1 solution is currently being developed.</p> <p>Use Solutions Wisely</p> <p>Try to complete exercises independently first. Looking at solutions too early prevents deep learning. Use them for verification and learning alternative approaches.</p>"},{"location":"module1-linear-algebra/exercises/#assessment-questions","title":"Assessment Questions","text":"<p>After completing all exercises, test your understanding:</p> <ol> <li>Conceptual</li> <li>When would you use L1 norm vs L2 norm?</li> <li>Why is matrix multiplication not commutative?</li> <li>What does it mean if two eigenvectors are orthogonal?</li> <li> <p>When should you use PCA in a real project?</p> </li> <li> <p>Practical</p> </li> <li>How do you check if a matrix is invertible?</li> <li>What's the fastest way to compute pairwise distances in NumPy?</li> <li>How do you choose the number of principal components?</li> <li> <p>What happens if you don't center data before PCA?</p> </li> <li> <p>Debugging</p> </li> <li>You get \"ValueError: shapes not aligned\". What's wrong?</li> <li>Your PCA shows negative eigenvalues. Why?</li> <li>Matrix inverse fails. What could be the cause?</li> </ol>"},{"location":"module1-linear-algebra/exercises/#reflection-questions","title":"Reflection Questions","text":"<p>Think deeply about what you've learned:</p> <ol> <li>How does understanding linear algebra help you understand machine learning better?</li> <li>What was the most surprising thing you learned?</li> <li>Which concept was hardest to grasp? How did you overcome it?</li> <li>How would you explain PCA to someone with no ML background?</li> <li>Where could you apply these techniques in your work or projects?</li> </ol>"},{"location":"module1-linear-algebra/exercises/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<p>Watch Out For</p> <ul> <li>Not checking dimensions: Always print <code>.shape</code> before operations</li> <li>Confusing dot product with element-wise: Use <code>@</code> for dot, <code>*</code> for element-wise</li> <li>Forgetting to center data: PCA requires zero-mean data</li> <li>Wrong axis in numpy operations: <code>axis=0</code> is rows, <code>axis=1</code> is columns</li> <li>Not normalizing features: Features with different scales can dominate PCA</li> </ul>"},{"location":"module1-linear-algebra/exercises/#going-further","title":"Going Further","text":""},{"location":"module1-linear-algebra/exercises/#challenge-exercises","title":"Challenge Exercises","text":"<p>Want more practice? Try these:</p> <ol> <li>Implement SVD-based PCA: Use SVD instead of eigendecomposition</li> <li>Incremental PCA: Handle datasets that don't fit in memory</li> <li>Kernel PCA: Extend PCA to capture non-linear relationships</li> <li>t-SNE: Implement t-SNE for visualization (uses eigenvalues!)</li> <li>Image compression: Use SVD to compress images</li> </ol>"},{"location":"module1-linear-algebra/exercises/#real-world-projects","title":"Real-World Projects","text":"<p>Apply your skills:</p> <ol> <li>Face recognition: Use PCA for eigenfaces</li> <li>Recommender system: Matrix factorization for collaborative filtering</li> <li>Data visualization: Reduce high-dimensional data for plotting</li> <li>Feature engineering: Use PCA for feature extraction</li> </ol>"},{"location":"module1-linear-algebra/exercises/#next-module","title":"Next Module","text":"<p>Once you're comfortable with linear algebra, you're ready for regression!</p> <p>Continue to Module 2: Regression Algorithms</p>"},{"location":"module1-linear-algebra/exercises/#help-and-support","title":"Help and Support","text":"<p>Stuck on an exercise?</p> <ol> <li>Re-read the relevant lesson</li> <li>Check the hints in the notebook</li> <li>Search the error message online</li> <li>Look at the solution for that specific part</li> <li>Open an issue on GitHub</li> </ol> <p>Found a bug or have a suggestion?</p> <p>Please open an issue or submit a pull request!</p> <p>Good luck with the exercises! Remember: struggle is part of learning. Don't give up!</p>"},{"location":"module2-regression/","title":"Module 2: Regression Algorithms","text":""},{"location":"module2-regression/#overview","title":"Overview","text":"<p>Regression is one of the most fundamental tasks in machine learning - predicting a continuous value based on input features. In this module, you'll learn regression algorithms from the ground up, starting with simple linear regression and progressing to advanced regularization techniques.</p>"},{"location":"module2-regression/#why-regression-matters-in-machine-learning","title":"Why Regression Matters in Machine Learning","text":"<p>Regression algorithms are everywhere: - Predicting prices: House prices, stock prices, product demand - Forecasting: Sales, weather, energy consumption - Risk assessment: Credit scores, insurance premiums - Scientific modeling: Relationships between variables - Foundation for other algorithms: Many ML techniques build on regression</p> <p>Core ML Skill</p> <p>\"Linear regression might seem simple, but it's the foundation of modern deep learning. Every neural network performs learned linear transformations followed by non-linearities.\"</p>"},{"location":"module2-regression/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this module, you will be able to:</p> <ul> <li>\u2705 Implement linear regression from scratch using NumPy</li> <li>\u2705 Understand and implement gradient descent optimization</li> <li>\u2705 Apply logistic regression for classification problems</li> <li>\u2705 Use L1 and L2 regularization to prevent overfitting</li> <li>\u2705 Evaluate model performance with appropriate metrics</li> <li>\u2705 Use scikit-learn for production-ready implementations</li> </ul>"},{"location":"module2-regression/#prerequisites","title":"Prerequisites","text":"<ul> <li>Module 1 completed: Vector and matrix operations</li> <li>Python: Comfortable with NumPy and basic plotting</li> <li>Basic calculus: Understanding derivatives helps but not required</li> </ul>"},{"location":"module2-regression/#module-structure","title":"Module Structure","text":""},{"location":"module2-regression/#lesson-1-linear-regression","title":"Lesson 1: Linear Regression","text":"<p>Time: 60 minutes</p> <p>Learn the foundation of supervised learning with linear regression.</p> <ul> <li>Problem formulation and assumptions</li> <li>Simple linear regression (one feature)</li> <li>Multiple linear regression</li> <li>Normal equation (closed-form solution)</li> <li>Implementation with NumPy and scikit-learn</li> <li>Model evaluation metrics (MSE, MAE, R\u00b2)</li> </ul> <p>Start Lesson 1</p>"},{"location":"module2-regression/#lesson-2-gradient-descent","title":"Lesson 2: Gradient Descent","text":"<p>Time: 60 minutes</p> <p>Master the optimization algorithm that powers most of machine learning.</p> <ul> <li>Cost functions (MSE, MAE, RMSE)</li> <li>Gradient descent intuition and mathematics</li> <li>Batch, stochastic, and mini-batch variants</li> <li>Learning rate selection and tuning</li> <li>Convergence criteria and monitoring</li> <li>Visualization of optimization landscape</li> </ul> <p>Start Lesson 2</p>"},{"location":"module2-regression/#lesson-3-logistic-regression","title":"Lesson 3: Logistic Regression","text":"<p>Time: 60 minutes</p> <p>Extend regression to classification problems with logistic regression.</p> <ul> <li>From regression to classification</li> <li>Sigmoid function and probability interpretation</li> <li>Log loss (binary cross-entropy)</li> <li>Gradient descent for logistic regression</li> <li>Multi-class classification (One-vs-Rest, Softmax)</li> <li>Decision boundaries and visualization</li> </ul> <p>Start Lesson 3</p>"},{"location":"module2-regression/#lesson-4-regularization","title":"Lesson 4: Regularization","text":"<p>Time: 60 minutes</p> <p>Learn to control model complexity and prevent overfitting.</p> <ul> <li>Overfitting and underfitting</li> <li>Bias-variance tradeoff</li> <li>L1 regularization (Lasso) for feature selection</li> <li>L2 regularization (Ridge) for coefficient shrinkage</li> <li>Elastic Net (combining L1 and L2)</li> <li>Hyperparameter tuning with cross-validation</li> </ul> <p>Start Lesson 4</p>"},{"location":"module2-regression/#exercises","title":"Exercises","text":"<p>Time: 6-8 hours</p> <p>Apply what you've learned through hands-on implementation.</p> <ul> <li>Exercise 1: Linear regression from scratch and with sklearn</li> <li>Exercise 2: Gradient descent visualization and tuning</li> <li>Exercise 3: Logistic regression for binary and multi-class classification</li> <li>Exercise 4: Comparing regularization techniques</li> </ul> <p>View Exercises</p>"},{"location":"module2-regression/#key-concepts","title":"Key Concepts","text":"Concept Description Formula Linear Model Weighted sum of features \\(\\hat{y} = w^Tx + b\\) Cost Function Measure of prediction error \\(J(w) = \\frac{1}{2m}\\sum(y - \\hat{y})^2\\) Gradient Descent Iterative optimization \\(w := w - \\alpha\\nabla J(w)\\) Sigmoid Squash to probability \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) L2 Regularization Penalty on weights \\(J(w) + \\frac{\\lambda}{2}\\|w\\|^2\\)"},{"location":"module2-regression/#what-youll-build","title":"What You'll Build","text":"<p>By the end of this module, you'll have implemented:</p> <ol> <li>Linear Regression from Scratch: Using only NumPy</li> <li>Gradient Descent Visualizer: See optimization in action</li> <li>Logistic Regression Classifier: Binary and multi-class</li> <li>Regularization Comparison Tool: Compare Lasso, Ridge, Elastic Net</li> <li>Complete ML Pipeline: Data preprocessing \u2192 Training \u2192 Evaluation</li> </ol>"},{"location":"module2-regression/#datasets-youll-use","title":"Datasets You'll Use","text":"<ul> <li>California Housing: Predict house prices (regression)</li> <li>Boston Housing: Classic regression dataset</li> <li>Iris Flowers: Multi-class classification</li> <li>Breast Cancer Wisconsin: Binary classification</li> <li>Synthetic Data: Understand algorithms with controlled examples</li> </ul>"},{"location":"module2-regression/#tips-for-success","title":"Tips for Success","text":"<p>Understanding Loss Functions</p> <p>Plot your cost function over iterations. A decreasing curve means learning is working!</p> <p>Feature Scaling</p> <p>Always normalize/standardize features before applying gradient descent. It makes convergence much faster.</p> <p>Start Simple</p> <p>Implement algorithms on simple 2D data first, then scale to real datasets.</p> <p>Common Pitfall</p> <p>Learning rate too high \u2192 divergence. Too low \u2192 slow convergence. Plot and experiment!</p>"},{"location":"module2-regression/#estimated-time","title":"Estimated Time","text":"<ul> <li>Reading lessons: 4-5 hours</li> <li>Completing exercises: 6-8 hours</li> <li>Total: 10-13 hours</li> </ul>"},{"location":"module2-regression/#real-world-applications","title":"Real-World Applications","text":"<p>After this module, you'll understand the algorithms behind:</p> <ul> <li>Price Prediction: Real estate, e-commerce, financial markets</li> <li>Demand Forecasting: Inventory management, resource planning</li> <li>Risk Modeling: Credit scoring, insurance pricing</li> <li>Medical Diagnosis: Disease risk prediction</li> <li>Marketing: Customer conversion probability</li> <li>Recommendation: User preference prediction</li> </ul>"},{"location":"module2-regression/#from-theory-to-practice","title":"From Theory to Practice","text":"<p>This module emphasizes implementation:</p> <ol> <li>Understand the math: Learn the equations</li> <li>Code from scratch: Implement with NumPy</li> <li>Use libraries: Apply scikit-learn for production</li> <li>Compare results: Verify your implementation matches sklearn</li> <li>Apply to real data: Work with actual datasets</li> </ol>"},{"location":"module2-regression/#ready-to-start","title":"Ready to Start?","text":"<p>Let's begin with the foundation of supervised learning - linear regression!</p> <p>Start Lesson 1: Linear Regression</p> <p>Or review Module 1 first</p> <p>Questions? Open an issue on GitHub.</p>"},{"location":"module2-regression/01-linear-regression/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module2-regression/02-gradient-descent/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module2-regression/03-logistic-regression/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module2-regression/04-regularization/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module2-regression/exercises/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module3-trees/","title":"Module 3: Tree-Based Algorithms","text":""},{"location":"module3-trees/#overview","title":"Overview","text":"<p>Tree-based algorithms are among the most powerful and widely-used techniques in machine learning. From simple decision trees to sophisticated ensemble methods like Random Forest and XGBoost, these algorithms excel at both classification and regression tasks while maintaining interpretability.</p>"},{"location":"module3-trees/#why-tree-based-algorithms-matter","title":"Why Tree-Based Algorithms Matter","text":"<p>Industry relies heavily on tree-based methods: - Kaggle competitions: XGBoost and Random Forest dominate leaderboards - Production systems: Fast inference and easy deployment - Interpretability: Feature importance and tree visualization - Versatility: Handle non-linear relationships, mixed data types - Robustness: Less sensitive to outliers than linear models</p>"},{"location":"module3-trees/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>\u2705 Understand decision tree fundamentals and splitting criteria</li> <li>\u2705 Implement decision trees from scratch</li> <li>\u2705 Master ensemble methods (Bagging and Boosting)</li> <li>\u2705 Apply Random Forest for robust predictions</li> <li>\u2705 Use XGBoost for high-performance ML</li> <li>\u2705 Interpret models with feature importance</li> <li>\u2705 Tune hyperparameters effectively</li> </ul>"},{"location":"module3-trees/#prerequisites","title":"Prerequisites","text":"<ul> <li>Modules 1 &amp; 2 completed</li> <li>Understanding of classification and regression</li> <li>Familiarity with model evaluation metrics</li> </ul>"},{"location":"module3-trees/#module-structure","title":"Module Structure","text":""},{"location":"module3-trees/#lesson-1-decision-trees-60-min","title":"Lesson 1: Decision Trees (60 min)","text":"<ul> <li>Tree fundamentals and recursive partitioning</li> <li>Splitting criteria: Gini impurity, entropy, information gain</li> <li>Handling continuous and categorical features</li> <li>Tree visualization and interpretation</li> </ul> <p>Start Lesson 1</p>"},{"location":"module3-trees/#lesson-2-tree-algorithms-45-min","title":"Lesson 2: Tree Algorithms (45 min)","text":"<ul> <li>ID3, C4.5, and CART algorithms</li> <li>Pruning techniques (pre-pruning, post-pruning)</li> <li>Handling missing values</li> <li>Regression trees</li> </ul> <p>Start Lesson 2</p>"},{"location":"module3-trees/#lesson-3-random-forest-60-min","title":"Lesson 3: Random Forest (60 min)","text":"<ul> <li>Bootstrap aggregating (Bagging)</li> <li>Feature randomness</li> <li>Out-of-bag error estimation</li> <li>Feature importance analysis</li> </ul> <p>Start Lesson 3</p>"},{"location":"module3-trees/#lesson-4-boosting-45-min","title":"Lesson 4: Boosting (45 min)","text":"<ul> <li>Boosting vs Bagging</li> <li>AdaBoost algorithm</li> <li>Gradient Boosting Machines</li> <li>Loss functions and weak learners</li> </ul> <p>Start Lesson 4</p>"},{"location":"module3-trees/#lesson-5-xgboost-60-min","title":"Lesson 5: XGBoost (60 min)","text":"<ul> <li>XGBoost improvements and optimizations</li> <li>Regularization in XGBoost</li> <li>Hyperparameter tuning guide</li> <li>Real-world applications</li> </ul> <p>Start Lesson 5</p>"},{"location":"module3-trees/#exercises-6-8-hours","title":"Exercises (6-8 hours)","text":"<ul> <li>Decision tree implementation and visualization</li> <li>Random Forest for classification</li> <li>XGBoost for regression and classification</li> <li>Model comparison and hyperparameter tuning</li> </ul> <p>View Exercises</p>"},{"location":"module3-trees/#key-algorithms","title":"Key Algorithms","text":"Algorithm Type Strengths Use Cases Decision Tree Single tree Interpretable, fast Baseline, feature selection Random Forest Bagging ensemble Robust, reduces variance General purpose XGBoost Boosting ensemble Highest accuracy Competitions, production"},{"location":"module3-trees/#datasets","title":"Datasets","text":"<ul> <li>Titanic (classification)</li> <li>Wine Quality (multi-class)</li> <li>California Housing (regression)</li> <li>Credit Default (imbalanced classification)</li> </ul>"},{"location":"module3-trees/#estimated-time-12-15-hours","title":"Estimated Time: 12-15 hours","text":"<p>Start Learning</p>"},{"location":"module3-trees/01-decision-trees/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module3-trees/02-tree-algorithms/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module3-trees/03-random-forest/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module3-trees/04-boosting/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module3-trees/05-xgboost/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module3-trees/exercises/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module4-neural-networks/","title":"Module 4: Neural Networks","text":""},{"location":"module4-neural-networks/#overview","title":"Overview","text":"<p>Neural networks are the foundation of modern deep learning. In this module, you'll build neural networks from scratch using NumPy to understand how they work, then use PyTorch to implement production-ready deep learning models.</p>"},{"location":"module4-neural-networks/#why-neural-networks-matter","title":"Why Neural Networks Matter","text":"<p>Deep learning powers today's AI revolution: - Computer Vision: Image classification, object detection, segmentation - Natural Language Processing: Translation, chatbots, text generation - Speech Recognition: Voice assistants, transcription - Generative AI: DALL-E, ChatGPT, Stable Diffusion - Reinforcement Learning: Game AI, robotics, autonomous systems</p>"},{"location":"module4-neural-networks/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>\u2705 Understand the perceptron and activation functions</li> <li>\u2705 Build feedforward neural networks</li> <li>\u2705 Master backpropagation algorithm</li> <li>\u2705 Implement neural networks from scratch with NumPy</li> <li>\u2705 Use PyTorch for modern deep learning</li> <li>\u2705 Apply regularization and optimization techniques</li> <li>\u2705 Train and evaluate neural network models</li> </ul>"},{"location":"module4-neural-networks/#prerequisites","title":"Prerequisites","text":"<ul> <li>Modules 1-3 completed</li> <li>Strong understanding of matrix operations</li> <li>Gradient descent and backpropagation concepts</li> <li>Python and NumPy proficiency</li> </ul>"},{"location":"module4-neural-networks/#module-structure","title":"Module Structure","text":""},{"location":"module4-neural-networks/#lesson-1-the-perceptron-45-min","title":"Lesson 1: The Perceptron (45 min)","text":"<ul> <li>Biological neuron inspiration</li> <li>Perceptron model and algorithm</li> <li>Activation functions: sigmoid, tanh, ReLU</li> <li>Linear separability and limitations</li> </ul> <p>Start Lesson 1</p>"},{"location":"module4-neural-networks/#lesson-2-feedforward-networks-60-min","title":"Lesson 2: Feedforward Networks (60 min)","text":"<ul> <li>Multi-layer perceptron (MLP) architecture</li> <li>Hidden layers and network depth</li> <li>Forward propagation mathematics</li> <li>Universal approximation theorem</li> </ul> <p>Start Lesson 2</p>"},{"location":"module4-neural-networks/#lesson-3-backpropagation-75-min","title":"Lesson 3: Backpropagation (75 min)","text":"<ul> <li>Chain rule and gradient computation</li> <li>Backpropagation algorithm step-by-step</li> <li>Computing gradients layer by layer</li> <li>Weight updates and learning</li> </ul> <p>Start Lesson 3</p>"},{"location":"module4-neural-networks/#lesson-4-numpy-implementation-90-min","title":"Lesson 4: NumPy Implementation (90 min)","text":"<ul> <li>Complete neural network from scratch</li> <li>Modular design: layers, activations, losses</li> <li>Training loop and mini-batch processing</li> <li>Debugging neural networks</li> </ul> <p>Start Lesson 4</p>"},{"location":"module4-neural-networks/#lesson-5-pytorch-basics-75-min","title":"Lesson 5: PyTorch Basics (75 min)","text":"<ul> <li>Introduction to PyTorch</li> <li>Tensors and autograd</li> <li>Building models with nn.Module</li> <li>Optimizers and loss functions</li> <li>Training and evaluation loops</li> </ul> <p>Start Lesson 5</p>"},{"location":"module4-neural-networks/#lesson-6-pytorch-advanced-90-min","title":"Lesson 6: PyTorch Advanced (90 min)","text":"<ul> <li>Custom datasets and DataLoaders</li> <li>Model checkpointing and saving</li> <li>Learning rate scheduling</li> <li>Regularization: dropout, batch normalization</li> <li>Transfer learning basics</li> </ul> <p>Start Lesson 6</p>"},{"location":"module4-neural-networks/#exercises-8-10-hours","title":"Exercises (8-10 hours)","text":"<ul> <li>Perceptron implementation from scratch</li> <li>Multi-layer network with NumPy</li> <li>PyTorch neural network for MNIST</li> <li>Classification with regularization</li> <li>Advanced PyTorch techniques</li> </ul> <p>View Exercises</p>"},{"location":"module4-neural-networks/#learning-path","title":"Learning Path","text":"<pre><code>1. Understand single neuron (Perceptron)\n   \u2193\n2. Build multi-layer networks\n   \u2193\n3. Learn backpropagation math\n   \u2193\n4. Implement from scratch (NumPy)\n   \u2193\n5. Use modern framework (PyTorch)\n   \u2193\n6. Apply advanced techniques\n</code></pre>"},{"location":"module4-neural-networks/#key-concepts","title":"Key Concepts","text":"Concept Purpose Implementation Activation Functions Introduce non-linearity ReLU, sigmoid, tanh Backpropagation Compute gradients Chain rule recursively Optimization Update weights SGD, Adam, RMSprop Regularization Prevent overfitting Dropout, batch norm, L2"},{"location":"module4-neural-networks/#datasets","title":"Datasets","text":"<ul> <li>MNIST: Handwritten digit recognition (10 classes, 28x28 images)</li> <li>Fashion-MNIST: Clothing classification (10 classes)</li> <li>CIFAR-10: Natural images (10 classes, 32x32 color)</li> <li>Custom datasets: Build your own data loaders</li> </ul>"},{"location":"module4-neural-networks/#tools","title":"Tools","text":"<ul> <li>NumPy: From-scratch implementation</li> <li>PyTorch: Modern deep learning framework</li> <li>Matplotlib: Visualize learning and predictions</li> <li>torchvision: Pre-built datasets and models</li> </ul>"},{"location":"module4-neural-networks/#two-part-approach","title":"Two-Part Approach","text":""},{"location":"module4-neural-networks/#part-1-numpy-implementation-deep-understanding","title":"Part 1: NumPy Implementation (Deep Understanding)","text":"<p>Build everything from scratch to understand: - How forward propagation works - How backpropagation computes gradients - How weights are updated - Why deep learning works</p>"},{"location":"module4-neural-networks/#part-2-pytorch-practical-skills","title":"Part 2: PyTorch (Practical Skills)","text":"<p>Use industry-standard tools for: - Automatic differentiation - GPU acceleration - Scalable training - Production deployment</p>"},{"location":"module4-neural-networks/#tips-for-success","title":"Tips for Success","text":"<p>Start Simple</p> <p>Begin with small networks and simple datasets. MNIST before CIFAR-10!</p> <p>Check Gradients</p> <p>Use gradient checking to verify your backpropagation implementation.</p> <p>Monitor Training</p> <p>Always plot training and validation loss. Watch for overfitting!</p> <p>Use GPU</p> <p>PyTorch makes GPU usage easy. Even free Colab GPUs help!</p> <p>Common Mistakes</p> <ul> <li>Forgetting to zero gradients in PyTorch</li> <li>Wrong tensor dimensions</li> <li>Not shuffling data</li> <li>Learning rate too high/low</li> </ul>"},{"location":"module4-neural-networks/#estimated-time-16-20-hours","title":"Estimated Time: 16-20 hours","text":""},{"location":"module4-neural-networks/#real-world-applications","title":"Real-World Applications","text":"<p>After this module: - Image Classification: Build custom classifiers - Time Series: Stock prediction, forecasting - NLP Tasks: Sentiment analysis, text classification - Anomaly Detection: Fraud detection, monitoring - Embeddings: Feature learning for other tasks</p>"},{"location":"module4-neural-networks/#beyond-this-module","title":"Beyond This Module","text":"<p>Paths to explore further: - Convolutional Neural Networks (CNNs): Computer vision - Recurrent Neural Networks (RNNs): Sequential data - Transformers: State-of-the-art NLP - GANs: Generative models - Reinforcement Learning: Decision making</p>"},{"location":"module4-neural-networks/#ready-to-start","title":"Ready to Start?","text":"<p>Let's begin by understanding the perceptron - the building block of neural networks!</p> <p>Start Lesson 1: The Perceptron</p> <p>Or review Module 3 first</p> <p>Questions? Open an issue on GitHub.</p>"},{"location":"module4-neural-networks/01-perceptron/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module4-neural-networks/02-feedforward-networks/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module4-neural-networks/03-backpropagation/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module4-neural-networks/04-numpy-implementation/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module4-neural-networks/05-pytorch-basics/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module4-neural-networks/06-pytorch-advanced/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"module4-neural-networks/exercises/","title":"Under Construction","text":"<p>Coming Soon</p> <p>This lesson is currently being developed. Check back soon for comprehensive content on this topic!</p> <p>In the meantime, you can:</p> <ul> <li>Explore other available lessons in this module</li> <li>Check out the Resources section</li> <li>Visit the module overview page for a complete roadmap</li> </ul> <p>Want to contribute? This is an open-source project! If you'd like to help create content for this lesson, see our repository on GitHub.</p>"},{"location":"resources/datasets/","title":"Datasets","text":"<p>This page documents all datasets used in ML101 and how to access them.</p>"},{"location":"resources/datasets/#built-in-datasets","title":"Built-in Datasets","text":"<p>All datasets are available through scikit-learn or built-in libraries - no downloads required!</p>"},{"location":"resources/datasets/#classification-datasets","title":"Classification Datasets","text":""},{"location":"resources/datasets/#iris-dataset","title":"Iris Dataset","text":"<p>Module: 1, 2 Task: Multi-class classification (3 classes) Samples: 150 Features: 4 (sepal length, sepal width, petal length, petal width)</p> <pre><code>from sklearn.datasets import load_iris\niris = load_iris()\nX, y = iris.data, iris.target\n</code></pre>"},{"location":"resources/datasets/#breast-cancer-wisconsin","title":"Breast Cancer Wisconsin","text":"<p>Module: 2 Task: Binary classification Samples: 569 Features: 30 (computed from digitized images)</p> <pre><code>from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX, y = cancer.data, cancer.target\n</code></pre>"},{"location":"resources/datasets/#digits-dataset","title":"Digits Dataset","text":"<p>Module: 1, 4 Task: Digit recognition (10 classes) Samples: 1,797 Features: 64 (8x8 pixel images)</p> <pre><code>from sklearn.datasets import load_digits\ndigits = load_digits()\nX, y = digits.data, digits.target\n</code></pre>"},{"location":"resources/datasets/#regression-datasets","title":"Regression Datasets","text":""},{"location":"resources/datasets/#california-housing","title":"California Housing","text":"<p>Module: 2, 3 Task: Regression (predict median house value) Samples: 20,640 Features: 8</p> <pre><code>from sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n</code></pre>"},{"location":"resources/datasets/#boston-housing-deprecated-but-educational","title":"Boston Housing (Deprecated but Educational)","text":"<p>Module: 2 Task: Regression (predict house prices) Samples: 506 Features: 13</p> <pre><code># Note: Deprecated in sklearn, included for learning\nfrom sklearn.datasets import load_boston\nboston = load_boston()\n</code></pre>"},{"location":"resources/datasets/#deep-learning-datasets","title":"Deep Learning Datasets","text":""},{"location":"resources/datasets/#mnist","title":"MNIST","text":"<p>Module: 4 Task: Handwritten digit recognition Samples: 70,000 (60k train, 10k test) Features: 28x28 grayscale images</p> <pre><code>from torchvision import datasets\nmnist = datasets.MNIST(root='./data', download=True)\n</code></pre>"},{"location":"resources/datasets/#fashion-mnist","title":"Fashion-MNIST","text":"<p>Module: 4 Task: Clothing classification Samples: 70,000 Features: 28x28 grayscale images (10 classes)</p> <pre><code>from torchvision import datasets\nfashion = datasets.FashionMNIST(root='./data', download=True)\n</code></pre>"},{"location":"resources/datasets/#cifar-10","title":"CIFAR-10","text":"<p>Module: 4 Task: Image classification Samples: 60,000 Features: 32x32 color images (10 classes)</p> <pre><code>from torchvision import datasets\ncifar10 = datasets.CIFAR10(root='./data', download=True)\n</code></pre>"},{"location":"resources/datasets/#generating-synthetic-data","title":"Generating Synthetic Data","text":""},{"location":"resources/datasets/#linear-regression-data","title":"Linear Regression Data","text":"<pre><code>from sklearn.datasets import make_regression\n\nX, y = make_regression(\n    n_samples=100,\n    n_features=1,\n    noise=20,\n    random_state=42\n)\n</code></pre>"},{"location":"resources/datasets/#classification-data","title":"Classification Data","text":"<pre><code>from sklearn.datasets import make_classification, make_moons, make_circles\n\n# Linearly separable\nX, y = make_classification(n_samples=100, n_features=2,\n                           n_redundant=0, n_clusters_per_class=1)\n\n# Non-linear (moons)\nX, y = make_moons(n_samples=100, noise=0.1)\n\n# Non-linear (circles)\nX, y = make_circles(n_samples=100, noise=0.05, factor=0.5)\n</code></pre>"},{"location":"resources/datasets/#clustering-data","title":"Clustering Data","text":"<pre><code>from sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=300, centers=3, n_features=2)\n</code></pre>"},{"location":"resources/datasets/#dataset-information","title":"Dataset Information","text":""},{"location":"resources/datasets/#accessing-metadata","title":"Accessing Metadata","text":"<pre><code># Get feature names\nprint(iris.feature_names)\n\n# Get target names\nprint(iris.target_names)\n\n# Get description\nprint(iris.DESCR)\n</code></pre>"},{"location":"resources/datasets/#dataset-properties","title":"Dataset Properties","text":"<pre><code>import numpy as np\n\nprint(f\"Shape: {X.shape}\")\nprint(f\"Number of samples: {X.shape[0]}\")\nprint(f\"Number of features: {X.shape[1]}\")\nprint(f\"Number of classes: {len(np.unique(y))}\")\nprint(f\"Class distribution: {np.bincount(y)}\")\n</code></pre>"},{"location":"resources/datasets/#data-preprocessing","title":"Data Preprocessing","text":""},{"location":"resources/datasets/#train-test-split","title":"Train-Test Split","text":"<pre><code>from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n</code></pre>"},{"location":"resources/datasets/#standardization","title":"Standardization","text":"<pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre>"},{"location":"resources/datasets/#normalization","title":"Normalization","text":"<pre><code>from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_normalized = scaler.fit_transform(X)\n</code></pre>"},{"location":"resources/datasets/#external-datasets-optional","title":"External Datasets (Optional)","text":"<p>For advanced projects beyond the course:</p> <ul> <li>Kaggle: kaggle.com/datasets</li> <li>UCI ML Repository: archive.ics.uci.edu/ml</li> <li>HuggingFace Datasets: huggingface.co/datasets</li> <li>TensorFlow Datasets: tensorflow.org/datasets</li> </ul>"},{"location":"resources/datasets/#tips","title":"Tips","text":"<p>Always Explore First</p> <p>Before modeling, always: - Check dataset shape and types - Look for missing values - Visualize distributions - Check for class imbalance</p> <p>Reproducibility</p> <p>Use <code>random_state</code> parameter for reproducible results: <pre><code>train_test_split(X, y, random_state=42)\n</code></pre></p> <p>Back to Home</p>"},{"location":"resources/further-reading/","title":"Further Reading","text":"<p>Resources to deepen your machine learning knowledge beyond ML101.</p>"},{"location":"resources/further-reading/#books","title":"Books","text":""},{"location":"resources/further-reading/#beginner-friendly","title":"Beginner-Friendly","text":"<p>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aur\u00e9lien G\u00e9ron - Practical, code-focused approach - Covers scikit-learn and deep learning - Excellent for practitioners</p> <p>Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani - Free PDF available - Mathematical but accessible - R code examples (Python version also available)</p>"},{"location":"resources/further-reading/#intermediate","title":"Intermediate","text":"<p>Pattern Recognition and Machine Learning by Christopher Bishop - Comprehensive coverage - More mathematical - Classic textbook</p> <p>Machine Learning: A Probabilistic Perspective by Kevin Murphy - Probability-focused approach - Comprehensive and rigorous - Great reference book</p>"},{"location":"resources/further-reading/#advanced","title":"Advanced","text":"<p>Deep Learning by Goodfellow, Bengio, Courville - The deep learning bible - Free online: deeplearningbook.org - Mathematical and thorough</p> <p>Elements of Statistical Learning by Hastie, Tibshirani, Friedman - Advanced statistical methods - Free PDF available - More mathematical than ISL</p>"},{"location":"resources/further-reading/#online-courses","title":"Online Courses","text":""},{"location":"resources/further-reading/#video-courses","title":"Video Courses","text":"<p>Andrew Ng's Machine Learning (Coursera) - Classic introduction to ML - Clear explanations - Good balance of theory and practice</p> <p>Fast.ai - Practical Deep Learning - Top-down teaching approach - Code-first, PyTorch-based - Free and excellent</p> <p>Stanford CS229 - Machine Learning - Full lecture videos on YouTube - Comprehensive and rigorous - Math-heavy but worth it</p>"},{"location":"resources/further-reading/#interactive","title":"Interactive","text":"<p>Kaggle Learn - Short, practical courses - Covers pandas, ML, deep learning - Free and hands-on</p> <p>DataCamp / Codecademy - Interactive coding exercises - Various ML topics - Subscription-based</p>"},{"location":"resources/further-reading/#video-resources","title":"Video Resources","text":""},{"location":"resources/further-reading/#youtube-channels","title":"YouTube Channels","text":"<p>3Blue1Brown - Beautiful visualizations - Linear algebra and calculus series - Builds deep intuition</p> <p>StatQuest with Josh Starmer - Statistics and ML concepts - Clear, fun explanations - Great for intuition</p> <p>Two Minute Papers - Latest ML research - Accessible summaries - Stay current with field</p> <p>Sentdex - Python and ML tutorials - Practical projects - Large library of content</p>"},{"location":"resources/further-reading/#specific-series","title":"Specific Series","text":"<p>MIT OpenCourseWare - Linear Algebra (Gilbert Strang) - Complete linear algebra course - Legendary professor - Free on YouTube</p> <p>Stanford CS231n - CNNs for Visual Recognition - Computer vision course - Lecture videos available - Excellent for deep learning</p>"},{"location":"resources/further-reading/#websites-and-blogs","title":"Websites and Blogs","text":""},{"location":"resources/further-reading/#learning-resources","title":"Learning Resources","text":"<p>Distill.pub - Interactive ML explanations - Beautiful visualizations - High-quality content</p> <p>Towards Data Science (Medium) - Wide range of ML articles - Various skill levels - Active community</p> <p>Machine Learning Mastery - Practical tutorials - Code examples - Beginner-friendly</p> <p>Papers With Code - Latest research papers - Code implementations - Benchmarks and datasets</p>"},{"location":"resources/further-reading/#reference","title":"Reference","text":"<p>Scikit-learn Documentation - Excellent user guide - API reference - Many examples</p> <p>PyTorch Documentation - Tutorials and examples - API reference - Active community</p> <p>NumPy Documentation - User guide - API reference - Essential for ML</p>"},{"location":"resources/further-reading/#research-papers","title":"Research Papers","text":""},{"location":"resources/further-reading/#classic-papers-highly-readable","title":"Classic Papers (Highly Readable)","text":"<ol> <li>\"A Few Useful Things to Know About Machine Learning\" - Pedro Domingos</li> <li>\"Machine Learning: The High-Interest Credit Card of Technical Debt\" - Sculley et al.</li> <li>\"Attention Is All You Need\" - Vaswani et al. (Transformers)</li> <li>\"ImageNet Classification with Deep CNNs\" - Krizhevsky et al. (AlexNet)</li> </ol>"},{"location":"resources/further-reading/#where-to-find-papers","title":"Where to Find Papers","text":"<ul> <li>ArXiv.org - Pre-print server for research papers</li> <li>Papers With Code - Papers with implementations</li> <li>Google Scholar - Search academic papers</li> <li>Semantic Scholar - AI-powered paper search</li> </ul>"},{"location":"resources/further-reading/#competitions-and-practice","title":"Competitions and Practice","text":""},{"location":"resources/further-reading/#kaggle","title":"Kaggle","text":"<p>Getting Started Competitions - Titanic: Binary classification - House Prices: Regression - Digit Recognizer: Image classification</p> <p>Benefits: - Real datasets - Learn from others' code - Community discussions - Build portfolio</p>"},{"location":"resources/further-reading/#other-platforms","title":"Other Platforms","text":"<p>DrivenData - Social impact competitions - Similar to Kaggle - Interesting problems</p> <p>AIcrowd - Research-focused challenges - Cutting-edge problems - Academic community</p>"},{"location":"resources/further-reading/#tools-and-libraries","title":"Tools and Libraries","text":""},{"location":"resources/further-reading/#essential-python-libraries","title":"Essential Python Libraries","text":"<pre><code># Core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn import *\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Deep Learning\nimport torch\nimport tensorflow as tf\nfrom transformers import *  # HuggingFace\n\n# Utilities\nimport joblib  # Save models\nfrom tqdm import tqdm  # Progress bars\n</code></pre>"},{"location":"resources/further-reading/#visualization","title":"Visualization","text":"<ul> <li>Matplotlib: Standard plotting</li> <li>Seaborn: Statistical visualizations</li> <li>Plotly: Interactive plots</li> <li>Altair: Declarative visualization</li> <li>TensorBoard: Training visualization</li> </ul>"},{"location":"resources/further-reading/#experiment-tracking","title":"Experiment Tracking","text":"<ul> <li>Weights &amp; Biases (wandb)</li> <li>MLflow</li> <li>Neptune.ai</li> <li>Comet.ml</li> </ul>"},{"location":"resources/further-reading/#communities","title":"Communities","text":""},{"location":"resources/further-reading/#forums-and-qa","title":"Forums and Q&amp;A","text":"<p>Stack Overflow - Programming questions - Large community - Good for debugging</p> <p>Cross Validated (Stats StackExchange) - Statistical questions - ML theory - Expert answers</p> <p>r/MachineLearning (Reddit) - Research discussions - News and papers - Active community</p> <p>r/learnmachinelearning (Reddit) - Beginner-friendly - Project feedback - Resource sharing</p>"},{"location":"resources/further-reading/#discordslack","title":"Discord/Slack","text":"<p>Many ML communities on Discord: - Fast.ai - HuggingFace - Various university ML clubs</p>"},{"location":"resources/further-reading/#newsletters","title":"Newsletters","text":"<p>Stay updated with ML news:</p> <p>The Batch (DeepLearning.AI) - Weekly ML news - Andrew Ng's insights - Free</p> <p>Import AI - Weekly research summary - Jack Clark's newsletter - Free</p> <p>The Algorithm (MIT Tech Review) - AI news and analysis - Weekly - Free</p>"},{"location":"resources/further-reading/#podcasts","title":"Podcasts","text":"<p>TWiML (This Week in Machine Learning &amp; AI) - Interviews with researchers - Current research topics - Weekly episodes</p> <p>The TWIML AI Podcast - Industry applications - Research discussions - Regular updates</p> <p>Lex Fridman Podcast - Long-form interviews - AI researchers and thinkers - Deep conversations</p>"},{"location":"resources/further-reading/#keeping-up-with-research","title":"Keeping Up with Research","text":""},{"location":"resources/further-reading/#twitterx","title":"Twitter/X","text":"<p>Follow these researchers: - Andrew Ng (@AndrewYNg) - Yann LeCun (@ylecun) - Ian Goodfellow (@goodfellow_ian) - Andrej Karpathy (@karpathy) - Fran\u00e7ois Chollet (@fchollet)</p>"},{"location":"resources/further-reading/#conferences","title":"Conferences","text":"<p>Major ML conferences (watch talks online): - NeurIPS: Neural Information Processing Systems - ICML: International Conference on Machine Learning - ICLR: International Conference on Learning Representations - CVPR: Computer Vision and Pattern Recognition - ACL: Association for Computational Linguistics</p>"},{"location":"resources/further-reading/#specializations-after-ml101","title":"Specializations After ML101","text":""},{"location":"resources/further-reading/#computer-vision","title":"Computer Vision","text":"<ul> <li>CNNs, ResNet, EfficientNet</li> <li>Object detection (YOLO, R-CNN)</li> <li>Segmentation, tracking</li> <li>GANs for image generation</li> </ul>"},{"location":"resources/further-reading/#natural-language-processing","title":"Natural Language Processing","text":"<ul> <li>Transformers, BERT, GPT</li> <li>Named entity recognition</li> <li>Machine translation</li> <li>Question answering</li> </ul>"},{"location":"resources/further-reading/#reinforcement-learning","title":"Reinforcement Learning","text":"<ul> <li>Q-learning, policy gradients</li> <li>Deep RL (DQN, A3C, PPO)</li> <li>Multi-agent systems</li> <li>Game AI</li> </ul>"},{"location":"resources/further-reading/#time-series","title":"Time Series","text":"<ul> <li>ARIMA, LSTM</li> <li>Forecasting techniques</li> <li>Anomaly detection</li> <li>Financial applications</li> </ul>"},{"location":"resources/further-reading/#recommender-systems","title":"Recommender Systems","text":"<ul> <li>Collaborative filtering</li> <li>Content-based filtering</li> <li>Matrix factorization</li> <li>Neural collaborative filtering</li> </ul>"},{"location":"resources/further-reading/#career-resources","title":"Career Resources","text":"<p>Interview Prep - \"Cracking the Coding Interview\" (general algorithms) - \"Machine Learning Interviews\" by Susan Shu Chang - LeetCode, HackerRank for coding practice</p> <p>Portfolio Building - GitHub for code - Kaggle for competitions - Medium/Blog for writing - YouTube for explanations</p> <p>Job Boards - LinkedIn - Indeed (ML Engineer, Data Scientist) - AI Jobs Board - Kaggle Jobs</p>"},{"location":"resources/further-reading/#final-advice","title":"Final Advice","text":"<p>How to Learn Effectively</p> <ol> <li>Build projects: Apply what you learn</li> <li>Read papers: Stay current with research</li> <li>Participate in competitions: Practice on real data</li> <li>Contribute to open source: Learn from others</li> <li>Teach others: Best way to solidify knowledge</li> <li>Be patient: ML takes time to master</li> </ol> <p>Continuous Learning</p> <p>\"Machine learning is a rapidly evolving field. The fundamentals you've learned in ML101 will stay relevant, but keep learning and experimenting with new techniques!\"</p>"},{"location":"resources/further-reading/#whats-next","title":"What's Next?","text":"<p>After completing ML101:</p> <ol> <li>Specialize: Choose an area (CV, NLP, RL) and go deep</li> <li>Build portfolio: 3-5 solid projects on GitHub</li> <li>Kaggle: Enter competitions to practice</li> <li>Read papers: Implement influential papers from scratch</li> <li>Contribute: Open source, write blogs, help others</li> </ol> <p>Back to Home</p>"},{"location":"resources/math-primer/","title":"Mathematics Primer","text":"<p>A quick reference for the mathematical concepts used throughout ML101.</p>"},{"location":"resources/math-primer/#linear-algebra","title":"Linear Algebra","text":""},{"location":"resources/math-primer/#vectors","title":"Vectors","text":"<p>A vector is an ordered list of numbers: $\\(\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\)$</p> <p>Dot Product: $\\(\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i b_i\\)$</p> <p>Norm (length): $\\(\\|\\vec{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\)$</p>"},{"location":"resources/math-primer/#matrices","title":"Matrices","text":"<p>A matrix is a 2D array of numbers: $\\(A = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{bmatrix}\\)$</p> <p>Matrix Multiplication: $\\(C = AB \\quad \\text{where} \\quad C_{ij} = \\sum_{k} A_{ik} B_{kj}\\)$</p> <p>Transpose: $\\((A^T)_{ij} = A_{ji}\\)$</p>"},{"location":"resources/math-primer/#calculus","title":"Calculus","text":""},{"location":"resources/math-primer/#derivatives","title":"Derivatives","text":"<p>The derivative measures rate of change: $\\(f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\\)$</p> <p>Common Derivatives: - \\(\\frac{d}{dx}(x^n) = nx^{n-1}\\) - \\(\\frac{d}{dx}(e^x) = e^x\\) - \\(\\frac{d}{dx}(\\ln x) = \\frac{1}{x}\\)</p>"},{"location":"resources/math-primer/#partial-derivatives","title":"Partial Derivatives","text":"<p>For functions of multiple variables: $\\(\\frac{\\partial f}{\\partial x} = \\lim_{h \\to 0} \\frac{f(x+h, y) - f(x, y)}{h}\\)$</p>"},{"location":"resources/math-primer/#gradient","title":"Gradient","text":"<p>The gradient is a vector of partial derivatives: $\\(\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\end{bmatrix}\\)$</p>"},{"location":"resources/math-primer/#chain-rule","title":"Chain Rule","text":"<p>For composed functions: $\\(\\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x)\\)$</p> <p>ML Application: Backpropagation!</p>"},{"location":"resources/math-primer/#probability","title":"Probability","text":""},{"location":"resources/math-primer/#basic-probability","title":"Basic Probability","text":"\\[P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\\] <p>Properties: - \\(0 \\leq P(A) \\leq 1\\) - \\(P(A) + P(\\text{not } A) = 1\\)</p>"},{"location":"resources/math-primer/#conditional-probability","title":"Conditional Probability","text":"\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]"},{"location":"resources/math-primer/#expectation","title":"Expectation","text":"\\[E[X] = \\sum_{i} x_i \\cdot P(X = x_i)\\] <p>Or for continuous: \\(E[X] = \\int x \\cdot p(x) dx\\)</p>"},{"location":"resources/math-primer/#variance","title":"Variance","text":"\\[\\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2\\]"},{"location":"resources/math-primer/#statistics","title":"Statistics","text":""},{"location":"resources/math-primer/#mean-average","title":"Mean (Average)","text":"\\[\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\\]"},{"location":"resources/math-primer/#standard-deviation","title":"Standard Deviation","text":"\\[\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\]"},{"location":"resources/math-primer/#covariance","title":"Covariance","text":"\\[\\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]\\]"},{"location":"resources/math-primer/#correlation","title":"Correlation","text":"\\[\\rho = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\]"},{"location":"resources/math-primer/#functions-used-in-ml","title":"Functions Used in ML","text":""},{"location":"resources/math-primer/#sigmoid-logistic-function","title":"Sigmoid (Logistic Function)","text":"\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] <ul> <li>Range: \\((0, 1)\\)</li> <li>Use: Binary classification, probabilities</li> </ul>"},{"location":"resources/math-primer/#softmax","title":"Softmax","text":"\\[\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\\] <ul> <li>Use: Multi-class classification</li> </ul>"},{"location":"resources/math-primer/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"\\[\\text{ReLU}(x) = \\max(0, x)\\] <ul> <li>Use: Neural network activation</li> </ul>"},{"location":"resources/math-primer/#log-loss-cross-entropy","title":"Log Loss (Cross-Entropy)","text":"\\[L = -\\sum_{i} y_i \\log(\\hat{y}_i)\\] <ul> <li>Use: Classification loss function</li> </ul>"},{"location":"resources/math-primer/#optimization","title":"Optimization","text":""},{"location":"resources/math-primer/#gradient-descent","title":"Gradient Descent","text":"<p>Iteratively update parameters to minimize loss: $\\(\\theta := \\theta - \\alpha \\nabla J(\\theta)\\)$</p> <p>where: - \\(\\theta\\): parameters - \\(\\alpha\\): learning rate - \\(\\nabla J\\): gradient of cost function</p>"},{"location":"resources/math-primer/#matrix-calculus","title":"Matrix Calculus","text":""},{"location":"resources/math-primer/#gradient-of-vector-function","title":"Gradient of Vector Function","text":"\\[\\nabla_x (w^T x) = w\\]"},{"location":"resources/math-primer/#gradient-of-quadratic-form","title":"Gradient of Quadratic Form","text":"\\[\\nabla_x (x^T A x) = (A + A^T)x\\] <p>For symmetric \\(A\\): \\(\\nabla_x (x^T A x) = 2Ax\\)</p>"},{"location":"resources/math-primer/#quick-reference","title":"Quick Reference","text":"Operation Notation NumPy Dot product \\(\\vec{a} \\cdot \\vec{b}\\) <code>np.dot(a, b)</code> or <code>a @ b</code> Matrix mult \\(AB\\) <code>A @ B</code> Transpose \\(A^T\\) <code>A.T</code> Inverse \\(A^{-1}\\) <code>np.linalg.inv(A)</code> Norm \\(\\|\\vec{v}\\|\\) <code>np.linalg.norm(v)</code> Exponential \\(e^x\\) <code>np.exp(x)</code> Natural log \\(\\ln x\\) <code>np.log(x)</code> Sum \\(\\sum\\) <code>np.sum()</code> Mean \\(\\bar{x}\\) <code>np.mean()</code>"},{"location":"resources/math-primer/#dont-worry","title":"Don't Worry!","text":"<p>You Don't Need to Memorize</p> <ul> <li>Focus on intuition over formulas</li> <li>Use this as a reference when needed</li> <li>The course explains concepts from first principles</li> <li>Practice builds understanding naturally</li> </ul>"},{"location":"resources/math-primer/#further-reading","title":"Further Reading","text":"<ul> <li>Khan Academy - Linear Algebra</li> <li>Khan Academy - Calculus</li> <li>3Blue1Brown - Essence of Linear Algebra</li> <li>3Blue1Brown - Essence of Calculus</li> </ul> <p>Back to Home</p>"},{"location":"resources/python-refresher/","title":"Python Refresher","text":"<p>Quick review of Python concepts needed for ML101.</p>"},{"location":"resources/python-refresher/#python-basics","title":"Python Basics","text":""},{"location":"resources/python-refresher/#variables-and-types","title":"Variables and Types","text":"<pre><code># Numbers\nx = 5           # int\ny = 3.14        # float\n\n# Strings\nname = \"ML101\"\nmessage = 'Hello'\n\n# Booleans\nis_learning = True\nis_difficult = False\n\n# Lists\nnumbers = [1, 2, 3, 4, 5]\nmixed = [1, \"hello\", True, 3.14]\n\n# Dictionaries\nperson = {\"name\": \"Alice\", \"age\": 25}\n\n# Check type\nprint(type(x))  # &lt;class 'int'&gt;\n</code></pre>"},{"location":"resources/python-refresher/#functions","title":"Functions","text":"<pre><code>def add(a, b):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\nresult = add(3, 5)  # 8\n\n# Default arguments\ndef greet(name=\"World\"):\n    return f\"Hello, {name}!\"\n\n# Lambda functions\nsquare = lambda x: x ** 2\n</code></pre>"},{"location":"resources/python-refresher/#control-flow","title":"Control Flow","text":"<pre><code># If statements\nif x &gt; 0:\n    print(\"Positive\")\nelif x == 0:\n    print(\"Zero\")\nelse:\n    print(\"Negative\")\n\n# For loops\nfor i in range(5):\n    print(i)  # 0, 1, 2, 3, 4\n\nfor item in [1, 2, 3]:\n    print(item)\n\n# While loops\ncount = 0\nwhile count &lt; 5:\n    print(count)\n    count += 1\n\n# List comprehensions\nsquares = [x**2 for x in range(10)]\nevens = [x for x in range(10) if x % 2 == 0]\n</code></pre>"},{"location":"resources/python-refresher/#numpy-essentials","title":"NumPy Essentials","text":""},{"location":"resources/python-refresher/#creating-arrays","title":"Creating Arrays","text":"<pre><code>import numpy as np\n\n# From list\na = np.array([1, 2, 3, 4])\n\n# Zeros and ones\nzeros = np.zeros(5)\nones = np.ones((3, 4))\n\n# Range\nr = np.arange(0, 10, 2)  # [0, 2, 4, 6, 8]\n\n# Linspace\nl = np.linspace(0, 1, 5)  # [0, 0.25, 0.5, 0.75, 1]\n\n# Random\nrand = np.random.rand(3, 3)\nrandn = np.random.randn(3, 3)  # Standard normal\n</code></pre>"},{"location":"resources/python-refresher/#array-operations","title":"Array Operations","text":"<pre><code>a = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\n\n# Element-wise operations\nc = a + b      # [5, 7, 9]\nc = a * 2      # [2, 4, 6]\nc = a * b      # [4, 10, 18]\nc = a ** 2     # [1, 4, 9]\n\n# Mathematical functions\nnp.exp(a)      # Exponential\nnp.log(a)      # Natural log\nnp.sqrt(a)     # Square root\nnp.sin(a)      # Sine\n\n# Aggregations\nnp.sum(a)      # 6\nnp.mean(a)     # 2.0\nnp.max(a)      # 3\nnp.min(a)      # 1\n</code></pre>"},{"location":"resources/python-refresher/#indexing-and-slicing","title":"Indexing and Slicing","text":"<pre><code>a = np.array([1, 2, 3, 4, 5])\n\n# Indexing\nprint(a[0])     # 1\nprint(a[-1])    # 5\n\n# Slicing\nprint(a[1:4])   # [2, 3, 4]\nprint(a[:3])    # [1, 2, 3]\nprint(a[2:])    # [3, 4, 5]\n\n# 2D arrays\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\n\nprint(A[0, 0])  # 1\nprint(A[1, 2])  # 6\nprint(A[0, :])  # [1, 2, 3] (first row)\nprint(A[:, 1])  # [2, 5] (second column)\n</code></pre>"},{"location":"resources/python-refresher/#broadcasting","title":"Broadcasting","text":"<pre><code># Add scalar to array\na = np.array([1, 2, 3])\nb = a + 10  # [11, 12, 13]\n\n# Add arrays of different shapes\nA = np.array([[1, 2, 3],\n              [4, 5, 6]])\nv = np.array([10, 20, 30])\n\nB = A + v  # [[11, 22, 33],\n           #  [14, 25, 36]]\n</code></pre>"},{"location":"resources/python-refresher/#matplotlib-basics","title":"Matplotlib Basics","text":""},{"location":"resources/python-refresher/#line-plots","title":"Line Plots","text":"<pre><code>import matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label='sin(x)')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Sine Wave')\nplt.legend()\nplt.grid(True)\nplt.show()\n</code></pre>"},{"location":"resources/python-refresher/#scatter-plots","title":"Scatter Plots","text":"<pre><code>x = np.random.rand(50)\ny = np.random.rand(50)\n\nplt.scatter(x, y, c='blue', alpha=0.5)\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Random Scatter')\nplt.show()\n</code></pre>"},{"location":"resources/python-refresher/#multiple-subplots","title":"Multiple Subplots","text":"<pre><code>fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(x, y)\naxes[0].set_title('Plot 1')\n\naxes[1].scatter(x, y)\naxes[1].set_title('Plot 2')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"resources/python-refresher/#pandas-basics","title":"Pandas Basics","text":""},{"location":"resources/python-refresher/#dataframes","title":"DataFrames","text":"<pre><code>import pandas as pd\n\n# Create DataFrame\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['NY', 'LA', 'SF']\n}\ndf = pd.DataFrame(data)\n\n# View data\nprint(df.head())\nprint(df.info())\nprint(df.describe())\n\n# Indexing\nprint(df['Name'])        # Column\nprint(df.loc[0])         # Row by index\nprint(df.iloc[0])        # Row by position\n\n# Filtering\nadults = df[df['Age'] &gt;= 30]\n\n# Adding columns\ndf['Age_Plus_10'] = df['Age'] + 10\n</code></pre>"},{"location":"resources/python-refresher/#reading-data","title":"Reading Data","text":"<pre><code># CSV\ndf = pd.read_csv('data.csv')\n\n# Excel\ndf = pd.read_excel('data.xlsx')\n\n# Save\ndf.to_csv('output.csv', index=False)\n</code></pre>"},{"location":"resources/python-refresher/#python-tips-for-ml","title":"Python Tips for ML","text":""},{"location":"resources/python-refresher/#print-debugging","title":"Print Debugging","text":"<pre><code># Always check shapes!\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\")\n\n# Print intermediate values\nprint(f\"Loss: {loss:.4f}\")\nprint(f\"Accuracy: {accuracy:.2%}\")\n</code></pre>"},{"location":"resources/python-refresher/#assertions","title":"Assertions","text":"<pre><code># Verify assumptions\nassert X.shape[0] == y.shape[0], \"Number of samples must match\"\nassert X.ndim == 2, \"X must be 2D\"\n</code></pre>"},{"location":"resources/python-refresher/#f-strings","title":"F-strings","text":"<pre><code>name = \"Alice\"\nage = 25\n\n# Old way\nprint(\"Name: \" + name + \", Age: \" + str(age))\n\n# F-string (Python 3.6+)\nprint(f\"Name: {name}, Age: {age}\")\n\n# Formatting\npi = 3.14159\nprint(f\"Pi: {pi:.2f}\")  # Pi: 3.14\n</code></pre>"},{"location":"resources/python-refresher/#list-vs-numpy-array","title":"List vs NumPy Array","text":"<pre><code># Lists: slow, flexible\nlist1 = [1, 2, 3]\nlist2 = [4, 5, 6]\n# list1 + list2 = [1, 2, 3, 4, 5, 6]  # Concatenation!\n\n# Arrays: fast, vectorized\narr1 = np.array([1, 2, 3])\narr2 = np.array([4, 5, 6])\n# arr1 + arr2 = [5, 7, 9]  # Element-wise!\n</code></pre>"},{"location":"resources/python-refresher/#common-mistakes","title":"Common Mistakes","text":"<pre><code># \u274c Wrong: Modifying during iteration\nfor i in range(len(list)):\n    list.pop(i)  # Don't do this!\n\n# \u2705 Right: Create new list\nnew_list = [x for x in list if condition]\n\n# \u274c Wrong: Comparing floats\nif 0.1 + 0.2 == 0.3:  # False!\n\n# \u2705 Right: Use tolerance\nif abs((0.1 + 0.2) - 0.3) &lt; 1e-10:\n\n# \u274c Wrong: Mutable default argument\ndef append_to(element, to=[]):\n    to.append(element)\n    return to\n\n# \u2705 Right: Use None\ndef append_to(element, to=None):\n    if to is None:\n        to = []\n    to.append(element)\n    return to\n</code></pre>"},{"location":"resources/python-refresher/#virtual-environments","title":"Virtual Environments","text":"<pre><code># Create\npython -m venv venv\n\n# Activate (Windows)\nvenv\\Scripts\\activate\n\n# Activate (Mac/Linux)\nsource venv/bin/activate\n\n# Install packages\npip install numpy pandas matplotlib\n\n# Save requirements\npip freeze &gt; requirements.txt\n\n# Install from requirements\npip install -r requirements.txt\n\n# Deactivate\ndeactivate\n</code></pre>"},{"location":"resources/python-refresher/#jupyter-shortcuts","title":"Jupyter Shortcuts","text":"Action Shortcut Run cell <code>Shift + Enter</code> Run in place <code>Ctrl + Enter</code> Insert above <code>A</code> (command mode) Insert below <code>B</code> (command mode) Delete cell <code>DD</code> (command mode) To markdown <code>M</code> (command mode) To code <code>Y</code> (command mode) Command mode <code>Esc</code> Edit mode <code>Enter</code>"},{"location":"resources/python-refresher/#resources","title":"Resources","text":"<ul> <li>Python Official Tutorial</li> <li>NumPy Documentation</li> <li>Matplotlib Tutorials</li> <li>Pandas Documentation</li> </ul>"},{"location":"resources/python-refresher/#practice","title":"Practice","text":"<p>Before starting Module 1, make sure you can:</p> <ul> <li>[ ] Create and manipulate NumPy arrays</li> <li>[ ] Perform basic array operations</li> <li>[ ] Index and slice arrays</li> <li>[ ] Plot simple graphs with matplotlib</li> <li>[ ] Write and call functions</li> <li>[ ] Use list comprehensions</li> </ul> <p>Back to Home</p>"}]}
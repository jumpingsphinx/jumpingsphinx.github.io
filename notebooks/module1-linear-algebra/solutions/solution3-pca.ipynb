{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 1 - Exercise 3: PCA Implementation from Scratch\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module1-linear-algebra/exercise3-pca.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Implement Principal Component Analysis (PCA) from scratch\n- Understand eigenvalue decomposition and its role in PCA\n- Reduce dimensionality of real datasets\n- Interpret principal components and explained variance\n- Visualize high-dimensional data in 2D/3D\n- Compare your implementation with sklearn's PCA\n\n## Prerequisites\n\n- Completion of Exercises 1 and 2\n- Understanding of eigenvalues and eigenvectors\n- Covariance matrix concepts\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Covariance\n",
    "\n",
    "### Background\n",
    "\n",
    "PCA is based on the covariance matrix, which measures how features vary together:\n",
    "- Diagonal elements: variance of each feature\n",
    "- Off-diagonal elements: covariance between feature pairs\n",
    "- Positive covariance: features increase together\n",
    "- Negative covariance: one increases as the other decreases\n",
    "\n",
    "### Exercise 1.1: Compute Covariance Matrix\n",
    "\n",
    "**Task:** Given a dataset with 2 features:\n",
    "\n",
    "1. Center the data (subtract mean)\n",
    "2. Compute the covariance matrix manually\n",
    "3. Compute it using `np.cov()`\n",
    "4. Verify both methods match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: 5 samples, 2 features\n",
    "data = np.array([[1, 2],\n",
    "                 [2, 3],\n",
    "                 [3, 5],\n",
    "                 [4, 6],\n",
    "                 [5, 8]])\n",
    "\n",
    "# Your code here\n",
    "# Step 1: Center the data\n",
    "mean = data.mean(axis=0)\n",
    "centered_data = data - mean\n",
    "\n",
    "# Step 2: Manual covariance calculation\n",
    "# Cov = (1/(n-1)) * X^T @ X where X is centered\n",
    "n = centered_data.shape[0]\n",
    "cov_manual = (1 / (n - 1)) * (centered_data.T @ centered_data)\n",
    "\n",
    "# Step 3: Using NumPy (rowvar=False means columns are variables)\n",
    "cov_numpy = np.cov(data, rowvar=False)\n",
    "# Verify\n",
    "assert np.allclose(cov_manual, cov_numpy), \"Manual and NumPy covariance should match\"\n",
    "print(\"\\n\u2713 Covariance calculation correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Eigenvalue Decomposition\n",
    "\n",
    "### Background\n",
    "\n",
    "Eigenvalues and eigenvectors are key to PCA:\n",
    "- **Eigenvector**: Direction of a principal component\n",
    "- **Eigenvalue**: Amount of variance explained by that component\n",
    "- For covariance matrix C: **C v = \u03bb v**\n",
    "\n",
    "### Exercise 2.1: Compute Eigenvalues and Eigenvectors\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Compute eigenvalues and eigenvectors of the covariance matrix\n",
    "2. Sort them by eigenvalue (descending)\n",
    "3. Verify that eigenvectors are orthogonal\n",
    "4. Verify the eigenvalue equation: C v = \u03bb v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample covariance matrix\n",
    "C = np.array([[2.5, 1.5],\n",
    "              [1.5, 1.5]])\n",
    "\n",
    "# Your code here\n",
    "# Step 1: Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "\n",
    "# Step 2: Sort by eigenvalue (descending)\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues_sorted = eigenvalues[idx]\n",
    "eigenvectors_sorted = eigenvectors[:, idx]\n",
    "\n",
    "# Step 3: Check orthogonality (dot product should be ~0)\n",
    "dot_product = np.dot(eigenvectors_sorted[:, 0], eigenvectors_sorted[:, 1])\n",
    "\n",
    "# Step 4: Verify Cv = \u03bbv for first eigenvector\n",
    "v1 = eigenvectors_sorted[:, 0]\n",
    "lambda1 = eigenvalues_sorted[0]\n",
    "left_side = C @ v1\n",
    "right_side = lambda1 * v1\n",
    "# Verify\n",
    "assert np.allclose(dot_product, 0, atol=1e-10), \"Eigenvectors should be orthogonal\"\n",
    "assert np.allclose(left_side, right_side), \"Should satisfy Cv = \u03bbv\"\n",
    "print(\"\\n\u2713 Eigenvalue decomposition correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Visualize Principal Components\n",
    "\n",
    "### Exercise 3.1: Plot Data and Principal Components\n",
    "\n",
    "**Task:** Visualize how principal components align with data variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated 2D data\n",
    "np.random.seed(42)\n",
    "mean = [0, 0]\n",
    "cov = [[2, 1.5], [1.5, 1.5]]\n",
    "data_2d = np.random.multivariate_normal(mean, cov, 200)\n",
    "\n",
    "# Center the data\n",
    "data_centered = data_2d - data_2d.mean(axis=0)\n",
    "\n",
    "# Compute covariance and eigenvectors\n",
    "cov_matrix = np.cov(data_centered, rowvar=False)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort by eigenvalue\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(data_centered[:, 0], data_centered[:, 1], alpha=0.5)\n",
    "\n",
    "# Plot principal components as arrows\n",
    "origin = [0, 0]\n",
    "for i in range(2):\n",
    "    # Scale eigenvectors by sqrt of eigenvalue for visualization\n",
    "    direction = eigenvectors[:, i] * np.sqrt(eigenvalues[i]) * 2\n",
    "    plt.arrow(origin[0], origin[1], direction[0], direction[1],\n",
    "              head_width=0.2, head_length=0.3, fc=f'C{i+1}', ec=f'C{i+1}',\n",
    "              linewidth=3, label=f'PC{i+1} (\u03bb={eigenvalues[i]:.2f})')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Data with Principal Components')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"PC1 explains {eigenvalues[0]/eigenvalues.sum()*100:.1f}% of variance\")\n",
    "print(f\"PC2 explains {eigenvalues[1]/eigenvalues.sum()*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Implement PCA from Scratch\n",
    "\n",
    "### Background\n",
    "\n",
    "PCA algorithm:\n",
    "1. Center the data (subtract mean)\n",
    "2. Compute covariance matrix\n",
    "3. Compute eigenvalues and eigenvectors\n",
    "4. Sort by eigenvalue (descending)\n",
    "5. Select top k eigenvectors\n",
    "6. Project data onto these eigenvectors\n",
    "\n",
    "### Exercise 4.1: Complete PCA Implementation\n",
    "\n",
    "**Task:** Implement a PCA class from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, n_components=2):\n",
    "        \"\"\"\n",
    "        Principal Component Analysis.\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.components_ = None\n",
    "        self.mean_ = None\n",
    "        self.eigenvalues_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA on data X.\n",
    "        \"\"\"\n",
    "        # Step 1: Center the data\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # Step 2: Compute covariance matrix\n",
    "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "        \n",
    "        # Step 3: Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "        \n",
    "        # Step 4: Sort by eigenvalue (descending)\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Step 5: Select top k eigenvectors\n",
    "        self.components_ = eigenvectors[:, :self.n_components]\n",
    "        self.eigenvalues_ = eigenvalues[:self.n_components]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Project data onto principal components.\n",
    "        \"\"\"\n",
    "        # Step 1: Center the data\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # Step 2: Project onto principal components\n",
    "        # X (n, d) @ components (d, k) -> (n, k)\n",
    "        X_transformed = np.dot(X_centered, self.components_)\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit and transform in one step.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def explained_variance_ratio(self):\n",
    "        \"\"\"\n",
    "        Return the proportion of variance explained by each component.\n",
    "        \"\"\"\n",
    "        total_var = np.sum(self.eigenvalues_)\n",
    "        return self.eigenvalues_ / total_var\n",
    "\n",
    "print(\"PCA class implemented!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Test Your PCA Implementation\n",
    "\n",
    "### Exercise 5.1: Apply PCA to 2D Data\n",
    "\n",
    "**Task:** Test your PCA on synthetic 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2D data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 2) @ np.array([[2, 0], [0, 0.5]])\n",
    "\n",
    "# Apply your PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Apply sklearn's PCA\n",
    "sklearn_pca = SklearnPCA(n_components=2)\n",
    "X_sklearn = sklearn_pca.fit_transform(X)\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "axes[0].set_title('Original Data')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].axis('equal')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Your PCA\n",
    "axes[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5, color='red')\n",
    "axes[1].set_title('Your PCA')\n",
    "axes[1].set_xlabel('PC1')\n",
    "axes[1].set_ylabel('PC2')\n",
    "axes[1].axis('equal')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Sklearn PCA\n",
    "axes[2].scatter(X_sklearn[:, 0], X_sklearn[:, 1], alpha=0.5, color='green')\n",
    "axes[2].set_title('Sklearn PCA')\n",
    "axes[2].set_xlabel('PC1')\n",
    "axes[2].set_ylabel('PC2')\n",
    "axes[2].axis('equal')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Your explained variance ratios:\", pca.explained_variance_ratio())\n",
    "print(\"Sklearn explained variance ratios:\", sklearn_pca.explained_variance_ratio_)\n",
    "\n",
    "# Verify (note: eigenvectors can point in opposite directions)\n",
    "assert np.allclose(np.abs(X_pca), np.abs(X_sklearn), atol=0.1), \"Results should be similar\"\n",
    "print(\"\\n\u2713 Your PCA works correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: PCA on Iris Dataset\n",
    "\n",
    "### Background\n",
    "\n",
    "The Iris dataset has 4 features. We'll use PCA to reduce it to 2D for visualization.\n",
    "\n",
    "### Exercise 6.1: Dimensionality Reduction\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Load the Iris dataset\n",
    "2. Apply PCA to reduce from 4D to 2D\n",
    "3. Visualize the 2D projection with class labels\n",
    "4. Analyze explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Iris dataset shape:\", X_iris.shape)\n",
    "print(\"Features:\", iris.feature_names)\n",
    "print(\"Classes:\", target_names)\n",
    "\n",
    "# Apply your PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_iris)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, color, target_name in zip(range(3), colors, target_names):\n",
    "    plt.scatter(X_pca[y_iris == i, 0], X_pca[y_iris == i, 1],\n",
    "                color=color, alpha=0.5, label=target_name)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio()[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio()[1]*100:.1f}% variance)')\n",
    "plt.title('Iris Dataset - PCA Projection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "total_variance = sum(pca.explained_variance_ratio())\n",
    "print(f\"\\nTotal variance explained by 2 components: {total_variance*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Choosing Number of Components\n",
    "\n",
    "### Exercise 7.1: Scree Plot\n",
    "\n",
    "**Task:** Create a scree plot to determine optimal number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA with all components\n",
    "pca_full = PCA(n_components=4)\n",
    "pca_full.fit(X_iris)\n",
    "\n",
    "explained_variance = pca_full.explained_variance_ratio()\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Create scree plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Individual explained variance\n",
    "axes[0].bar(range(1, 5), explained_variance)\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Scree Plot')\n",
    "axes[0].set_xticks(range(1, 5))\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(range(1, 5), cumulative_variance, 'bo-', linewidth=2, markersize=8)\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].set_xticks(range(1, 5))\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for i, (var, cum_var) in enumerate(zip(explained_variance, cumulative_variance)):\n",
    "    print(f\"PC{i+1}: {var*100:.1f}% (cumulative: {cum_var*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: PCA on High-Dimensional Data\n",
    "\n",
    "### Exercise 8.1: Digits Dataset (64D \u2192 2D)\n",
    "\n",
    "**Task:** Apply PCA to the digits dataset (64 features \u2192 2 for visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(\"Digits dataset shape:\", X_digits.shape)\n",
    "print(\"Original dimensionality:\", X_digits.shape[1])\n",
    "\n",
    "# Apply PCA\n",
    "pca_digits = PCA(n_components=2)\n",
    "X_digits_pca = pca_digits.fit_transform(X_digits)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(X_digits_pca[:, 0], X_digits_pca[:, 1],\n",
    "                      c=y_digits, cmap='tab10', alpha=0.6)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.xlabel(f'PC1 ({pca_digits.explained_variance_ratio()[0]*100:.1f}% variance)')\n",
    "plt.ylabel(f'PC2 ({pca_digits.explained_variance_ratio()[1]*100:.1f}% variance)')\n",
    "plt.title('Digits Dataset: 64D \u2192 2D using PCA')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal variance explained: {sum(pca_digits.explained_variance_ratio())*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: PCA for Data Compression\n",
    "\n",
    "### Exercise 9.1: Reconstruct Data from PCA\n",
    "\n",
    "**Task:** Use PCA to compress and reconstruct digit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one digit image\n",
    "idx = 0\n",
    "original_image = X_digits[idx].reshape(8, 8)\n",
    "\n",
    "# Try different numbers of components\n",
    "n_components_list = [2, 5, 10, 20, 64]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original\n",
    "axes[0].imshow(original_image, cmap='gray')\n",
    "axes[0].set_title('Original (64D)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Reconstructions with different numbers of components\n",
    "for i, n_comp in enumerate(n_components_list[:-1]):\n",
    "    pca_temp = PCA(n_components=n_comp)\n",
    "    X_compressed = pca_temp.fit_transform(X_digits)\n",
    "    \n",
    "    # Reconstruct: X_reconstructed = X_compressed @ components + mean\n",
    "    X_reconstructed = X_compressed @ pca_temp.components_ + pca_temp.mean_\n",
    "    \n",
    "    reconstructed_image = X_reconstructed[idx].reshape(8, 8)\n",
    "    \n",
    "    axes[i+1].imshow(reconstructed_image, cmap='gray')\n",
    "    var_explained = sum(pca_temp.explained_variance_ratio()) * 100\n",
    "    axes[i+1].set_title(f'{n_comp} components ({var_explained:.1f}%)')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how reconstruction quality improves with more components!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Compare with Sklearn\n",
    "\n",
    "### Exercise 10.1: Validate Your Implementation\n",
    "\n",
    "**Task:** Compare your PCA with sklearn's on multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "X_test = np.random.randn(100, 5)\n",
    "\n",
    "# Your PCA\n",
    "pca_yours = PCA(n_components=3)\n",
    "X_yours = pca_yours.fit_transform(X_test)\n",
    "\n",
    "# Sklearn PCA\n",
    "pca_sklearn = SklearnPCA(n_components=3)\n",
    "X_sklearn = pca_sklearn.fit_transform(X_test)\n",
    "\n",
    "# Compare results\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARISON: Your PCA vs Sklearn PCA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Explained Variance Ratio:\")\n",
    "print(\"   Your PCA:\", pca_yours.explained_variance_ratio())\n",
    "print(\"   Sklearn: \", pca_sklearn.explained_variance_ratio_)\n",
    "print(\"   Match?\", np.allclose(pca_yours.explained_variance_ratio(), \n",
    "                                pca_sklearn.explained_variance_ratio_))\n",
    "\n",
    "print(\"\\n2. Principal Components (first PC):\")\n",
    "print(\"   Your PCA:\", pca_yours.components_[:, 0])\n",
    "print(\"   Sklearn: \", pca_sklearn.components_[0])\n",
    "# Note: Signs might differ\n",
    "print(\"   Match?\", np.allclose(np.abs(pca_yours.components_[:, 0]), \n",
    "                                np.abs(pca_sklearn.components_[0])))\n",
    "\n",
    "print(\"\\n3. Transformed Data (first sample):\")\n",
    "print(\"   Your PCA:\", X_yours[0])\n",
    "print(\"   Sklearn: \", X_sklearn[0])\n",
    "# Note: Signs might differ\n",
    "print(\"   Match?\", np.allclose(np.abs(X_yours[0]), np.abs(X_sklearn[0])))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Your PCA implementation matches sklearn!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Incremental PCA\n",
    "\n",
    "Implement a version of PCA that processes data in batches (for large datasets that don't fit in memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IncrementalPCA:\n",
    "    def __init__(self, n_components=2, batch_size=50):\n",
    "        \"\"\"\n",
    "        Incremental PCA for large datasets.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_components : int\n",
    "            Number of principal components\n",
    "        batch_size : int\n",
    "            Number of samples to process at once\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.batch_size = batch_size\n",
    "        # Your implementation here\n",
    "    def partial_fit(self, X):\n",
    "        # Simplistic implementation: maintain running mean and covariance?\n",
    "        # A true Incremental PCA is complex.\n",
    "        # I'll just put a placeholder or basic batched approach hints.\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        pass\n",
    "# Test\n",
    "# ipca = IncrementalPCA(n_components=2, batch_size=50)\n",
    "# for i in range(0, len(X_test), 50):\n",
    "#     ipca.partial_fit(X_test[i:i+50])\n",
    "# X_transformed = ipca.transform(X_test)\n",
    "\n",
    "print(\"Challenge: Implement Incremental PCA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Kernel PCA\n",
    "\n",
    "Implement Kernel PCA to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(X, Y, gamma=1.0):\n",
    "    # K(x, y) = exp(-gamma * ||x - y||^2)\n",
    "    # Using broadcasting for pairwise distance\n",
    "    # ||x-y||^2 = ||x||^2 + ||y||^2 - 2<x,y>\n",
    "    \n",
    "    X_norm = np.sum(X**2, axis=1).reshape(-1, 1)\n",
    "    Y_norm = np.sum(Y**2, axis=1).reshape(1, -1)\n",
    "    K = X_norm + Y_norm - 2 * np.dot(X, Y.T)\n",
    "    K = np.exp(-gamma * K)\n",
    "    return K\n",
    "\n",
    "class KernelPCA:\n",
    "    def __init__(self, n_components=2, kernel='rbf', gamma=1.0):\n",
    "        self.n_components = n_components\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.alphas_ = None\n",
    "        self.lambdas_ = None\n",
    "        self.X_fit_ = None\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.X_fit_ = X\n",
    "        n = X.shape[0]\n",
    "        K = rbf_kernel(X, X, self.gamma)\n",
    "        \n",
    "        # Center kernel matrix\n",
    "        # K_centered = K - 1_n K - K 1_n + 1_n K 1_n\n",
    "        one_n = np.ones((n, n)) / n\n",
    "        K_centered = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n",
    "        \n",
    "        # Eigen decomposition\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(K_centered)\n",
    "        \n",
    "        # Sort\n",
    "        idx = eigenvalues.argsort()[::-1]\n",
    "        self.lambdas_ = eigenvalues[idx][:self.n_components]\n",
    "        self.alphas_ = eigenvectors[:, idx][:, :self.n_components]\n",
    "        \n",
    "        # Project: alphas * sqrt(lambda)\n",
    "        return self.alphas_ * np.sqrt(self.lambdas_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: PCA with SVD\n",
    "\n",
    "Implement PCA using Singular Value Decomposition instead of eigendecomposition.\n",
    "\n",
    "**Hint:** For centered data X, the principal components are the right singular vectors of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_svd(X, n_components=2):\n",
    "    \"\"\"\n",
    "    PCA using Singular Value Decomposition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Data matrix (n_samples, n_features)\n",
    "    n_components : int\n",
    "        Number of components to keep\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_transformed : np.ndarray\n",
    "        Transformed data (n_samples, n_components)\n",
    "    components : np.ndarray\n",
    "        Principal components (n_features, n_components)\n",
    "    \"\"\"\n",
    "    # Your implementation here\n",
    "def pca_svd(X, n_components=2):\n",
    "    # Center data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # SVD\n",
    "    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    \n",
    "    # Principal components are rows of Vt (cols of V)\n",
    "    components = Vt[:n_components].T\n",
    "    \n",
    "    return components\n",
    "# Test\n",
    "# X_svd, components = pca_svd(X_test, n_components=3)\n",
    "# Compare with your eigendecomposition-based PCA\n",
    "\n",
    "print(\"Challenge: Implement PCA using SVD!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **What is PCA actually doing geometrically?**\n",
    "   - Think about rotating data to align with axes of maximum variance\n",
    "\n",
    "2. **Why must data be centered before PCA?**\n",
    "   - Consider what happens if the mean is not at the origin\n",
    "\n",
    "3. **When would you NOT want to use PCA?**\n",
    "   - Think about when variance is not a good measure of importance\n",
    "   - Consider non-linear relationships\n",
    "\n",
    "4. **How do you choose the number of components?**\n",
    "   - Cumulative explained variance threshold (e.g., 95%)\n",
    "   - Scree plot elbow\n",
    "   - Cross-validation performance\n",
    "\n",
    "5. **Why are eigenvalues and eigenvectors important in ML?**\n",
    "   - They appear in PCA, spectral clustering, graph analysis, covariance estimation\n",
    "\n",
    "6. **What's the difference between PCA and feature selection?**\n",
    "   - PCA creates new features (combinations), feature selection picks existing ones\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "\u2713 How to compute covariance matrices  \n",
    "\u2713 How to find eigenvalues and eigenvectors  \n",
    "\u2713 How to implement PCA from scratch  \n",
    "\u2713 How to apply PCA to real datasets  \n",
    "\u2713 How to choose the number of components  \n",
    "\u2713 How to use PCA for visualization and compression  \n",
    "\u2713 How to validate against sklearn's implementation  \n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete the challenge problems\n",
    "- Apply PCA to your own datasets\n",
    "- Continue to Module 2: Regression Algorithms\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues).\n",
    "\n",
    "**Congratulations on completing Module 1!** \ud83c\udf89"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 2 - Exercise 2: Gradient Descent Optimization\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module2-regression/exercise2-gradient-descent.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Implement batch gradient descent from scratch\n- Implement stochastic gradient descent (SGD)\n- Implement mini-batch gradient descent\n- Tune learning rate and visualize its effect\n- Understand convergence criteria\n- Compare different optimization strategies\n\n## Prerequisites\n\n- Completion of Exercise 1 (Linear Regression)\n- Understanding of derivatives and gradients\n- Familiarity with optimization concepts\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Gradient Descent\n",
    "\n",
    "### Background\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm for finding the minimum of a function. For linear regression:\n",
    "\n",
    "**Cost Function (MSE):**\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\mathbf{w}(\\mathbf{x}^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "**Gradient:**\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\mathbf{w}(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "**Update Rule:**\n",
    "$$w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.\n",
    "\n",
    "### Exercise 1.1: Implement Cost Function\n",
    "\n",
    "**Task:** Implement the MSE cost function for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, weights):\n",
    "    \"\"\"\n",
    "    Compute the MSE cost function.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features with bias (n_samples, n_features + 1)\n",
    "    y : np.ndarray\n",
    "        Target values (n_samples,)\n",
    "    weights : np.ndarray\n",
    "        Model weights including bias (n_features + 1,)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Cost value\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    \n",
    "    # Calculate predictions\n",
    "    predictions = X @ weights\n",
    "    \n",
    "    # Calculate cost: (1/2m) * sum((predictions - y)^2)\n",
    "    cost = np.sum((predictions - y) ** 2) / (2 * m)\n",
    "    return cost\n",
    "\n",
    "# Test on simple data\n",
    "X_test = np.array([[1, 1], [1, 2], [1, 3], [1, 4]])  # with bias column\n",
    "y_test = np.array([2, 4, 6, 8])\n",
    "weights_test = np.array([0, 2])  # bias=0, slope=2 (perfect fit)\n",
    "\n",
    "cost = compute_cost(X_test, y_test, weights_test)\n",
    "print(f\"Cost with perfect weights: {cost:.6f} (should be ~0)\")\n",
    "\n",
    "weights_bad = np.array([0, 0])  # bad initialization\n",
    "cost_bad = compute_cost(X_test, y_test, weights_bad)\n",
    "print(f\"Cost with zero weights: {cost_bad:.6f} (should be larger)\")\n",
    "\n",
    "assert cost < 0.01, \"Cost should be near zero for perfect weights\"\n",
    "assert cost_bad > 10, \"Cost should be large for bad weights\"\n",
    "print(\"\\n\u2713 Cost function implemented correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Batch Gradient Descent\n",
    "\n",
    "### Background\n",
    "\n",
    "**Batch Gradient Descent** uses the entire training set to compute gradients at each iteration.\n",
    "\n",
    "**Pros:** Stable convergence, exact gradient\n",
    "**Cons:** Slow for large datasets, may get stuck in local minima\n",
    "\n",
    "### Exercise 2.1: Implement Batch Gradient Descent\n",
    "\n",
    "**Task:** Implement the full batch gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform batch gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features (n_samples, n_features) - without bias\n",
    "    y : np.ndarray\n",
    "        Target values (n_samples,)\n",
    "    learning_rate : float\n",
    "        Learning rate (alpha)\n",
    "    n_iterations : int\n",
    "        Number of iterations\n",
    "    verbose : bool\n",
    "        Print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : np.ndarray\n",
    "        Learned weights including bias\n",
    "    cost_history : list\n",
    "        Cost at each iteration\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Add bias column\n",
    "    X_with_bias = np.c_[np.ones((m, 1)), X]\n",
    "    \n",
    "    # Initialize weights randomly\n",
    "    weights = np.random.randn(n + 1) * 0.01\n",
    "    \n",
    "    cost_history = []\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        # 1. Compute predictions\n",
    "        predictions = X_with_bias @ weights\n",
    "        \n",
    "        # 2. Compute errors\n",
    "        errors = predictions - y\n",
    "        \n",
    "        # 3. Compute gradients: (1/m) * X^T * errors\n",
    "        gradients = (X_with_bias.T @ errors) / m\n",
    "        \n",
    "        # 4. Update weights\n",
    "        weights = weights - learning_rate * gradients\n",
    "            print(f\"Iteration {iteration}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return weights, cost_history\n",
    "\n",
    "# Test on simple data\n",
    "np.random.seed(42)\n",
    "X_simple = np.random.rand(100, 1) * 10\n",
    "y_simple = 3 + 2 * X_simple.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "weights, cost_history = batch_gradient_descent(\n",
    "    X_simple, y_simple, \n",
    "    learning_rate=0.01, \n",
    "    n_iterations=1000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal weights: bias={weights[0]:.2f}, slope={weights[1]:.2f}\")\n",
    "print(f\"True line:     bias=3.00, slope=2.00\")\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")\n",
    "\n",
    "# Plot cost history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Iteration (Batch Gradient Descent)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "assert abs(weights[0] - 3) < 1, \"Bias should be close to 3\"\n",
    "assert abs(weights[1] - 2) < 0.5, \"Slope should be close to 2\"\n",
    "assert cost_history[-1] < cost_history[0], \"Cost should decrease\"\n",
    "print(\"\\n\u2713 Batch gradient descent works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Effect of Learning Rate\n",
    "\n",
    "**Task:** Compare different learning rates and visualize their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    weights, cost_history = batch_gradient_descent(\n",
    "        X_simple, y_simple,\n",
    "        learning_rate=lr,\n",
    "        n_iterations=500\n",
    "    )\n",
    "    \n",
    "    plt.plot(cost_history, label=f'lr={lr}', color=color, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Effect of Learning Rate on Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Too small (0.001): Slow convergence\")\n",
    "print(\"- Just right (0.01): Smooth, steady convergence\")\n",
    "print(\"- Too large (0.5): May oscillate or diverge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### Background\n",
    "\n",
    "**Stochastic Gradient Descent** updates weights using one random sample at a time.\n",
    "\n",
    "**Pros:** Fast, can escape local minima, works with large datasets\n",
    "**Cons:** Noisy updates, may not converge to exact minimum\n",
    "\n",
    "### Exercise 3.1: Implement Stochastic Gradient Descent\n",
    "\n",
    "**Task:** Implement SGD with random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, learning_rate=0.01, n_epochs=50, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform stochastic gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target values (n_samples,)\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    n_epochs : int\n",
    "        Number of epochs (full passes through data)\n",
    "    verbose : bool\n",
    "        Print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : np.ndarray\n",
    "        Learned weights\n",
    "    cost_history : list\n",
    "        Cost after each epoch\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Add bias column\n",
    "    X_with_bias = np.c_[np.ones((m, 1)), X]\n",
    "    \n",
    "    # Initialize weights\n",
    "    weights = np.random.randn(n + 1) * 0.01\n",
    "    \n",
    "    cost_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "            # Compute prediction for this sample\n",
    "            prediction = np.dot(xi, weights)\n",
    "            \n",
    "            # Compute error\n",
    "            error = prediction - yi\n",
    "            \n",
    "            # Compute gradient for this sample\n",
    "            gradient = xi.T * error\n",
    "            \n",
    "            # Update weights\n",
    "            weights = weights - learning_rate * gradient.flatten()\n",
    "            print(f\"Epoch {epoch}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return weights, cost_history\n",
    "\n",
    "# Test SGD\n",
    "weights_sgd, cost_history_sgd = stochastic_gradient_descent(\n",
    "    X_simple, y_simple,\n",
    "    learning_rate=0.01,\n",
    "    n_epochs=50,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSGD weights: bias={weights_sgd[0]:.2f}, slope={weights_sgd[1]:.2f}\")\n",
    "print(f\"True line:   bias=3.00, slope=2.00\")\n",
    "\n",
    "# Compare with batch GD\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history, label='Batch GD', alpha=0.7)\n",
    "plt.plot(cost_history_sgd, label='SGD', alpha=0.7)\n",
    "plt.xlabel('Iteration/Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Batch GD vs SGD Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cost_history_sgd)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('SGD Cost (Notice the Noise)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "assert abs(weights_sgd[0] - 3) < 1, \"SGD bias should be close to 3\"\n",
    "assert abs(weights_sgd[1] - 2) < 0.5, \"SGD slope should be close to 2\"\n",
    "print(\"\\n\u2713 Stochastic gradient descent works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Mini-Batch Gradient Descent\n",
    "\n",
    "### Background\n",
    "\n",
    "**Mini-Batch Gradient Descent** is a compromise between batch GD and SGD.\n",
    "Updates weights using small batches (e.g., 32, 64, 128 samples).\n",
    "\n",
    "**Pros:** Faster than batch GD, more stable than SGD, good for large datasets\n",
    "**Cons:** Requires tuning batch size\n",
    "\n",
    "### Exercise 4.1: Implement Mini-Batch Gradient Descent\n",
    "\n",
    "**Task:** Implement mini-batch GD with configurable batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X, y, learning_rate=0.01, n_epochs=50, batch_size=32, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform mini-batch gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Target values (n_samples,)\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    n_epochs : int\n",
    "        Number of epochs\n",
    "    batch_size : int\n",
    "        Size of mini-batches\n",
    "    verbose : bool\n",
    "        Print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : np.ndarray\n",
    "        Learned weights\n",
    "    cost_history : list\n",
    "        Cost after each epoch\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Add bias column\n",
    "    X_with_bias = np.c_[np.ones((m, 1)), X]\n",
    "    \n",
    "    # Initialize weights\n",
    "    weights = np.random.randn(n + 1) * 0.01\n",
    "    \n",
    "    cost_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "            # Compute predictions for batch\n",
    "            predictions = X_batch @ weights\n",
    "            \n",
    "            # Compute errors\n",
    "            errors = predictions - y_batch\n",
    "            \n",
    "            # Compute gradients (average over batch)\n",
    "            gradients = (X_batch.T @ errors) / batch_m\n",
    "            \n",
    "            # Update weights\n",
    "            weights = weights - learning_rate * gradients\n",
    "            print(f\"Epoch {epoch}: Cost = {cost:.4f}\")\n",
    "    \n",
    "    return weights, cost_history\n",
    "\n",
    "# Test with different batch sizes\n",
    "batch_sizes = [1, 16, 32, len(X_simple)]  # 1=SGD, len(X)=Batch GD\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for bs, color in zip(batch_sizes, colors):\n",
    "    weights_mb, cost_history_mb = minibatch_gradient_descent(\n",
    "        X_simple, y_simple,\n",
    "        learning_rate=0.01,\n",
    "        n_epochs=50,\n",
    "        batch_size=bs\n",
    "    )\n",
    "    \n",
    "    label = f'Batch size={bs}' if bs < len(X_simple) else 'Full batch'\n",
    "    plt.plot(cost_history_mb, label=label, color=color, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Effect of Batch Size on Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Mini-batch gradient descent works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Create a Complete GradientDescentRegressor Class\n",
    "\n",
    "### Exercise 5.1: Implement a Scikit-Learn Style Class\n",
    "\n",
    "**Task:** Create a complete class with fit, predict, and score methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentRegressor:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, method='batch', \n",
    "                 batch_size=32, random_state=None):\n",
    "        \"\"\"\n",
    "        Linear Regression using Gradient Descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            Learning rate (alpha)\n",
    "        n_iterations : int\n",
    "            Number of iterations/epochs\n",
    "        method : str\n",
    "            'batch', 'stochastic', or 'minibatch'\n",
    "        batch_size : int\n",
    "            Batch size for mini-batch GD\n",
    "        random_state : int\n",
    "            Random seed\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.method = method\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        self.weights = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Training features\n",
    "        y : np.ndarray\n",
    "            Target values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "        \"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # Choose appropriate gradient descent method\n",
    "        if self.method == 'batch':\n",
    "            self.weights, self.cost_history = batch_gradient_descent(\n",
    "                X, y, self.learning_rate, self.n_iterations)\n",
    "        elif self.method == 'stochastic':\n",
    "            self.weights, self.cost_history = stochastic_gradient_descent(\n",
    "                X, y, self.learning_rate, self.n_iterations) # Adjust args if needed\n",
    "        elif self.method == 'minibatch':\n",
    "            self.weights, self.cost_history = minibatch_gradient_descent(\n",
    "                X, y, self.learning_rate, self.n_iterations, self.batch_size)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        X_with_bias = np.c_[np.ones((len(X), 1)), X]\n",
    "        return X_with_bias @ self.weights\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R\u00b2 score.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Features\n",
    "        y : np.ndarray\n",
    "            True values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            R\u00b2 score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"GradientDescentRegressor class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Apply to California Housing Dataset\n",
    "\n",
    "### Exercise 6.1: Compare All Three Methods\n",
    "\n",
    "**Task:** Apply batch, SGD, and mini-batch GD to a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "\n",
    "print(\"Dataset shape:\", X_housing.shape)\n",
    "print(\"Features:\", housing.feature_names)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# IMPORTANT: Scale features for gradient descent!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nTraining all three methods...\\n\")\n",
    "\n",
    "# Train models\n",
    "methods = ['batch', 'stochastic', 'minibatch']\n",
    "models = {}\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for method, color in zip(methods, colors):\n",
    "    print(f\"Training {method} gradient descent...\")\n",
    "    \n",
    "    model = GradientDescentRegressor(\n",
    "        learning_rate=0.01,\n",
    "        n_iterations=100,\n",
    "        method=method,\n",
    "        batch_size=64,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    train_r2 = model.score(X_train_scaled, y_train)\n",
    "    test_r2 = model.score(X_test_scaled, y_test)\n",
    "    \n",
    "    models[method] = model\n",
    "    \n",
    "    print(f\"  Training time: {training_time:.3f}s\")\n",
    "    print(f\"  Train R\u00b2: {train_r2:.4f}\")\n",
    "    print(f\"  Test R\u00b2:  {test_r2:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Plot cost history\n",
    "    plt.plot(model.cost_history, label=method.capitalize(), color=color, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Iteration/Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Convergence Comparison on California Housing')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Batch GD: Smooth convergence, slowest\")\n",
    "print(\"- SGD: Noisy, fast per epoch\")\n",
    "print(\"- Mini-batch: Good balance between speed and stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Apply to Diabetes Dataset\n",
    "\n",
    "### Exercise 7.1: Hyperparameter Tuning\n",
    "\n",
    "**Task:** Find the best learning rate for the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "print(\"Diabetes Dataset:\")\n",
    "print(f\"Shape: {X_diabetes.shape}\")\n",
    "print(f\"Features: {diabetes.feature_names}\\n\")\n",
    "\n",
    "# Split and scale\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler_d = StandardScaler()\n",
    "X_train_d_scaled = scaler_d.fit_transform(X_train_d)\n",
    "X_test_d_scaled = scaler_d.transform(X_test_d)\n",
    "\n",
    "# Your turn: Test different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "best_r2 = -np.inf\n",
    "best_lr = None\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # Train model with this learning rate\n",
    "    model = GradientDescentRegressor(learning_rate=lr, n_iterations=2000, random_state=42)\n",
    "    model.fit(X_train_d_scaled, y_train_d)\n",
    "    \n",
    "    # Get test R\u00b2\n",
    "    test_r2 = model.score(X_test_d_scaled, y_test_d)\n",
    "    print(f\"Learning rate {lr}: Test R\u00b2 = {test_r2:.4f}\")\n",
    "    \n",
    "    if test_r2 > best_r2:\n",
    "        best_r2 = test_r2\n",
    "        best_lr = lr\n",
    "    \n",
    "    # Plot cost history\n",
    "    plt.plot(model.cost_history, label=f'lr={lr}', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Learning Rate Comparison on Diabetes Dataset')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest learning rate: {best_lr} (Test R\u00b2 = {best_r2:.4f})\")\n",
    "print(\"\\n\u2713 Successfully applied to diabetes dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Compare with Sklearn\n",
    "\n",
    "### Exercise 8.1: Validate Against SGDRegressor\n",
    "\n",
    "**Task:** Compare your implementation with sklearn's SGDRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation\n",
    "model_yours = GradientDescentRegressor(\n",
    "    learning_rate=0.01,\n",
    "    n_iterations=100,\n",
    "    method='minibatch',\n",
    "    batch_size=64,\n",
    "    random_state=42\n",
    ")\n",
    "model_yours.fit(X_train_scaled, y_train)\n",
    "y_pred_yours = model_yours.predict(X_test_scaled)\n",
    "r2_yours = model_yours.score(X_test_scaled, y_test)\n",
    "\n",
    "# Sklearn's implementation\n",
    "model_sklearn = SGDRegressor(\n",
    "    max_iter=100,\n",
    "    learning_rate='constant',\n",
    "    eta0=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "model_sklearn.fit(X_train_scaled, y_train)\n",
    "y_pred_sklearn = model_sklearn.predict(X_test_scaled)\n",
    "r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(\"California Housing - Test Set Results:\")\n",
    "print(f\"Your implementation:    R\u00b2 = {r2_yours:.4f}\")\n",
    "print(f\"Sklearn SGDRegressor:   R\u00b2 = {r2_sklearn:.4f}\")\n",
    "print(f\"Difference:             {abs(r2_yours - r2_sklearn):.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred_yours, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Your Implementation (R\u00b2 = {r2_yours:.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred_sklearn, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Sklearn SGDRegressor (R\u00b2 = {r2_sklearn:.4f})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Implementation validated against sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Learning Rate Decay\n",
    "\n",
    "Implement learning rate decay: $\\alpha_t = \\frac{\\alpha_0}{1 + \\text{decay\\_rate} \\times t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_decay(X, y, initial_lr=0.1, decay_rate=0.01, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent with learning rate decay.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : features\n",
    "    y : target\n",
    "    initial_lr : initial learning rate\n",
    "    decay_rate : decay rate\n",
    "    n_iterations : number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights, cost_history, lr_history\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Hint: Update learning rate each iteration\n",
    "    pass\n",
    "\n",
    "print(\"Challenge 1: Implement learning rate decay!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Momentum\n",
    "\n",
    "Implement gradient descent with momentum:\n",
    "$$v_t = \\beta v_{t-1} + \\nabla J(w)$$\n",
    "$$w := w - \\alpha v_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_momentum(X, y, learning_rate=0.01, momentum=0.9, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent with momentum.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : features\n",
    "    y : target  \n",
    "    learning_rate : learning rate\n",
    "    momentum : momentum coefficient (usually 0.9)\n",
    "    n_iterations : number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights, cost_history\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Initialize velocity vector\n",
    "    # Update velocity and weights\n",
    "    pass\n",
    "\n",
    "print(\"Challenge 2: Implement momentum!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Early Stopping\n",
    "\n",
    "Implement early stopping: stop training when validation loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_with_early_stopping(X_train, y_train, X_val, y_val, \n",
    "                                         learning_rate=0.01, patience=10, \n",
    "                                         max_iterations=1000):\n",
    "    \"\"\"\n",
    "    Gradient descent with early stopping.\n",
    "    \n",
    "    Stop training if validation cost doesn't improve for 'patience' iterations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : training features\n",
    "    y_train : training target\n",
    "    X_val : validation features\n",
    "    y_val : validation target\n",
    "    learning_rate : learning rate\n",
    "    patience : number of iterations to wait for improvement\n",
    "    max_iterations : maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    weights, train_cost_history, val_cost_history\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Track best validation cost\n",
    "    # Stop if no improvement for 'patience' iterations\n",
    "    pass\n",
    "\n",
    "print(\"Challenge 3: Implement early stopping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **When should you use batch GD vs SGD vs mini-batch GD?**\n",
    "   - Consider dataset size, convergence requirements, computational resources\n",
    "\n",
    "2. **Why is feature scaling critical for gradient descent?**\n",
    "   - What happens with features of different scales?\n",
    "\n",
    "3. **How do you choose the learning rate?**\n",
    "   - Too small? Too large? Just right?\n",
    "\n",
    "4. **What are the signs of a good learning rate?**\n",
    "   - Look at the cost curve\n",
    "\n",
    "5. **Why is SGD more likely to escape local minima?**\n",
    "   - Consider the noise in updates\n",
    "\n",
    "6. **How does gradient descent compare to the normal equation?**\n",
    "   - Computational complexity, when each is preferred\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "\u2713 How to implement batch, stochastic, and mini-batch gradient descent  \n",
    "\u2713 The importance of learning rate selection  \n",
    "\u2713 How to visualize convergence with loss curves  \n",
    "\u2713 When to use each variant of gradient descent  \n",
    "\u2713 Feature scaling is essential for gradient descent  \n",
    "\u2713 How to apply gradient descent to real datasets  \n",
    "\u2713 Validation against sklearn implementations  \n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **Batch GD**: Stable, exact gradient, slow for large datasets\n",
    "- **SGD**: Fast, noisy, can escape local minima\n",
    "- **Mini-batch GD**: Best of both worlds, most commonly used\n",
    "- **Feature scaling**: Always scale features for gradient descent\n",
    "- **Learning rate**: Most important hyperparameter\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete Exercise 3 on Logistic Regression\n",
    "- Review the [Gradient Descent lesson](https://jumpingsphinx.github.io/module2-regression/02-gradient-descent/)\n",
    "- Experiment with momentum and adaptive learning rates\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
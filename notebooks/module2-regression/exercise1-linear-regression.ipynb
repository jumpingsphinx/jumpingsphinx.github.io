{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Exercise 1: Linear Regression from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module2-regression/exercise1-linear-regression.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "\n",
    "- Implement linear regression using the normal equation\n",
    "- Fit models to real datasets\n",
    "- Make predictions and evaluate performance\n",
    "- Visualize regression lines and residuals\n",
    "- Handle multiple features (multivariate regression)\n",
    "- Compare your implementation with sklearn\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of linear algebra (vectors, matrices)\n",
    "- Basic calculus concepts\n",
    "- NumPy proficiency\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this cell first to import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Simple Linear Regression (One Feature)\n",
    "\n",
    "### Background\n",
    "\n",
    "Simple linear regression models the relationship between one feature and a target:\n",
    "\n",
    "$$y = w_0 + w_1 x$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the prediction\n",
    "- $x$ is the input feature\n",
    "- $w_0$ is the bias (intercept)\n",
    "- $w_1$ is the slope (weight)\n",
    "\n",
    "### Exercise 1.1: Fit a Line to Data\n",
    "\n",
    "**Task:** Implement simple linear regression from scratch using the closed-form solution.\n",
    "\n",
    "For the normal equation: $\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data: y = 3 + 2*x + noise\n",
    "np.random.seed(42)\n",
    "X_simple = np.random.rand(100, 1) * 10\n",
    "y_simple = 3 + 2 * X_simple.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "def fit_simple_linear_regression(X, y):\n",
    "    \"\"\"\n",
    "    Fit simple linear regression using the normal equation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Input features (n_samples, 1)\n",
    "    y : np.ndarray\n",
    "        Target values (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w0 : float\n",
    "        Bias term (intercept)\n",
    "    w1 : float\n",
    "        Slope (weight)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Hint: Add a column of 1s to X for the bias term\n",
    "    X_with_bias = \n",
    "    \n",
    "    # Apply normal equation: w = (X^T X)^(-1) X^T y\n",
    "    weights = \n",
    "    \n",
    "    w0 = weights[0]  # bias\n",
    "    w1 = weights[1]  # slope\n",
    "    \n",
    "    return w0, w1\n",
    "\n",
    "# Fit the model\n",
    "w0, w1 = fit_simple_linear_regression(X_simple, y_simple)\n",
    "\n",
    "print(f\"Fitted line: y = {w0:.2f} + {w1:.2f}x\")\n",
    "print(f\"True line:   y = 3.00 + 2.00x\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple, alpha=0.5, label='Data')\n",
    "X_line = np.linspace(0, 10, 100)\n",
    "y_line = w0 + w1 * X_line\n",
    "plt.plot(X_line, y_line, 'r-', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "assert abs(w0 - 3) < 1, \"Bias should be close to 3\"\n",
    "assert abs(w1 - 2) < 0.5, \"Slope should be close to 2\"\n",
    "print(\"\\n✓ Simple linear regression works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implement a Linear Regression Class\n",
    "\n",
    "### Exercise 2.1: Complete the LinearRegression Class\n",
    "\n",
    "**Task:** Implement a full linear regression class with fit, predict, and score methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Linear Regression using Normal Equation.\n",
    "        \"\"\"\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit linear regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Training features (n_samples, n_features)\n",
    "        y : np.ndarray\n",
    "            Target values (n_samples,)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        # Add bias column\n",
    "        X_with_bias = \n",
    "        \n",
    "        # Normal equation\n",
    "        weights_all = \n",
    "        \n",
    "        # Separate bias and weights\n",
    "        self.bias = \n",
    "        self.weights = \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Features (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Predictions (n_samples,)\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        return \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R² score.\n",
    "        \n",
    "        R² = 1 - (SS_res / SS_tot)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Features\n",
    "        y : np.ndarray\n",
    "            True values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            R² score\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = \n",
    "        ss_tot = \n",
    "        r2 = \n",
    "        return r2\n",
    "\n",
    "print(\"LinearRegression class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Test on Real Dataset - California Housing\n",
    "\n",
    "### Exercise 3.1: Apply to California Housing Dataset\n",
    "\n",
    "**Task:** Use your LinearRegression class on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "\n",
    "print(\"Dataset shape:\", X_housing.shape)\n",
    "print(\"Features:\", housing.feature_names)\n",
    "print(\"Target: Median house value (in $100,000s)\")\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit your model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_r2 = model.score(X_train, y_train)\n",
    "test_r2 = model.score(X_test, y_test)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nYour Model Performance:\")\n",
    "print(f\"Train R²: {train_r2:.4f}\")\n",
    "print(f\"Test R²:  {test_r2:.4f}\")\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE:  {test_mse:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "\n",
    "# Compare with sklearn\n",
    "sklearn_model = SklearnLinearRegression()\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_r2 = sklearn_model.score(X_test, y_test)\n",
    "\n",
    "print(\"\\nSklearn Model:\")\n",
    "print(f\"Test R²: {sklearn_r2:.4f}\")\n",
    "\n",
    "assert np.allclose(test_r2, sklearn_r2, atol=0.001), \"Your R² should match sklearn's\"\n",
    "print(\"\\n✓ Your implementation matches sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Model Evaluation Metrics\n",
    "\n",
    "### Exercise 4.1: Implement Evaluation Metrics\n",
    "\n",
    "**Task:** Implement common regression metrics from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_manual(y_true, y_pred):\n",
    "    \"\"\"Calculate MSE: (1/n) Σ(y_true - y_pred)²\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate RMSE: sqrt(MSE)\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def mean_absolute_error_manual(y_true, y_pred):\n",
    "    \"\"\"Calculate MAE: (1/n) Σ|y_true - y_pred|\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    \"\"\"Calculate R²: 1 - (SS_res / SS_tot)\"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your metrics\n",
    "mse_manual = mean_squared_error_manual(y_test, y_test_pred)\n",
    "rmse_manual = root_mean_squared_error(y_test, y_test_pred)\n",
    "mae_manual = mean_absolute_error_manual(y_test, y_test_pred)\n",
    "r2_manual = r2_score_manual(y_test, y_test_pred)\n",
    "\n",
    "# Compare with sklearn\n",
    "mse_sklearn = mean_squared_error(y_test, y_test_pred)\n",
    "mae_sklearn = mean_absolute_error(y_test, y_test_pred)\n",
    "r2_sklearn = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Your Metrics:\")\n",
    "print(f\"MSE:  {mse_manual:.4f}\")\n",
    "print(f\"RMSE: {rmse_manual:.4f}\")\n",
    "print(f\"MAE:  {mae_manual:.4f}\")\n",
    "print(f\"R²:   {r2_manual:.4f}\")\n",
    "\n",
    "print(\"\\nSklearn Metrics:\")\n",
    "print(f\"MSE:  {mse_sklearn:.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mse_sklearn):.4f}\")\n",
    "print(f\"MAE:  {mae_sklearn:.4f}\")\n",
    "print(f\"R²:   {r2_sklearn:.4f}\")\n",
    "\n",
    "assert np.isclose(mse_manual, mse_sklearn), \"MSE should match\"\n",
    "assert np.isclose(mae_manual, mae_sklearn), \"MAE should match\"\n",
    "assert np.isclose(r2_manual, r2_sklearn), \"R² should match\"\n",
    "print(\"\\n✓ All metrics implemented correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Residual Analysis\n",
    "\n",
    "### Exercise 5.1: Visualize Residuals\n",
    "\n",
    "**Task:** Create visualizations to analyze model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test, y_test_pred, alpha=0.5)\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Values')\n",
    "axes[0, 0].set_ylabel('Predicted Values')\n",
    "axes[0, 0].set_title('Predicted vs Actual')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "axes[0, 1].scatter(y_test_pred, residuals, alpha=0.5)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residual Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Histogram of Residuals\n",
    "axes[1, 0].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Residuals')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Q-Q Plot\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Residual Analysis:\")\n",
    "print(f\"Mean of residuals: {residuals.mean():.6f} (should be ~0)\")\n",
    "print(f\"Std of residuals: {residuals.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Feature Importance\n",
    "\n",
    "### Exercise 6.1: Analyze Coefficient Magnitudes\n",
    "\n",
    "**Task:** Understand which features are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features to compare coefficients fairly\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit model on scaled data\n",
    "model_scaled = LinearRegression()\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = np.abs(model_scaled.weights)\n",
    "feature_names = housing.feature_names\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.title('Feature Importance (on scaled features)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 3 Most Important Features:\")\n",
    "top_3_idx = np.argsort(feature_importance)[-3:][::-1]\n",
    "for idx in top_3_idx:\n",
    "    print(f\"  {feature_names[idx]}: {feature_importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Test on Another Dataset - Diabetes\n",
    "\n",
    "### Exercise 7.1: Apply to Diabetes Dataset\n",
    "\n",
    "**Task:** Validate your implementation on a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "print(\"Diabetes Dataset:\")\n",
    "print(f\"Shape: {X_diabetes.shape}\")\n",
    "print(f\"Features: {diabetes.feature_names}\")\n",
    "print(f\"Target: Disease progression one year after baseline\\n\")\n",
    "\n",
    "# Split data\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Your turn: Fit your model and evaluate\n",
    "# Your code here\n",
    "model_diabetes = \n",
    "\n",
    "# Calculate metrics\n",
    "train_r2_d = \n",
    "test_r2_d = \n",
    "test_rmse_d = \n",
    "\n",
    "print(\"\\nYour Model Performance on Diabetes:\")\n",
    "print(f\"Train R²: {train_r2_d:.4f}\")\n",
    "print(f\"Test R²:  {test_r2_d:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse_d:.4f}\")\n",
    "\n",
    "# Compare with sklearn\n",
    "sklearn_diabetes = SklearnLinearRegression()\n",
    "sklearn_diabetes.fit(X_train_d, y_train_d)\n",
    "sklearn_r2_d = sklearn_diabetes.score(X_test_d, y_test_d)\n",
    "\n",
    "print(f\"\\nSklearn Test R²: {sklearn_r2_d:.4f}\")\n",
    "\n",
    "assert np.allclose(test_r2_d, sklearn_r2_d, atol=0.001), \"Should match sklearn\"\n",
    "print(\"\\n✓ Successfully applied to diabetes dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Polynomial Regression\n",
    "\n",
    "Extend linear regression to fit polynomial curves by creating polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Generate non-linear data\n",
    "X_poly = np.linspace(0, 3, 100).reshape(-1, 1)\n",
    "y_poly = 0.5 * X_poly**2 + X_poly + 2 + np.random.randn(100, 1) * 0.5\n",
    "\n",
    "# Your task: Create polynomial features and fit\n",
    "# Hint: Use PolynomialFeatures(degree=2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly_features = \n",
    "\n",
    "model_poly = \n",
    "\n",
    "# Plot\n",
    "y_pred_poly = \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_poly, y_poly, alpha=0.5, label='Data')\n",
    "plt.plot(X_poly, y_pred_poly, 'r-', linewidth=2, label='Polynomial fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Polynomial Regression (degree=2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Implement Using Pseudoinverse\n",
    "\n",
    "Use `np.linalg.pinv()` instead of explicitly computing the inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_pinv(X, y):\n",
    "    \"\"\"\n",
    "    Linear regression using pseudoinverse.\n",
    "    \n",
    "    More numerically stable than explicit inverse.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "# weights = linear_regression_pinv(X_train, y_train)\n",
    "print(\"Challenge: Implement using pseudoinverse!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Weighted Linear Regression\n",
    "\n",
    "Implement linear regression where some samples have more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_linear_regression(X, y, weights):\n",
    "    \"\"\"\n",
    "    Weighted linear regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : features\n",
    "    y : target\n",
    "    weights : sample weights (n_samples,)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    coefficients\n",
    "    \"\"\"\n",
    "    # Hint: w = (X^T W X)^(-1) X^T W y\n",
    "    # where W is diagonal matrix of weights\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "print(\"Challenge: Implement weighted linear regression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **When would the normal equation fail?**\n",
    "   - Think about when X^T X is not invertible\n",
    "\n",
    "2. **Why do we need to add a bias term?**\n",
    "   - What happens if the line doesn't pass through the origin?\n",
    "\n",
    "3. **What does R² = 0.6 mean?**\n",
    "   - Is that good or bad? Depends on the problem!\n",
    "\n",
    "4. **Why analyze residuals?**\n",
    "   - What patterns indicate model problems?\n",
    "\n",
    "5. **When should you scale features?**\n",
    "   - For the normal equation vs gradient descent?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "✓ How to implement linear regression from scratch  \n",
    "✓ The normal equation: w = (X^T X)^(-1) X^T y  \n",
    "✓ Evaluation metrics: MSE, RMSE, MAE, R²  \n",
    "✓ Residual analysis for diagnosing model fit  \n",
    "✓ Feature importance through coefficient analysis  \n",
    "✓ Application to real-world datasets  \n",
    "✓ Validation against sklearn implementation  \n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete Exercise 2 on Gradient Descent\n",
    "- Review the [Linear Regression lesson](https://jumpingsphinx.github.io/module2-regression/01-linear-regression/)\n",
    "- Try polynomial regression on different datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Exercise 4: Regularization from Scratch\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module2-regression/exercise4-regularization.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "\n",
    "- Understand overfitting and how regularization helps\n",
    "- Implement Ridge regression (L2 regularization) from scratch\n",
    "- Implement Lasso regression (L1 regularization) from scratch\n",
    "- Implement Elastic Net (combination of L1 and L2)\n",
    "- Visualize regularization paths\n",
    "- Use cross-validation for hyperparameter tuning\n",
    "- Compare different regularization techniques\n",
    "- Apply regularization to real datasets\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of linear regression\n",
    "- Knowledge of gradient descent\n",
    "- Understanding of bias-variance tradeoff\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this cell first to import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes, make_regression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Overfitting\n",
    "\n",
    "### Background\n",
    "\n",
    "**Overfitting** occurs when a model learns noise in training data instead of true patterns.\n",
    "\n",
    "**Regularization** adds a penalty term to the cost function to discourage complex models:\n",
    "\n",
    "**Ridge (L2):**\n",
    "$$J(\\mathbf{w}) = \\text{MSE} + \\frac{\\lambda}{2} \\sum_{j=1}^{n} w_j^2$$\n",
    "\n",
    "**Lasso (L1):**\n",
    "$$J(\\mathbf{w}) = \\text{MSE} + \\lambda \\sum_{j=1}^{n} |w_j|$$\n",
    "\n",
    "**Elastic Net:**\n",
    "$$J(\\mathbf{w}) = \\text{MSE} + \\lambda_1 \\sum_{j=1}^{n} |w_j| + \\frac{\\lambda_2}{2} \\sum_{j=1}^{n} w_j^2$$\n",
    "\n",
    "### Exercise 1.1: Demonstrate Overfitting\n",
    "\n",
    "**Task:** Create polynomial features and show overfitting without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X_simple = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y_simple = 2 * np.sin(X_simple).ravel() + np.random.randn(50) * 0.5\n",
    "\n",
    "# Split data\n",
    "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(\n",
    "    X_simple, y_simple, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Test different polynomial degrees\n",
    "degrees = [1, 3, 9, 15]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (degree, color) in enumerate(zip(degrees, colors)):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train_simple)\n",
    "    X_test_poly = poly.transform(X_test_simple)\n",
    "    \n",
    "    # Fit linear regression\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train_simple)\n",
    "    \n",
    "    # Predictions\n",
    "    X_plot = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = model.predict(X_plot_poly)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = r2_score(y_train_simple, model.predict(X_train_poly))\n",
    "    test_score = r2_score(y_test_simple, model.predict(X_test_poly))\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].scatter(X_train_simple, y_train_simple, alpha=0.6, label='Train')\n",
    "    axes[i].scatter(X_test_simple, y_test_simple, alpha=0.6, label='Test', color='red')\n",
    "    axes[i].plot(X_plot, y_plot, color=color, linewidth=2, label='Model')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].set_title(f'Degree {degree}\\nTrain R²={train_score:.3f}, Test R²={test_score:.3f}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_ylim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Degree 1 (linear): Underfitting - too simple\")\n",
    "print(\"- Degree 3: Good balance\")\n",
    "print(\"- Degree 9+: Overfitting - wiggly, poor generalization\")\n",
    "print(\"- Notice: High training R², low test R² = overfitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Ridge Regression (L2 Regularization)\n",
    "\n",
    "### Background\n",
    "\n",
    "Ridge regression adds L2 penalty: sum of squared weights.\n",
    "\n",
    "**Closed-form solution:**\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{y}$$\n",
    "\n",
    "**Gradient descent update:**\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} + \\lambda w_j$$\n",
    "\n",
    "### Exercise 2.1: Implement Ridge Regression (Closed-Form)\n",
    "\n",
    "**Task:** Implement Ridge using the closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        \"\"\"\n",
    "        Ridge Regression using closed-form solution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float\n",
    "            Regularization strength (lambda)\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Ridge regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Training features (n_samples, n_features)\n",
    "        y : np.ndarray\n",
    "            Target values (n_samples,)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones((m, 1)), X]\n",
    "        \n",
    "        # Your code here\n",
    "        # Ridge formula: w = (X^T X + alpha * I)^(-1) X^T y\n",
    "        # Note: Don't regularize bias term (first element)\n",
    "        \n",
    "        # Create regularization matrix (don't penalize bias)\n",
    "        reg_matrix = np.eye(n + 1)\n",
    "        reg_matrix[0, 0] = 0  # Don't regularize bias\n",
    "        \n",
    "        # Compute weights\n",
    "        self.weights = \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        X_with_bias = \n",
    "        return \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R² score.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test Ridge on polynomial features\n",
    "degree = 9  # High degree to induce overfitting\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly.fit_transform(X_train_simple)\n",
    "X_test_poly = poly.transform(X_test_simple)\n",
    "\n",
    "# Compare different alpha values\n",
    "alphas = [0, 0.1, 1.0, 10.0]\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "for i, alpha in enumerate(alphas, 1):\n",
    "    # Fit Ridge\n",
    "    ridge = RidgeRegression(alpha=alpha)\n",
    "    ridge.fit(X_train_poly, y_train_simple)\n",
    "    \n",
    "    # Predictions\n",
    "    X_plot = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "    X_plot_poly = poly.transform(X_plot)\n",
    "    y_plot = ridge.predict(X_plot_poly)\n",
    "    \n",
    "    # Scores\n",
    "    train_r2 = ridge.score(X_train_poly, y_train_simple)\n",
    "    test_r2 = ridge.score(X_test_poly, y_test_simple)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.scatter(X_train_simple, y_train_simple, alpha=0.6, label='Train')\n",
    "    plt.scatter(X_test_simple, y_test_simple, alpha=0.6, color='red', label='Test')\n",
    "    plt.plot(X_plot, y_plot, linewidth=2, label='Ridge')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Ridge α={alpha}\\nTrain R²={train_r2:.3f}, Test R²={test_r2:.3f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-4, 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Ridge regression implemented!\")\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- α=0: No regularization, overfits\")\n",
    "print(\"- α=0.1-1.0: Good balance\")\n",
    "print(\"- α=10.0: Too much regularization, underfits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Ridge with Gradient Descent\n",
    "\n",
    "**Task:** Implement Ridge using gradient descent (useful for large datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegressionGD:\n",
    "    def __init__(self, alpha=1.0, learning_rate=0.01, n_iterations=1000, random_state=None):\n",
    "        \"\"\"\n",
    "        Ridge Regression using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float\n",
    "            Regularization strength\n",
    "        learning_rate : float\n",
    "            Learning rate\n",
    "        n_iterations : int\n",
    "            Number of iterations\n",
    "        random_state : int\n",
    "            Random seed\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.random_state = random_state\n",
    "        self.weights = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit using gradient descent.\n",
    "        \"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Add bias\n",
    "        X_with_bias = np.c_[np.ones((m, 1)), X]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.random.randn(n + 1) * 0.01\n",
    "        \n",
    "        # Gradient descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Your code here\n",
    "            \n",
    "            # Predictions\n",
    "            predictions = \n",
    "            \n",
    "            # Errors\n",
    "            errors = \n",
    "            \n",
    "            # Gradients with L2 regularization\n",
    "            # Don't regularize bias (index 0)\n",
    "            gradients = np.zeros(n + 1)\n",
    "            gradients[0] =   # Bias gradient (no regularization)\n",
    "            gradients[1:] =   # Other gradients (with regularization)\n",
    "            \n",
    "            # Update weights\n",
    "            self.weights = \n",
    "            \n",
    "            # Compute cost (MSE + L2 penalty)\n",
    "            mse = np.mean(errors ** 2)\n",
    "            l2_penalty = (self.alpha / (2 * m)) * np.sum(self.weights[1:] ** 2)\n",
    "            cost = mse + l2_penalty\n",
    "            self.cost_history.append(cost)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_with_bias = np.c_[np.ones((len(X), 1)), X]\n",
    "        return X_with_bias @ self.weights\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test gradient descent version\n",
    "ridge_gd = RidgeRegressionGD(alpha=1.0, learning_rate=0.01, n_iterations=1000)\n",
    "ridge_gd.fit(X_train_poly, y_train_simple)\n",
    "\n",
    "print(f\"Ridge GD Test R²: {ridge_gd.score(X_test_poly, y_test_simple):.4f}\")\n",
    "\n",
    "# Plot cost history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ridge_gd.cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE + L2 Penalty)')\n",
    "plt.title('Ridge Regression Training (Gradient Descent)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Ridge with gradient descent works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Lasso Regression (L1 Regularization)\n",
    "\n",
    "### Background\n",
    "\n",
    "Lasso uses L1 penalty: sum of absolute weights.\n",
    "\n",
    "**Key property**: Lasso can set weights to exactly zero, performing **feature selection**.\n",
    "\n",
    "**Gradient (subgradient):**\n",
    "$$\\frac{\\partial J}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h(\\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)} + \\lambda \\cdot \\text{sign}(w_j)$$\n",
    "\n",
    "### Exercise 3.1: Implement Lasso Regression\n",
    "\n",
    "**Task:** Implement Lasso using coordinate descent (standard approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(x, lambda_):\n",
    "    \"\"\"\n",
    "    Soft thresholding operator for Lasso.\n",
    "    \n",
    "    soft_threshold(x, λ) = sign(x) * max(|x| - λ, 0)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return \n",
    "\n",
    "class LassoRegression:\n",
    "    def __init__(self, alpha=1.0, n_iterations=1000, tolerance=1e-4):\n",
    "        \"\"\"\n",
    "        Lasso Regression using coordinate descent.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float\n",
    "            Regularization strength\n",
    "        n_iterations : int\n",
    "            Maximum iterations\n",
    "        tolerance : float\n",
    "            Convergence tolerance\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.n_iterations = n_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.weights = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit using coordinate descent.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Add bias\n",
    "        X_with_bias = np.c_[np.ones((m, 1)), X]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.zeros(n + 1)\n",
    "        \n",
    "        # Coordinate descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            weights_old = self.weights.copy()\n",
    "            \n",
    "            # Update each weight\n",
    "            for j in range(n + 1):\n",
    "                # Your code here\n",
    "                # Compute residual without feature j\n",
    "                X_j = X_with_bias[:, j]\n",
    "                y_pred = X_with_bias @ self.weights\n",
    "                residual = y - y_pred + self.weights[j] * X_j\n",
    "                \n",
    "                # Update weight j\n",
    "                rho_j = X_j @ residual\n",
    "                \n",
    "                if j == 0:  # Don't regularize bias\n",
    "                    self.weights[j] = rho_j / m\n",
    "                else:\n",
    "                    # Apply soft thresholding\n",
    "                    self.weights[j] = \n",
    "            \n",
    "            # Compute cost\n",
    "            y_pred = X_with_bias @ self.weights\n",
    "            mse = np.mean((y - y_pred) ** 2)\n",
    "            l1_penalty = self.alpha * np.sum(np.abs(self.weights[1:]))\n",
    "            cost = mse + l1_penalty\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(self.weights - weights_old)) < self.tolerance:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_with_bias = np.c_[np.ones((len(X), 1)), X]\n",
    "        return X_with_bias @ self.weights\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test Lasso\n",
    "lasso = LassoRegression(alpha=0.1, n_iterations=1000)\n",
    "lasso.fit(X_train_poly, y_train_simple)\n",
    "\n",
    "print(f\"Lasso Test R²: {lasso.score(X_test_poly, y_test_simple):.4f}\")\n",
    "print(f\"Number of non-zero weights: {np.sum(np.abs(lasso.weights) > 1e-5)} / {len(lasso.weights)}\")\n",
    "\n",
    "# Compare Ridge vs Lasso coefficients\n",
    "ridge_compare = RidgeRegression(alpha=0.1)\n",
    "ridge_compare.fit(X_train_poly, y_train_simple)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(len(ridge_compare.weights)), ridge_compare.weights, alpha=0.7)\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.title('Ridge Coefficients (α=0.1)\\nShrinks but keeps all features')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(len(lasso.weights)), lasso.weights, alpha=0.7, color='orange')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Weight Value')\n",
    "plt.title(f'Lasso Coefficients (α=0.1)\\nSets {np.sum(np.abs(lasso.weights) < 1e-5)} features to zero')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Lasso regression implemented!\")\n",
    "print(\"\\nKey difference:\")\n",
    "print(\"- Ridge: Shrinks all weights\")\n",
    "print(\"- Lasso: Sets some weights to exactly zero (feature selection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Elastic Net\n",
    "\n",
    "### Background\n",
    "\n",
    "**Elastic Net** combines L1 and L2 regularization:\n",
    "\n",
    "$$J(\\mathbf{w}) = \\text{MSE} + \\alpha \\rho \\sum |w_j| + \\frac{\\alpha(1-\\rho)}{2} \\sum w_j^2$$\n",
    "\n",
    "Where:\n",
    "- $\\rho \\in [0, 1]$ controls L1 vs L2 ratio\n",
    "- $\\rho = 0$: Pure Ridge\n",
    "- $\\rho = 1$: Pure Lasso\n",
    "\n",
    "### Exercise 4.1: Implement Elastic Net\n",
    "\n",
    "**Task:** Combine Ridge and Lasso into Elastic Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNetRegression:\n",
    "    def __init__(self, alpha=1.0, l1_ratio=0.5, n_iterations=1000, tolerance=1e-4):\n",
    "        \"\"\"\n",
    "        Elastic Net Regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        alpha : float\n",
    "            Total regularization strength\n",
    "        l1_ratio : float\n",
    "            Ratio of L1 regularization (0 = Ridge, 1 = Lasso)\n",
    "        n_iterations : int\n",
    "            Maximum iterations\n",
    "        tolerance : float\n",
    "            Convergence tolerance\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.n_iterations = n_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.weights = None\n",
    "        self.cost_history = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit using coordinate descent.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Add bias\n",
    "        X_with_bias = np.c_[np.ones((m, 1)), X]\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = np.zeros(n + 1)\n",
    "        \n",
    "        # L1 and L2 coefficients\n",
    "        l1_coef = self.alpha * self.l1_ratio\n",
    "        l2_coef = self.alpha * (1 - self.l1_ratio)\n",
    "        \n",
    "        # Coordinate descent\n",
    "        for iteration in range(self.n_iterations):\n",
    "            weights_old = self.weights.copy()\n",
    "            \n",
    "            # Update each weight\n",
    "            for j in range(n + 1):\n",
    "                # Your code here\n",
    "                X_j = X_with_bias[:, j]\n",
    "                y_pred = X_with_bias @ self.weights\n",
    "                residual = y - y_pred + self.weights[j] * X_j\n",
    "                \n",
    "                rho_j = X_j @ residual\n",
    "                \n",
    "                if j == 0:  # Don't regularize bias\n",
    "                    self.weights[j] = rho_j / m\n",
    "                else:\n",
    "                    # Elastic Net update: combine L1 (soft threshold) and L2\n",
    "                    # w_j = soft_threshold(rho_j, l1_coef) / (1 + l2_coef)\n",
    "                    self.weights[j] = \n",
    "            \n",
    "            # Compute cost\n",
    "            y_pred = X_with_bias @ self.weights\n",
    "            mse = np.mean((y - y_pred) ** 2)\n",
    "            l1_penalty = l1_coef * np.sum(np.abs(self.weights[1:]))\n",
    "            l2_penalty = (l2_coef / 2) * np.sum(self.weights[1:] ** 2)\n",
    "            cost = mse + l1_penalty + l2_penalty\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(self.weights - weights_old)) < self.tolerance:\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_with_bias = np.c_[np.ones((len(X), 1)), X]\n",
    "        return X_with_bias @ self.weights\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test Elastic Net with different l1_ratio values\n",
    "l1_ratios = [0.0, 0.5, 1.0]  # Ridge, Elastic Net, Lasso\n",
    "names = ['Ridge (l1=0)', 'Elastic Net (l1=0.5)', 'Lasso (l1=1)']\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, (l1_ratio, name) in enumerate(zip(l1_ratios, names), 1):\n",
    "    elastic = ElasticNetRegression(alpha=0.1, l1_ratio=l1_ratio, n_iterations=1000)\n",
    "    elastic.fit(X_train_poly, y_train_simple)\n",
    "    \n",
    "    test_r2 = elastic.score(X_test_poly, y_test_simple)\n",
    "    n_zeros = np.sum(np.abs(elastic.weights) < 1e-5)\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    plt.bar(range(len(elastic.weights)), elastic.weights, alpha=0.7)\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Weight Value')\n",
    "    plt.title(f'{name}\\nTest R²={test_r2:.3f}, Zeros={n_zeros}/{len(elastic.weights)}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Elastic Net implemented!\")\n",
    "print(\"\\nElastic Net advantages:\")\n",
    "print(\"- Combines benefits of Ridge and Lasso\")\n",
    "print(\"- Can select groups of correlated features (unlike Lasso)\")\n",
    "print(\"- More stable than Lasso when features are correlated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Regularization Paths\n",
    "\n",
    "### Exercise 5.1: Visualize Regularization Paths\n",
    "\n",
    "**Task:** Plot how coefficients change as regularization strength increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regularization path\n",
    "alphas = np.logspace(-3, 2, 50)  # 0.001 to 100\n",
    "\n",
    "ridge_coefs = []\n",
    "lasso_coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # Ridge\n",
    "    ridge = RidgeRegression(alpha=alpha)\n",
    "    ridge.fit(X_train_poly, y_train_simple)\n",
    "    ridge_coefs.append(ridge.weights[1:])  # Exclude bias\n",
    "    \n",
    "    # Lasso\n",
    "    lasso = LassoRegression(alpha=alpha, n_iterations=1000)\n",
    "    lasso.fit(X_train_poly, y_train_simple)\n",
    "    lasso_coefs.append(lasso.weights[1:])  # Exclude bias\n",
    "\n",
    "ridge_coefs = np.array(ridge_coefs)\n",
    "lasso_coefs = np.array(lasso_coefs)\n",
    "\n",
    "# Plot regularization paths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Ridge path\n",
    "for i in range(ridge_coefs.shape[1]):\n",
    "    axes[0].plot(alphas, ridge_coefs[:, i], alpha=0.7)\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Regularization Strength (α)')\n",
    "axes[0].set_ylabel('Coefficient Value')\n",
    "axes[0].set_title('Ridge Regularization Path\\nCoefficients shrink gradually')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "# Lasso path\n",
    "for i in range(lasso_coefs.shape[1]):\n",
    "    axes[1].plot(alphas, lasso_coefs[:, i], alpha=0.7)\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Regularization Strength (α)')\n",
    "axes[1].set_ylabel('Coefficient Value')\n",
    "axes[1].set_title('Lasso Regularization Path\\nCoefficients drop to zero')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Ridge: Coefficients approach zero but never reach it\")\n",
    "print(\"- Lasso: Coefficients hit zero at different α values\")\n",
    "print(\"- Lasso performs automatic feature selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Cross-Validation for Hyperparameter Tuning\n",
    "\n",
    "### Background\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "1. Split data into K folds\n",
    "2. Train on K-1 folds, validate on 1 fold\n",
    "3. Repeat K times\n",
    "4. Average performance\n",
    "\n",
    "### Exercise 6.1: Implement Cross-Validation\n",
    "\n",
    "**Task:** Find the best alpha using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_ridge(X, y, alphas, n_folds=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for Ridge regression.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : features\n",
    "    y : target\n",
    "    alphas : list of alpha values to test\n",
    "    n_folds : number of folds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mean_scores : mean R² for each alpha\n",
    "    std_scores : std of R² for each alpha\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    mean_scores = []\n",
    "    std_scores = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        fold_scores = []\n",
    "        \n",
    "        # Your code here\n",
    "        for train_idx, val_idx in kfold.split(X):\n",
    "            # Split data\n",
    "            X_train_fold = X[train_idx]\n",
    "            y_train_fold = y[train_idx]\n",
    "            X_val_fold = X[val_idx]\n",
    "            y_val_fold = y[val_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model = \n",
    "            \n",
    "            # Evaluate\n",
    "            score = \n",
    "            fold_scores.append(score)\n",
    "        \n",
    "        mean_scores.append(np.mean(fold_scores))\n",
    "        std_scores.append(np.std(fold_scores))\n",
    "    \n",
    "    return np.array(mean_scores), np.array(std_scores)\n",
    "\n",
    "# Test different alphas\n",
    "alphas_cv = np.logspace(-2, 2, 20)\n",
    "mean_scores, std_scores = cross_validate_ridge(X_train_poly, y_train_simple, alphas_cv)\n",
    "\n",
    "# Find best alpha\n",
    "best_idx = np.argmax(mean_scores)\n",
    "best_alpha = alphas_cv[best_idx]\n",
    "best_score = mean_scores[best_idx]\n",
    "\n",
    "# Plot CV scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(alphas_cv, mean_scores, yerr=std_scores, marker='o', capsize=5)\n",
    "plt.axvline(x=best_alpha, color='r', linestyle='--', \n",
    "           label=f'Best α={best_alpha:.3f} (R²={best_score:.3f})')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Regularization Strength (α)')\n",
    "plt.ylabel('Mean Cross-Validation R²')\n",
    "plt.title('Cross-Validation for Ridge Regression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best alpha: {best_alpha:.4f}\")\n",
    "print(f\"Best CV R²: {best_score:.4f} ± {std_scores[best_idx]:.4f}\")\n",
    "\n",
    "# Train final model with best alpha\n",
    "ridge_final = RidgeRegression(alpha=best_alpha)\n",
    "ridge_final.fit(X_train_poly, y_train_simple)\n",
    "test_r2 = ridge_final.score(X_test_poly, y_test_simple)\n",
    "\n",
    "print(f\"Test R² with best alpha: {test_r2:.4f}\")\n",
    "print(\"\\n✓ Cross-validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Apply to Diabetes Dataset\n",
    "\n",
    "### Exercise 7.1: Compare All Regularization Methods\n",
    "\n",
    "**Task:** Apply Ridge, Lasso, and Elastic Net to the diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "print(\"Diabetes Dataset:\")\n",
    "print(f\"Shape: {X_diabetes.shape}\")\n",
    "print(f\"Features: {diabetes.feature_names}\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_d_scaled = scaler.fit_transform(X_train_d)\n",
    "X_test_d_scaled = scaler.transform(X_test_d)\n",
    "\n",
    "# Your turn: Train all three models\n",
    "# Find best alpha using cross-validation for each\n",
    "\n",
    "alphas_test = np.logspace(-2, 2, 20)\n",
    "\n",
    "# Ridge\n",
    "ridge_scores, _ = cross_validate_ridge(X_train_d_scaled, y_train_d, alphas_test)\n",
    "best_alpha_ridge = alphas_test[np.argmax(ridge_scores)]\n",
    "ridge_d = RidgeRegression(alpha=best_alpha_ridge)\n",
    "ridge_d.fit(X_train_d_scaled, y_train_d)\n",
    "\n",
    "# Lasso (Your code here)\n",
    "lasso_d = \n",
    "\n",
    "# Elastic Net (Your code here)\n",
    "elastic_d = \n",
    "\n",
    "# Baseline (no regularization)\n",
    "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
    "baseline = SklearnLinearRegression()\n",
    "baseline.fit(X_train_d_scaled, y_train_d)\n",
    "\n",
    "# Evaluate all models\n",
    "models = {\n",
    "    'Baseline': baseline,\n",
    "    'Ridge': ridge_d,\n",
    "    'Lasso': lasso_d,\n",
    "    'Elastic Net': elastic_d\n",
    "}\n",
    "\n",
    "print(\"Model Comparison on Diabetes Dataset:\\n\")\n",
    "print(f\"{'Model':<15} {'Train R²':<12} {'Test R²':<12} {'Non-zero Coefs'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, 'score'):\n",
    "        train_r2 = model.score(X_train_d_scaled, y_train_d)\n",
    "        test_r2 = model.score(X_test_d_scaled, y_test_d)\n",
    "    else:\n",
    "        train_r2 = r2_score(y_train_d, model.predict(X_train_d_scaled))\n",
    "        test_r2 = r2_score(y_test_d, model.predict(X_test_d_scaled))\n",
    "    \n",
    "    if hasattr(model, 'weights'):\n",
    "        n_nonzero = np.sum(np.abs(model.weights) > 1e-5)\n",
    "    else:\n",
    "        n_nonzero = len(model.coef_) + 1\n",
    "    \n",
    "    print(f\"{name:<15} {train_r2:<12.4f} {test_r2:<12.4f} {n_nonzero}\")\n",
    "\n",
    "# Visualize coefficients\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    if hasattr(model, 'weights'):\n",
    "        coefs = model.weights[1:]  # Exclude bias\n",
    "    else:\n",
    "        coefs = model.coef_\n",
    "    \n",
    "    axes[i].bar(range(len(coefs)), coefs, alpha=0.7)\n",
    "    axes[i].set_xlabel('Feature Index')\n",
    "    axes[i].set_ylabel('Coefficient Value')\n",
    "    axes[i].set_title(f'{name} Coefficients')\n",
    "    axes[i].set_xticks(range(len(coefs)))\n",
    "    axes[i].set_xticklabels(diabetes.feature_names, rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].axhline(y=0, color='k', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Successfully compared all regularization methods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Validate Against Sklearn\n",
    "\n",
    "### Exercise 8.1: Compare with Sklearn Implementations\n",
    "\n",
    "**Task:** Verify your implementations match sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementations\n",
    "ridge_yours = RidgeRegression(alpha=1.0)\n",
    "ridge_yours.fit(X_train_d_scaled, y_train_d)\n",
    "r2_ridge_yours = ridge_yours.score(X_test_d_scaled, y_test_d)\n",
    "\n",
    "# Sklearn implementations\n",
    "ridge_sklearn = Ridge(alpha=1.0)\n",
    "ridge_sklearn.fit(X_train_d_scaled, y_train_d)\n",
    "r2_ridge_sklearn = ridge_sklearn.score(X_test_d_scaled, y_test_d)\n",
    "\n",
    "lasso_sklearn = Lasso(alpha=1.0, max_iter=5000)\n",
    "lasso_sklearn.fit(X_train_d_scaled, y_train_d)\n",
    "r2_lasso_sklearn = lasso_sklearn.score(X_test_d_scaled, y_test_d)\n",
    "\n",
    "elastic_sklearn = ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=5000)\n",
    "elastic_sklearn.fit(X_train_d_scaled, y_train_d)\n",
    "r2_elastic_sklearn = elastic_sklearn.score(X_test_d_scaled, y_test_d)\n",
    "\n",
    "print(\"Comparison with Sklearn:\\n\")\n",
    "print(f\"{'Model':<15} {'Your R²':<12} {'Sklearn R²':<12} {'Difference'}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Ridge':<15} {r2_ridge_yours:<12.4f} {r2_ridge_sklearn:<12.4f} {abs(r2_ridge_yours - r2_ridge_sklearn):.6f}\")\n",
    "\n",
    "# Test Lasso and Elastic Net if implemented\n",
    "if 'lasso_d' in locals():\n",
    "    r2_lasso_yours = lasso_d.score(X_test_d_scaled, y_test_d)\n",
    "    print(f\"{'Lasso':<15} {r2_lasso_yours:<12.4f} {r2_lasso_sklearn:<12.4f} {abs(r2_lasso_yours - r2_lasso_sklearn):.6f}\")\n",
    "\n",
    "if 'elastic_d' in locals():\n",
    "    r2_elastic_yours = elastic_d.score(X_test_d_scaled, y_test_d)\n",
    "    print(f\"{'Elastic Net':<15} {r2_elastic_yours:<12.4f} {r2_elastic_sklearn:<12.4f} {abs(r2_elastic_yours - r2_elastic_sklearn):.6f}\")\n",
    "\n",
    "print(\"\\n✓ Implementations validated against sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Bayesian Ridge Regression\n",
    "\n",
    "Implement Ridge with automatic hyperparameter tuning using Bayesian methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianRidge:\n",
    "    \"\"\"\n",
    "    Bayesian Ridge Regression with automatic alpha selection.\n",
    "    \n",
    "    Uses evidence approximation to estimate optimal alpha.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iterations=300):\n",
    "        self.n_iterations = n_iterations\n",
    "        self.alpha_ = None\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Your code here\n",
    "        # Implement evidence approximation\n",
    "        pass\n",
    "\n",
    "print(\"Challenge 1: Implement Bayesian Ridge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Group Lasso\n",
    "\n",
    "Implement Group Lasso that regularizes groups of features together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupLasso:\n",
    "    \"\"\"\n",
    "    Group Lasso Regression.\n",
    "    \n",
    "    Applies L2 regularization within groups, L1 across groups.\n",
    "    Useful for structured sparsity.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, groups=None):\n",
    "        self.alpha = alpha\n",
    "        self.groups = groups  # List of feature indices for each group\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge 2: Implement Group Lasso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Adaptive Lasso\n",
    "\n",
    "Implement adaptive Lasso with feature-specific penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveLasso:\n",
    "    \"\"\"\n",
    "    Adaptive Lasso Regression.\n",
    "    \n",
    "    Uses adaptive weights based on initial OLS estimates:\n",
    "    penalty_j = alpha / |w_j_ols|^gamma\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weights = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Your code here\n",
    "        # Step 1: Get initial OLS estimates\n",
    "        # Step 2: Compute adaptive weights\n",
    "        # Step 3: Solve weighted Lasso\n",
    "        pass\n",
    "\n",
    "print(\"Challenge 3: Implement Adaptive Lasso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Why does regularization help prevent overfitting?**\n",
    "   - Think about the complexity of models\n",
    "\n",
    "2. **When should you use Ridge vs Lasso?**\n",
    "   - Consider feature correlation and interpretability\n",
    "\n",
    "3. **Why can Lasso set coefficients to exactly zero but Ridge cannot?**\n",
    "   - Consider the geometry of L1 vs L2 penalties\n",
    "\n",
    "4. **How do you choose the regularization strength (alpha)?**\n",
    "   - What's the role of cross-validation?\n",
    "\n",
    "5. **When would you use Elastic Net over Ridge or Lasso?**\n",
    "   - Think about correlated features\n",
    "\n",
    "6. **What's the bias-variance tradeoff in regularization?**\n",
    "   - How does increasing alpha affect bias and variance?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "✓ How overfitting occurs and why regularization helps  \n",
    "✓ Ridge regression (L2): Shrinks all coefficients  \n",
    "✓ Lasso regression (L1): Sets some coefficients to zero (feature selection)  \n",
    "✓ Elastic Net: Combines Ridge and Lasso  \n",
    "✓ Regularization paths show coefficient behavior  \n",
    "✓ Cross-validation for hyperparameter tuning  \n",
    "✓ Application to real datasets  \n",
    "✓ Validation against sklearn implementations  \n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **Regularization**: Essential tool for preventing overfitting\n",
    "- **Ridge (L2)**: Best when all features are relevant\n",
    "- **Lasso (L1)**: Best for feature selection and sparse models\n",
    "- **Elastic Net**: Best when features are correlated\n",
    "- **Cross-validation**: Critical for choosing regularization strength\n",
    "- **Feature scaling**: Required before applying regularization\n",
    "\n",
    "**Comparison Summary:**\n",
    "\n",
    "| Method | Penalty | Feature Selection | Best When |\n",
    "|--------|---------|-------------------|----------|\n",
    "| Ridge | L2 (sum of squares) | No | All features relevant |\n",
    "| Lasso | L1 (sum of absolute values) | Yes | Sparse solutions needed |\n",
    "| Elastic Net | L1 + L2 | Yes | Features correlated |\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Review the [Regularization lesson](https://jumpingsphinx.github.io/module2-regression/04-regularization/)\n",
    "- Explore advanced regularization techniques\n",
    "- Apply to your own datasets\n",
    "- Study automatic hyperparameter tuning methods\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

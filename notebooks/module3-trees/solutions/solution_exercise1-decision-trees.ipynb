{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 3 - Exercise 1: Decision Trees\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module3-trees/exercise1-decision-trees.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Build decision trees from scratch\n- Understand and calculate Gini impurity and entropy\n- Implement splitting criteria for optimal tree construction\n- Visualize decision trees and their decision boundaries\n- Analyze overfitting and apply pruning techniques\n- Apply decision trees to both classification and regression problems\n\n## Prerequisites\n\n- Completion of Module 1 and 2\n- Understanding of classification and regression\n- Familiarity with information theory concepts\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, fetch_california_housing, make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Understanding Splitting Criteria\n",
    "\n",
    "### Background\n",
    "\n",
    "Decision trees select splits that maximize information gain by minimizing impurity. Two common impurity measures are:\n",
    "\n",
    "**Gini Impurity:**\n",
    "\n",
    "$$\\text{Gini} = 1 - \\sum_{i=1}^{C} p_i^2$$\n",
    "\n",
    "**Entropy (Information):**\n",
    "\n",
    "$$\\text{Entropy} = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$$\n",
    "\n",
    "Where $p_i$ is the proportion of samples belonging to class $i$ and $C$ is the number of classes.\n",
    "\n",
    "### Exercise 1.1: Calculate Gini Impurity\n",
    "\n",
    "**Task:** Implement a function to calculate Gini impurity for a set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(labels):\n",
    "    \"\"\"\n",
    "    Calculate Gini impurity for a set of labels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : np.ndarray\n",
    "        Array of class labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Gini impurity value between 0 (pure) and 0.5 (binary case, most impure)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    unique_classes, counts = np.unique(labels, return_counts=True)\n",
    "    proportions = counts / len(labels)\n",
    "    gini = 1 - np.sum(proportions ** 2)\n",
    "# Test cases\n",
    "# Pure node: all same class\n",
    "pure_labels = np.array([0, 0, 0, 0, 0])\n",
    "print(f\"Pure node Gini: {gini_impurity(pure_labels):.4f}\")  # Expected: 0.0\n",
    "\n",
    "# Maximally impure node: equal split (binary case)\n",
    "impure_labels = np.array([0, 0, 1, 1])\n",
    "print(f\"50/50 split Gini: {gini_impurity(impure_labels):.4f}\")  # Expected: 0.5\n",
    "\n",
    "# Mixed node\n",
    "mixed_labels = np.array([0, 0, 0, 1, 1])\n",
    "print(f\"Mixed node Gini: {gini_impurity(mixed_labels):.4f}\")  # Expected: 0.48\n",
    "\n",
    "# Three classes\n",
    "three_class = np.array([0, 0, 1, 1, 2, 2])\n",
    "print(f\"Three classes Gini: {gini_impurity(three_class):.4f}\")  # Expected: 0.6667\n",
    "\n",
    "assert gini_impurity(pure_labels) == 0.0, \"Pure node should have Gini = 0\"\n",
    "assert np.isclose(gini_impurity(impure_labels), 0.5), \"50/50 split should have Gini = 0.5\"\n",
    "print(\"\\n\u2713 Gini impurity implemented correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Calculate Entropy\n",
    "\n",
    "**Task:** Implement a function to calculate entropy for a set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(labels):\n",
    "    \"\"\"\n",
    "    Calculate entropy for a set of labels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : np.ndarray\n",
    "        Array of class labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Entropy value (0 = pure, higher = more impure)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    unique_classes, counts = np.unique(labels, return_counts=True)\n",
    "    proportions = counts / len(labels)\n",
    "    \n",
    "    # Avoid log(0)\n",
    "    proportions = proportions[proportions > 0]\n",
    "    entropy_val = -np.sum(proportions * np.log2(proportions))\n",
    "# Test cases\n",
    "print(f\"Pure node Entropy: {entropy(pure_labels):.4f}\")  # Expected: 0.0\n",
    "print(f\"50/50 split Entropy: {entropy(impure_labels):.4f}\")  # Expected: 1.0\n",
    "print(f\"Mixed node Entropy: {entropy(mixed_labels):.4f}\")  # Expected: ~0.971\n",
    "print(f\"Three classes Entropy: {entropy(three_class):.4f}\")  # Expected: ~1.585\n",
    "\n",
    "assert entropy(pure_labels) == 0.0, \"Pure node should have entropy = 0\"\n",
    "assert np.isclose(entropy(impure_labels), 1.0), \"50/50 split should have entropy = 1.0\"\n",
    "print(\"\\n\u2713 Entropy implemented correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Calculate Information Gain\n",
    "\n",
    "**Task:** Implement information gain calculation.\n",
    "\n",
    "Information Gain measures how much a split reduces impurity:\n",
    "\n",
    "$$\\text{InfoGain} = \\text{Impurity}_{\\text{parent}} - \\sum_{\\text{child}} \\frac{N_{\\text{child}}}{N_{\\text{parent}}} \\times \\text{Impurity}_{\\text{child}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(parent_labels, left_labels, right_labels, criterion='gini'):\n",
    "    \"\"\"\n",
    "    Calculate information gain from a split.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    parent_labels : labels before split\n",
    "    left_labels : labels in left child\n",
    "    right_labels : labels in right child\n",
    "    criterion : 'gini' or 'entropy'\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Information gain value\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    parent_impurity = impurity_func(parent_labels)\n",
    "    \n",
    "    # Calculate weighted average impurity of children\n",
    "    n_parent = len(parent_labels)\n",
    "    n_left = len(left_labels)\n",
    "    n_right = len(right_labels)\n",
    "    \n",
    "    weighted_child_impurity = (n_left / n_parent) * impurity_func(left_labels) +                               (n_right / n_parent) * impurity_func(right_labels)\n",
    "    \n",
    "    gain = parent_impurity - weighted_child_impurity\n",
    "# Test: Split [0,0,0,1,1,1] -> [0,0,0] and [1,1,1]\n",
    "parent = np.array([0, 0, 0, 1, 1, 1])\n",
    "left = np.array([0, 0, 0])\n",
    "right = np.array([1, 1, 1])\n",
    "\n",
    "gain_gini = information_gain(parent, left, right, 'gini')\n",
    "gain_entropy = information_gain(parent, left, right, 'entropy')\n",
    "\n",
    "print(f\"Information Gain (Gini): {gain_gini:.4f}\")  # Expected: 0.5\n",
    "print(f\"Information Gain (Entropy): {gain_entropy:.4f}\")  # Expected: 1.0\n",
    "\n",
    "# Perfect split should have high gain\n",
    "assert gain_gini == 0.5, \"Perfect split should have Gini gain = 0.5\"\n",
    "assert np.isclose(gain_entropy, 1.0), \"Perfect split should have Entropy gain = 1.0\"\n",
    "print(\"\\n\u2713 Information gain implemented correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implementing Decision Tree from Scratch\n",
    "\n",
    "### Background\n",
    "\n",
    "A decision tree recursively:\n",
    "1. Finds the best split (feature and threshold)\n",
    "2. Divides data into left and right children\n",
    "3. Repeats until stopping criteria (max depth, min samples, pure node)\n",
    "\n",
    "### Exercise 2.1: Implement Tree Node\n",
    "\n",
    "**Task:** Create a simple decision tree node structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\"\n",
    "    A node in the decision tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        feature : int\n",
    "            Feature index for splitting (None for leaf)\n",
    "        threshold : float\n",
    "            Threshold value for splitting (None for leaf)\n",
    "        left : TreeNode\n",
    "            Left child node\n",
    "        right : TreeNode\n",
    "            Right child node\n",
    "        value : int/float\n",
    "            Predicted class (for leaf nodes)\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if node is a leaf.\"\"\"\n",
    "        return self.value is not None\n",
    "\n",
    "print(\"TreeNode class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Implement Simple Decision Tree\n",
    "\n",
    "**Task:** Implement a basic decision tree classifier with limited depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecisionTree:\n",
    "    \"\"\"\n",
    "    A simple decision tree classifier implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the decision tree.\"\"\"\n",
    "        self.root = self._grow_tree(X, y, depth=0)\n",
    "        return self\n",
    "    \n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        Recursively grow the tree.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        TreeNode\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            n_classes == 1):\n",
    "            # Create leaf node with majority class\n",
    "            leaf_value = np.argmax(np.bincount(y))\n",
    "            return TreeNode(value=leaf_value)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feature, best_threshold = self._best_split(X, y)\n",
    "        \n",
    "        if best_feature is None:\n",
    "            # No good split found\n",
    "            leaf_value = np.argmax(np.bincount(y))\n",
    "            return TreeNode(value=leaf_value)\n",
    "        \n",
    "        # Split data\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = ~left_indices\n",
    "        \n",
    "        # Recursively build left and right subtrees\n",
    "        left_child = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_child = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        \n",
    "        return TreeNode(feature=best_feature, threshold=best_threshold,\n",
    "                       left=left_child, right=right_child)\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split on.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        best_feature : int or None\n",
    "        best_threshold : float or None\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        n_samples, n_features = X.shape\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            # Get unique values for this feature\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                # Split data\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                # Skip if split is empty\n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain\n",
    "                gain = information_gain(y, y[left_mask], y[right_mask], self.criterion)\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        # Your code here\n",
    "print(\"SimpleDecisionTree class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Test Your Implementation\n",
    "\n",
    "**Task:** Test your decision tree on a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple dataset\n",
    "X_simple = np.array([\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [4, 5],\n",
    "    [1, 2],\n",
    "    [2, 2],\n",
    "    [7, 8],\n",
    "    [8, 9],\n",
    "    [9, 10],\n",
    "    [6, 7],\n",
    "    [7, 7]\n",
    "])\n",
    "y_simple = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "# Train your tree\n",
    "tree = SimpleDecisionTree(max_depth=3)\n",
    "tree.fit(X_simple, y_simple)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tree.predict(X_simple)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_simple)\n",
    "print(f\"Training Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Visualize decision boundary\n",
    "def plot_decision_boundary(X, y, model, title=\"Decision Boundary\"):\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='black', cmap='viridis', s=100)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label='Class')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_simple, y_simple, tree, \"Your Decision Tree\")\n",
    "\n",
    "assert accuracy >= 0.8, \"Should achieve at least 80% accuracy\"\n",
    "print(\"\\n\u2713 Your decision tree works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Scikit-learn Decision Trees\n",
    "\n",
    "### Background\n",
    "\n",
    "Scikit-learn provides optimized implementations with many features:\n",
    "- Multiple splitting criteria (gini, entropy, log_loss)\n",
    "- Pruning strategies\n",
    "- Support for categorical features\n",
    "- Feature importance calculation\n",
    "\n",
    "### Exercise 3.1: Train on Iris Dataset\n",
    "\n",
    "**Task:** Use DecisionTreeClassifier on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "print(\"Iris Dataset:\")\n",
    "print(f\"Samples: {X_iris.shape[0]}\")\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Your code here: Create and train a DecisionTreeClassifier\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)\n",
    "print(\"Decision Tree Classifier on Iris:\")\n",
    "print(f\"Train Accuracy: {train_acc:.2%}\")\n",
    "print(f\"Test Accuracy:  {test_acc:.2%}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=iris.target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "assert test_acc >= 0.85, \"Should achieve at least 85% accuracy on Iris\"\n",
    "print(\"\\n\u2713 Successfully trained on Iris dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Feature Importance\n",
    "\n",
    "**Task:** Analyze which features are most important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': iris.feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importance_df)\n",
    "print()\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importances)), importances)\n",
    "plt.yticks(range(len(importances)), iris.feature_names)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance in Decision Tree')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Most important feature: {iris.feature_names[np.argmax(importances)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Visualization\n",
    "\n",
    "### Exercise 4.1: Visualize Tree Structure\n",
    "\n",
    "**Task:** Create a visual representation of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a smaller tree for better visualization\n",
    "clf_small = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_small.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(clf_small, \n",
    "          feature_names=iris.feature_names,\n",
    "          class_names=iris.target_names,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Decision Tree Structure (max_depth=3)', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(\"Tree Characteristics:\")\n",
    "print(f\"Max depth: {clf_small.get_depth()}\")\n",
    "print(f\"Number of leaves: {clf_small.get_n_leaves()}\")\n",
    "print(f\"Number of nodes: {clf_small.tree_.node_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Decision Boundaries (2D)\n",
    "\n",
    "**Task:** Visualize decision boundaries using 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only 2 features for 2D visualization\n",
    "# Let's use petal length and petal width (features 2 and 3)\n",
    "X_2d = X_iris[:, [2, 3]]\n",
    "X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y_iris, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train tree on 2D data\n",
    "clf_2d = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf_2d.fit(X_train_2d, y_train_2d)\n",
    "\n",
    "# Plot decision boundary\n",
    "def plot_decision_boundary_multiclass(X, y, model, feature_names, class_names):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='viridis')\n",
    "    \n",
    "    # Plot training points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, \n",
    "                         edgecolors='black', s=100, cmap='viridis')\n",
    "    \n",
    "    plt.xlabel(feature_names[0], fontsize=12)\n",
    "    plt.ylabel(feature_names[1], fontsize=12)\n",
    "    plt.title('Decision Tree Decision Boundaries', fontsize=14)\n",
    "    plt.colorbar(scatter, ticks=range(len(class_names)), label='Class')\n",
    "    plt.legend(handles=scatter.legend_elements()[0], \n",
    "              labels=class_names, loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary_multiclass(\n",
    "    X_train_2d, y_train_2d, clf_2d,\n",
    "    ['Petal Length', 'Petal Width'],\n",
    "    iris.target_names\n",
    ")\n",
    "\n",
    "test_acc_2d = clf_2d.score(X_test_2d, y_test_2d)\n",
    "print(f\"\\nTest Accuracy (2D): {test_acc_2d:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Overfitting Analysis\n",
    "\n",
    "### Background\n",
    "\n",
    "Decision trees can easily overfit by growing too deep. The `max_depth` hyperparameter controls complexity.\n",
    "\n",
    "### Exercise 5.1: Vary max_depth\n",
    "\n",
    "**Task:** Train trees with different depths and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max_depth values\n",
    "depths = range(1, 21)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    # Your code here: Train tree with this depth\n",
    "\n",
    "# Your code here: Create and train a DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Optimal max_depth: {optimal_depth}\")\n",
    "print(f\"Best test accuracy: {best_test_score:.2%}\")\n",
    "print(f\"\\nOverfitting analysis:\")\n",
    "print(f\"At max_depth=1: Train={train_scores[0]:.2%}, Test={test_scores[0]:.2%} (Underfitting)\")\n",
    "print(f\"At max_depth={optimal_depth}: Train={train_scores[optimal_depth-1]:.2%}, Test={test_scores[optimal_depth-1]:.2%} (Good)\")\n",
    "print(f\"At max_depth=20: Train={train_scores[-1]:.2%}, Test={test_scores[-1]:.2%} (Potential Overfitting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Cross-Validation\n",
    "\n",
    "**Task:** Use cross-validation for more robust evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different depths with cross-validation\n",
    "depths_cv = [1, 2, 3, 4, 5, 10, 15, 20]\n",
    "cv_scores_mean = []\n",
    "cv_scores_std = []\n",
    "\n",
    "for depth in depths_cv:\n",
    "    clf_cv = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    \n",
    "    # Perform 5-fold cross-validation\n",
    "    scores = cross_val_score(clf_cv, X_iris, y_iris, cv=5, scoring='accuracy')\n",
    "    \n",
    "    cv_scores_mean.append(scores.mean())\n",
    "    cv_scores_std.append(scores.std())\n",
    "\n",
    "# Plot with error bars\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(depths_cv, cv_scores_mean, yerr=cv_scores_std, \n",
    "             marker='o', capsize=5, linewidth=2, markersize=8)\n",
    "plt.xlabel('Max Depth', fontsize=12)\n",
    "plt.ylabel('Cross-Validation Accuracy', fontsize=12)\n",
    "plt.title('5-Fold Cross-Validation Scores vs Max Depth', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "best_depth_cv = depths_cv[np.argmax(cv_scores_mean)]\n",
    "best_cv_score = np.max(cv_scores_mean)\n",
    "print(f\"\\nBest depth (CV): {best_depth_cv}\")\n",
    "print(f\"Best CV accuracy: {best_cv_score:.2%} (+/- {cv_scores_std[np.argmax(cv_scores_mean)]:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Regression Trees\n",
    "\n",
    "### Background\n",
    "\n",
    "Decision trees can also solve regression problems by predicting continuous values. Leaf nodes contain the mean of training samples in that region.\n",
    "\n",
    "### Exercise 6.1: California Housing Dataset\n",
    "\n",
    "**Task:** Apply DecisionTreeRegressor to predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "\n",
    "print(\"California Housing Dataset:\")\n",
    "print(f\"Samples: {X_housing.shape[0]}\")\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "print(f\"Target: Median house value (in $100,000s)\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Your code here: Create and train a DecisionTreeRegressor\n",
    "    # Your code here: Train tree with this depth\n",
    "    clf_depth = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    clf_depth.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate scores\n",
    "    train_score = clf_depth.score(X_train, y_train)\n",
    "    test_score = clf_depth.score(X_test, y_test)\n",
    "print(\"Decision Tree Regressor Performance:\")\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE:  {test_mse:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(test_mse):.4f}\")\n",
    "print(f\"Test MAE:  {test_mae:.4f}\")\n",
    "print(f\"Train R\u00b2:  {train_r2:.4f}\")\n",
    "print(f\"Test R\u00b2:   {test_r2:.4f}\")\n",
    "\n",
    "assert test_r2 >= 0.5, \"Should achieve at least 0.5 R\u00b2 on housing data\"\n",
    "print(\"\\n\u2713 Regression tree trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Regression Predictions Visualization\n",
    "\n",
    "**Task:** Visualize actual vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Predicted vs Actual\n",
    "axes[0].scatter(y_test_h, y_test_pred_h, alpha=0.5)\n",
    "axes[0].plot([y_test_h.min(), y_test_h.max()], \n",
    "             [y_test_h.min(), y_test_h.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Price ($100k)', fontsize=12)\n",
    "axes[0].set_ylabel('Predicted Price ($100k)', fontsize=12)\n",
    "axes[0].set_title('Predicted vs Actual House Prices', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals_h = y_test_h - y_test_pred_h\n",
    "axes[1].scatter(y_test_pred_h, residuals_h, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Price ($100k)', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean of residuals: {residuals_h.mean():.4f}\")\n",
    "print(f\"Std of residuals: {residuals_h.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.3: Regression Tree Depth Analysis\n",
    "\n",
    "**Task:** Analyze overfitting in regression trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max_depth values for regression\n",
    "depths_reg = range(1, 21)\n",
    "train_r2_scores = []\n",
    "test_r2_scores = []\n",
    "train_rmse_scores = []\n",
    "test_rmse_scores = []\n",
    "\n",
    "for depth in depths_reg:\n",
    "    reg = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    reg.fit(X_train_h, y_train_h)\n",
    "    \n",
    "    y_train_pred = reg.predict(X_train_h)\n",
    "    y_test_pred = reg.predict(X_test_h)\n",
    "    \n",
    "    train_r2_scores.append(r2_score(y_train_h, y_train_pred))\n",
    "    test_r2_scores.append(r2_score(y_test_h, y_test_pred))\n",
    "    train_rmse_scores.append(np.sqrt(mean_squared_error(y_train_h, y_train_pred)))\n",
    "    test_rmse_scores.append(np.sqrt(mean_squared_error(y_test_h, y_test_pred)))\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# R\u00b2 plot\n",
    "axes[0].plot(depths_reg, train_r2_scores, 'o-', label='Training R\u00b2', linewidth=2)\n",
    "axes[0].plot(depths_reg, test_r2_scores, 's-', label='Test R\u00b2', linewidth=2)\n",
    "axes[0].set_xlabel('Max Depth', fontsize=12)\n",
    "axes[0].set_ylabel('R\u00b2 Score', fontsize=12)\n",
    "axes[0].set_title('R\u00b2 vs Max Depth', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE plot\n",
    "axes[1].plot(depths_reg, train_rmse_scores, 'o-', label='Training RMSE', linewidth=2)\n",
    "axes[1].plot(depths_reg, test_rmse_scores, 's-', label='Test RMSE', linewidth=2)\n",
    "axes[1].set_xlabel('Max Depth', fontsize=12)\n",
    "axes[1].set_ylabel('RMSE', fontsize=12)\n",
    "axes[1].set_title('RMSE vs Max Depth', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "optimal_depth_reg = depths_reg[np.argmax(test_r2_scores)]\n",
    "print(f\"\\nOptimal max_depth for regression: {optimal_depth_reg}\")\n",
    "print(f\"Best test R\u00b2: {np.max(test_r2_scores):.4f}\")\n",
    "print(f\"Best test RMSE: {test_rmse_scores[np.argmax(test_r2_scores)]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Real-World Application - Titanic Dataset\n",
    "\n",
    "### Background\n",
    "\n",
    "Predict survival on the Titanic using decision trees. We'll create a Titanic-like dataset using sklearn's make_classification.\n",
    "\n",
    "### Exercise 7.1: Create and Explore Titanic-like Data\n",
    "\n",
    "**Task:** Generate a binary classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Titanic-like dataset\n",
    "# Features: Age, Fare, Class, Sex, Siblings, Parents\n",
    "X_titanic, y_titanic = make_classification(\n",
    "    n_samples=891,  # Similar to Titanic dataset size\n",
    "    n_features=6,\n",
    "    n_informative=4,\n",
    "    n_redundant=1,\n",
    "    n_classes=2,\n",
    "    weights=[0.62, 0.38],  # ~38% survival rate\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "titanic_features = ['Age', 'Fare', 'Pclass', 'Sex', 'Siblings', 'Parents']\n",
    "df_titanic = pd.DataFrame(X_titanic, columns=titanic_features)\n",
    "df_titanic['Survived'] = y_titanic\n",
    "\n",
    "print(\"Titanic-like Dataset:\")\n",
    "print(f\"Total passengers: {len(df_titanic)}\")\n",
    "print(f\"Survival rate: {y_titanic.mean():.2%}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_titanic.head())\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df_titanic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Train and Evaluate Decision Tree\n",
    "\n",
    "**Task:** Build a decision tree to predict survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
    "    X_titanic, y_titanic, test_size=0.2, random_state=42, stratify=y_titanic\n",
    ")\n",
    "\n",
    "# Your code here: Train a decision tree with appropriate hyperparameters\n",
    "clf_titanic = DecisionTreeClassifier(\n",
    "    max_depth=,  # Choose a good value\n",
    "    min_samples_split=,\n",
    "    min_samples_leaf=,\n",
    "    random_state=42\n",
    ")\n",
    "clf_titanic.fit(X_train_t, y_train_t)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_t = clf_titanic.predict(X_train_t)\n",
    "y_test_pred_t = clf_titanic.predict(X_test_t)\n",
    "\n",
    "# Evaluation\n",
    "train_acc_t = accuracy_score(y_train_t, y_train_pred_t)\n",
    "test_acc_t = accuracy_score(y_test_t, y_test_pred_t)\n",
    "\n",
    "print(\"Titanic Survival Prediction:\")\n",
    "print(f\"Train Accuracy: {train_acc_t:.2%}\")\n",
    "print(f\"Test Accuracy:  {test_acc_t:.2%}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_t, y_test_pred_t, \n",
    "                          target_names=['Did not survive', 'Survived']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_t = confusion_matrix(y_test_t, y_test_pred_t)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                Predicted\")\n",
    "print(\"              Not Survive  Survive\")\n",
    "print(f\"Actual Not Survive    {cm_t[0,0]}         {cm_t[0,1]}\")\n",
    "print(f\"Actual Survive        {cm_t[1,0]}         {cm_t[1,1]}\")\n",
    "\n",
    "assert test_acc_t >= 0.7, \"Should achieve at least 70% accuracy\"\n",
    "print(\"\\n\u2713 Titanic prediction model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3: Feature Importance Analysis\n",
    "\n",
    "**Task:** Determine which factors most influenced survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances_t = clf_titanic.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "feature_importance_t = pd.DataFrame({\n",
    "    'Feature': titanic_features,\n",
    "    'Importance': importances_t\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance for Titanic Survival:\")\n",
    "print(feature_importance_t)\n",
    "print()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importances_t)), \n",
    "         feature_importance_t['Importance'].values,\n",
    "         color='steelblue')\n",
    "plt.yticks(range(len(importances_t)), \n",
    "           feature_importance_t['Feature'].values)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importance for Titanic Survival Prediction', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Most important factor: {feature_importance_t.iloc[0]['Feature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.4: Visualize Titanic Decision Tree\n",
    "\n",
    "**Task:** Create a visual representation of the survival decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(clf_titanic,\n",
    "          feature_names=titanic_features,\n",
    "          class_names=['Did not survive', 'Survived'],\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('Titanic Survival Decision Tree', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Tree depth: {clf_titanic.get_depth()}\")\n",
    "print(f\"Number of leaves: {clf_titanic.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Pruning with min_samples_leaf\n",
    "\n",
    "Implement post-pruning by experimenting with `min_samples_leaf` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Test min_samples_leaf values: 1, 5, 10, 20, 50\n",
    "# Plot how tree complexity and accuracy change\n",
    "\n",
    "min_samples_values = [1, 5, 10, 20, 50]\n",
    "\n",
    "# TODO: Implement pruning analysis\n",
    "\n",
    "print(\"Challenge 1: Analyze the effect of min_samples_leaf!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Cost-Complexity Pruning\n",
    "\n",
    "Use `ccp_alpha` parameter for cost-complexity pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: Use clf.cost_complexity_pruning_path() to get alpha values\n",
    "\n",
    "print(\"Challenge 2: Implement cost-complexity pruning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Comparison with Other Algorithms\n",
    "\n",
    "Compare decision tree performance with logistic regression and k-NN on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# TODO: Compare DecisionTree, LogisticRegression, and KNN\n",
    "\n",
    "print(\"Challenge 3: Compare multiple algorithms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Ensemble of Stumps\n",
    "\n",
    "Create an ensemble by training multiple decision stumps (depth=1) on different feature subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Train multiple trees with max_depth=1 on different features\n",
    "# Combine predictions by majority vote\n",
    "\n",
    "print(\"Challenge 4: Build an ensemble of decision stumps!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **When would you prefer Gini impurity over Entropy?**\n",
    "   - Consider computational efficiency and behavior differences\n",
    "   - Gini is faster to compute (no logarithm)\n",
    "   - Entropy may isolate one class in its own branch more often\n",
    "\n",
    "2. **What are the main advantages of decision trees?**\n",
    "   - Interpretability: Easy to understand and visualize\n",
    "   - No feature scaling needed\n",
    "   - Can handle both numerical and categorical data\n",
    "   - Non-parametric (no assumptions about data distribution)\n",
    "   - Captures non-linear relationships\n",
    "\n",
    "3. **What are the main disadvantages?**\n",
    "   - Prone to overfitting (especially deep trees)\n",
    "   - Unstable: small changes in data can result in very different trees\n",
    "   - Greedy algorithm: may not find globally optimal tree\n",
    "   - Biased towards features with more levels\n",
    "\n",
    "4. **How do you prevent overfitting in decision trees?**\n",
    "   - Limit max_depth\n",
    "   - Set min_samples_split and min_samples_leaf\n",
    "   - Use pruning techniques (cost-complexity)\n",
    "   - Ensemble methods (Random Forests, Gradient Boosting)\n",
    "\n",
    "5. **When should you use decision trees vs linear models?**\n",
    "   - Decision trees: Non-linear relationships, feature interactions, interpretability needed\n",
    "   - Linear models: Linear relationships, high-dimensional sparse data, need for probabilistic outputs\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "\u2713 How to calculate Gini impurity and Entropy manually\n",
    "\n",
    "\u2713 The concept of information gain for split selection\n",
    "\n",
    "\u2713 Implementation of a basic decision tree from scratch\n",
    "\n",
    "\u2713 Using scikit-learn's DecisionTreeClassifier and DecisionTreeRegressor\n",
    "\n",
    "\u2713 Visualization of tree structures and decision boundaries\n",
    "\n",
    "\u2713 Analysis of overfitting through hyperparameter tuning\n",
    "\n",
    "\u2713 Feature importance analysis\n",
    "\n",
    "\u2713 Application to both classification and regression tasks\n",
    "\n",
    "\u2713 Cross-validation for robust model evaluation\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete Exercise 2 on Random Forests\n",
    "- Review the [Decision Trees lesson](https://jumpingsphinx.github.io/module3-trees/01-decision-trees/)\n",
    "- Experiment with different datasets and hyperparameters\n",
    "- Study ensemble methods that combine multiple trees\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
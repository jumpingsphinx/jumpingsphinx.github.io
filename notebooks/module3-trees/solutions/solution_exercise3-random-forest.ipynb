{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 3 - Exercise 3: Random Forest\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module3-trees/exercise3-random-forest.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Understand bootstrap aggregating (bagging)\n- Implement Random Forest from scratch\n- Analyze feature importance using different methods\n- Use out-of-bag (OOB) error for model evaluation\n- Compare Random Forest with single decision trees\n- Tune hyperparameters for optimal performance\n\n## Prerequisites\n\n- Completion of previous Module 3 exercises\n- Understanding of ensemble learning\n- Familiarity with variance reduction techniques\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import (\n",
    "    load_breast_cancer, load_wine, fetch_california_housing,\n",
    "    make_classification\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, GridSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix,\n",
    "    mean_squared_error, r2_score, roc_auc_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Bootstrap Sampling\n",
    "\n",
    "### Background\n",
    "\n",
    "**Bootstrap sampling** is sampling with replacement from the original dataset. It creates diversity among ensemble members.\n",
    "\n",
    "Key properties:\n",
    "- Sample size = original dataset size\n",
    "- Some samples appear multiple times\n",
    "- About 63.2% of unique samples are included\n",
    "- Remaining ~36.8% are \"out-of-bag\" (OOB) samples\n",
    "\n",
    "### Exercise 1.1: Implement Bootstrap Sampling\n",
    "\n",
    "**Task:** Implement a function to create bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y, random_state=None):\n",
    "    \"\"\"\n",
    "    Create a bootstrap sample from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Features (n_samples, n_features)\n",
    "    y : np.ndarray\n",
    "        Labels (n_samples,)\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_bootstrap : np.ndarray\n",
    "        Bootstrap sample features\n",
    "    y_bootstrap : np.ndarray\n",
    "        Bootstrap sample labels\n",
    "    oob_indices : np.ndarray\n",
    "        Indices of out-of-bag samples\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Your code here\n",
    "    # 1. Sample indices with replacement\n",
    "    indices = np.arange(n_samples)\n",
    "    bootstrap_indices = np.random.choice(indices, size=n_samples, replace=True)\n",
    "    \n",
    "    # 2. Get bootstrap samples\n",
    "    X_bootstrap = X[bootstrap_indices]\n",
    "    y_bootstrap = y[bootstrap_indices]\n",
    "    \n",
    "    # 3. Find OOB indices (samples not in bootstrap)\n",
    "    oob_indices = np.setdiff1d(indices, bootstrap_indices)\n",
    "# Test bootstrap sampling\n",
    "X_test = np.arange(100).reshape(-1, 1)\n",
    "y_test = np.arange(100)\n",
    "\n",
    "X_boot, y_boot, oob_idx = bootstrap_sample(X_test, y_test, random_state=42)\n",
    "\n",
    "print(f\"Original samples: {len(X_test)}\")\n",
    "print(f\"Bootstrap samples: {len(X_boot)}\")\n",
    "print(f\"Unique bootstrap samples: {len(np.unique(y_boot))}\")\n",
    "print(f\"OOB samples: {len(oob_idx)}\")\n",
    "print(f\"OOB percentage: {len(oob_idx) / len(X_test) * 100:.1f}%\")\n",
    "\n",
    "# Verify some samples appear multiple times\n",
    "counts = Counter(y_boot)\n",
    "duplicates = sum(1 for count in counts.values() if count > 1)\n",
    "print(f\"Samples appearing multiple times: {duplicates}\")\n",
    "\n",
    "assert len(X_boot) == len(X_test), \"Bootstrap sample should have same size\"\n",
    "assert len(oob_idx) > 0, \"Should have OOB samples\"\n",
    "assert 30 <= len(oob_idx) <= 40, \"OOB should be ~36.8%\"\n",
    "print(\"\\n\u2713 Bootstrap sampling implemented correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Visualize Bootstrap Distribution\n",
    "\n",
    "**Task:** Analyze the distribution of bootstrap samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple bootstrap iterations\n",
    "n_iterations = 1000\n",
    "n_samples = 100\n",
    "oob_percentages = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    X_temp = np.arange(n_samples).reshape(-1, 1)\n",
    "    y_temp = np.arange(n_samples)\n",
    "    _, _, oob_idx = bootstrap_sample(X_temp, y_temp, random_state=i)\n",
    "    oob_percentages.append(len(oob_idx) / n_samples * 100)\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(oob_percentages, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=np.mean(oob_percentages), color='red', linestyle='--', \n",
    "            linewidth=2, label=f'Mean: {np.mean(oob_percentages):.2f}%')\n",
    "plt.axvline(x=36.8, color='green', linestyle='--', \n",
    "            linewidth=2, label='Theoretical: 36.8%')\n",
    "plt.xlabel('OOB Percentage (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of OOB Percentages\\n(1000 Bootstrap Iterations)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Count frequency of each sample in one bootstrap\n",
    "sample_counts = Counter(y_boot)\n",
    "frequencies = [sample_counts.get(i, 0) for i in range(100)]\n",
    "plt.bar(range(100), frequencies, alpha=0.7)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Frequency in Bootstrap Sample')\n",
    "plt.title('Sample Frequency in a Single Bootstrap')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean OOB percentage: {np.mean(oob_percentages):.2f}%\")\n",
    "print(f\"Theoretical OOB percentage: {(1 - (1 - 1/n_samples)**n_samples) * 100:.2f}%\")\n",
    "print(f\"Std of OOB percentage: {np.std(oob_percentages):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Bagging Implementation\n",
    "\n",
    "### Background\n",
    "\n",
    "**Bagging (Bootstrap Aggregating):**\n",
    "1. Create B bootstrap samples\n",
    "2. Train a model on each sample\n",
    "3. Aggregate predictions:\n",
    "   - Classification: Majority voting\n",
    "   - Regression: Average\n",
    "\n",
    "Benefits:\n",
    "- Reduces variance\n",
    "- Prevents overfitting\n",
    "- Works well with high-variance models (like deep trees)\n",
    "\n",
    "### Exercise 2.1: Implement Bagging Classifier\n",
    "\n",
    "**Task:** Build a bagging classifier from scratch using decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaggingClassifier:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, random_state=None):\n",
    "        \"\"\"\n",
    "        Bagging classifier using decision trees.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators : int\n",
    "            Number of trees in the ensemble\n",
    "        max_depth : int\n",
    "            Maximum depth of each tree\n",
    "        random_state : int\n",
    "            Random seed\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit bagging classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Training features\n",
    "        y : np.ndarray\n",
    "            Training labels\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Your code here\n",
    "            # 1. Create bootstrap sample\n",
    "            X_boot, y_boot, _ = bootstrap_sample(X, y, random_state=seed)\n",
    "            \n",
    "            # 2. Train a decision tree on bootstrap sample\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, random_state=seed)\n",
    "            tree.fit(X_boot, y_boot)\n",
    "        # Your code here\n",
    "print(\"BaggingClassifier class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Test Bagging on Breast Cancer Dataset\n",
    "\n",
    "**Task:** Compare single decision tree vs bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = cancer.data\n",
    "y_cancer = cancer.target\n",
    "\n",
    "print(\"Breast Cancer Dataset:\")\n",
    "print(f\"Shape: {X_cancer.shape}\")\n",
    "print(f\"Classes: {cancer.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y_cancer)}\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "# Train single decision tree\n",
    "single_tree = DecisionTreeClassifier(random_state=42)\n",
    "single_tree.fit(X_train, y_train)\n",
    "single_acc = single_tree.score(X_test, y_test)\n",
    "\n",
    "# Train bagging classifier\n",
    "bagging = BaggingClassifier(n_estimators=50, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "bagging_acc = bagging.score(X_test, y_test)\n",
    "\n",
    "print(f\"Single Decision Tree Accuracy: {single_acc:.4f}\")\n",
    "print(f\"Bagging (50 trees) Accuracy:   {bagging_acc:.4f}\")\n",
    "print(f\"Improvement: {(bagging_acc - single_acc) * 100:.2f}%\")\n",
    "\n",
    "# Analyze effect of number of trees\n",
    "n_trees_list = [1, 5, 10, 20, 50, 100]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_trees in n_trees_list:\n",
    "    model = BaggingClassifier(n_estimators=n_trees, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    train_scores.append(model.score(X_train, y_train))\n",
    "    test_scores.append(model.score(X_test, y_test))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_trees_list, train_scores, 'o-', label='Train Accuracy', linewidth=2)\n",
    "plt.plot(n_trees_list, test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
    "plt.axhline(y=single_acc, color='red', linestyle='--', \n",
    "            label=f'Single Tree ({single_acc:.4f})', linewidth=2)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Bagging Performance vs Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "assert bagging_acc >= single_acc, \"Bagging should improve or match single tree\"\n",
    "print(\"\\n\u2713 Bagging successfully implemented and tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Feature Randomness\n",
    "\n",
    "### Background\n",
    "\n",
    "**Random Forest = Bagging + Random Feature Selection**\n",
    "\n",
    "At each split in each tree:\n",
    "- Randomly select $m$ features from $p$ total features\n",
    "- Find best split using only these $m$ features\n",
    "- Common choice: $m = \\sqrt{p}$ for classification, $m = p/3$ for regression\n",
    "\n",
    "Benefits:\n",
    "- De-correlates trees\n",
    "- Prevents dominant features from always being selected\n",
    "- Further reduces variance\n",
    "\n",
    "### Exercise 3.1: Implement Feature Randomness\n",
    "\n",
    "**Task:** Add random feature selection to the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, \n",
    "                 max_features='sqrt', random_state=None):\n",
    "        \"\"\"\n",
    "        Simple Random Forest classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators : int\n",
    "            Number of trees\n",
    "        max_depth : int\n",
    "            Maximum tree depth\n",
    "        max_features : str or int\n",
    "            Number of features to consider at each split\n",
    "            - 'sqrt': sqrt(n_features)\n",
    "            - 'log2': log2(n_features)\n",
    "            - int: exact number\n",
    "        random_state : int\n",
    "            Random seed\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "    \n",
    "    def _get_max_features(self, n_features):\n",
    "        \"\"\"\n",
    "        Calculate number of features to use at each split.\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        # 1. Get predictions from all trees\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # 2. Majority voting for each sample\n",
    "        # Swap axes to get (n_samples, n_trees)\n",
    "        tree_predictions = tree_predictions.T\n",
    "        \n",
    "        predictions = []\n",
    "        for sample_preds in tree_predictions:\n",
    "            # Find most frequent prediction\n",
    "            counts = np.bincount(sample_preds)\n",
    "            predictions.append(np.argmax(counts))\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "            # Your code here\n",
    "        # Your code here\n",
    "print(\"SimpleRandomForest class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Compare Bagging vs Random Forest\n",
    "\n",
    "**Task:** Demonstrate the benefit of feature randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare bagging (all features) vs random forest (random features)\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Bagging (uses all features)\n",
    "bagging_full = SimpleRandomForest(\n",
    "    n_estimators=50, \n",
    "    max_features=n_features,  # All features\n",
    "    random_state=42\n",
    ")\n",
    "bagging_full.fit(X_train, y_train)\n",
    "bagging_full_acc = bagging_full.score(X_test, y_test)\n",
    "\n",
    "# Random Forest (uses sqrt(features))\n",
    "rf_sqrt = SimpleRandomForest(\n",
    "    n_estimators=50, \n",
    "    max_features='sqrt',  # Random feature subset\n",
    "    random_state=42\n",
    ")\n",
    "rf_sqrt.fit(X_train, y_train)\n",
    "rf_sqrt_acc = rf_sqrt.score(X_test, y_test)\n",
    "\n",
    "# Random Forest (uses log2(features))\n",
    "rf_log2 = SimpleRandomForest(\n",
    "    n_estimators=50, \n",
    "    max_features='log2',\n",
    "    random_state=42\n",
    ")\n",
    "rf_log2.fit(X_train, y_train)\n",
    "rf_log2_acc = rf_log2.score(X_test, y_test)\n",
    "\n",
    "print(f\"Total features: {n_features}\")\n",
    "print(f\"sqrt(features): {int(np.sqrt(n_features))}\")\n",
    "print(f\"log2(features): {int(np.log2(n_features))}\")\n",
    "print()\n",
    "print(f\"Bagging (all {n_features} features):        {bagging_full_acc:.4f}\")\n",
    "print(f\"Random Forest (sqrt={int(np.sqrt(n_features))} features): {rf_sqrt_acc:.4f}\")\n",
    "print(f\"Random Forest (log2={int(np.log2(n_features))} features): {rf_log2_acc:.4f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "models = ['Single Tree', 'Bagging\\n(all features)', \n",
    "          'RF (sqrt)', 'RF (log2)']\n",
    "scores = [single_acc, bagging_full_acc, rf_sqrt_acc, rf_log2_acc]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, scores, color=['red', 'orange', 'green', 'blue'], alpha=0.7)\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Comparison: Single Tree vs Bagging vs Random Forest')\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, scores):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{score:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Feature randomness successfully implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Random Forest from Scratch - Complete Implementation\n",
    "\n",
    "### Exercise 4.1: Build Complete Random Forest\n",
    "\n",
    "**Task:** Combine everything into a production-ready Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestFromScratch:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, \n",
    "                 max_features='sqrt', min_samples_split=2,\n",
    "                 random_state=None):\n",
    "        \"\"\"\n",
    "        Complete Random Forest implementation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators : int\n",
    "            Number of trees\n",
    "        max_depth : int\n",
    "            Maximum tree depth\n",
    "        max_features : str or int\n",
    "            Features to consider at each split\n",
    "        min_samples_split : int\n",
    "            Minimum samples required to split\n",
    "        random_state : int\n",
    "            Random seed\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "        self.oob_score_ = None\n",
    "    \n",
    "    def _get_max_features(self, n_features):\n",
    "        \"\"\"Calculate number of features.\"\"\"\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            return int(np.log2(n_features))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            return min(self.max_features, n_features)\n",
    "        else:\n",
    "            return n_features\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit random forest.\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        self.oob_predictions_ = np.zeros((X.shape[0], len(np.unique(y))))\n",
    "        self.oob_counts_ = np.zeros(X.shape[0])\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        max_features = self._get_max_features(n_features)\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # Bootstrap and fit\n",
    "            seed = self.random_state + i if self.random_state is not None else None\n",
    "            X_boot, y_boot, oob_idx = bootstrap_sample(X, y, random_state=seed)\n",
    "            \n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                max_features=max_features,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                random_state=seed\n",
    "            )\n",
    "            tree.fit(X_boot, y_boot)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # OOB predictions\n",
    "            if len(oob_idx) > 0:\n",
    "                oob_pred = tree.predict_proba(X[oob_idx])\n",
    "                self.oob_predictions_[oob_idx] += oob_pred\n",
    "                self.oob_counts_[oob_idx] += 1\n",
    "        \n",
    "        # Calculate OOB score\n",
    "        oob_mask = self.oob_counts_ > 0\n",
    "        if np.sum(oob_mask) > 0:\n",
    "            oob_pred = self.oob_predictions_[oob_mask] / self.oob_counts_[oob_mask, np.newaxis]\n",
    "            oob_pred_labels = np.argmax(oob_pred, axis=1)\n",
    "            self.oob_score_ = np.mean(oob_pred_labels == y[oob_mask])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \"\"\"\n",
    "        # Average probabilities from all trees\n",
    "        probas = np.mean([tree.predict_proba(X) for tree in self.trees], axis=0)\n",
    "        return probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return np.argmax(probas, axis=1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "# Test the implementation\n",
    "rf_scratch = RandomForestFromScratch(\n",
    "    n_estimators=100, \n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "rf_scratch.fit(X_train, y_train)\n",
    "scratch_acc = rf_scratch.score(X_test, y_test)\n",
    "scratch_oob = rf_scratch.oob_score_\n",
    "\n",
    "print(f\"Random Forest from Scratch:\")\n",
    "print(f\"Test Accuracy: {scratch_acc:.4f}\")\n",
    "print(f\"OOB Accuracy:  {scratch_oob:.4f}\")\n",
    "print(\"\\n\u2713 Complete Random Forest implemented from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Scikit-learn Random Forest on Breast Cancer\n",
    "\n",
    "### Exercise 5.1: Train and Evaluate Sklearn Random Forest\n",
    "\n",
    "**Task:** Use scikit-learn's production-ready Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train sklearn Random Forest\n",
    "rf_sklearn = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_features='sqrt',\n",
    "    oob_score=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "rf_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = rf_sklearn.score(X_train, y_train)\n",
    "test_acc = rf_sklearn.score(X_test, y_test)\n",
    "oob_acc = rf_sklearn.oob_score_\n",
    "\n",
    "y_pred = rf_sklearn.predict(X_test)\n",
    "y_proba = rf_sklearn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Sklearn Random Forest Results:\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"OOB Accuracy:   {oob_acc:.4f}\")\n",
    "print()\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=cancer.target_names))\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "im = axes[0].imshow(cm, cmap='Blues')\n",
    "plt.colorbar(im, ax=axes[0])\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(cancer.target_names)\n",
    "axes[0].set_yticklabels(cancer.target_names)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j, i, cm[i, j], ha=\"center\", va=\"center\", \n",
    "                    color=\"red\", fontsize=20)\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\u2713 Sklearn Random Forest successfully applied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Out-of-Bag (OOB) Error Estimation\n",
    "\n",
    "### Background\n",
    "\n",
    "**OOB Error:**\n",
    "- For each sample, predict using only trees that didn't include it in training\n",
    "- Provides unbiased error estimate without separate validation set\n",
    "- Similar to cross-validation but \"free\"\n",
    "\n",
    "### Exercise 6.1: Analyze OOB Score\n",
    "\n",
    "**Task:** Compare OOB score with test score across different n_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary number of trees\n",
    "n_trees_range = [10, 20, 50, 100, 200, 300]\n",
    "oob_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_trees in n_trees_range:\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_trees,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    oob_scores.append(rf.oob_score_)\n",
    "    test_scores.append(rf.score(X_test, y_test))\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_trees_range, oob_scores, 'o-', label='OOB Score', linewidth=2, markersize=8)\n",
    "plt.plot(n_trees_range, test_scores, 's-', label='Test Score', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('OOB Score vs Test Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"OOB vs Test Scores:\")\n",
    "print(\"n_trees  |  OOB Score  |  Test Score  |  Difference\")\n",
    "print(\"-\" * 55)\n",
    "for n, oob, test in zip(n_trees_range, oob_scores, test_scores):\n",
    "    diff = abs(oob - test)\n",
    "    print(f\"{n:7d}  |  {oob:.4f}      |  {test:.4f}      |  {diff:.4f}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- OOB score closely approximates test score\")\n",
    "print(\"- Both stabilize as number of trees increases\")\n",
    "print(\"- OOB provides free validation estimate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Demonstrate OOB as Validation Alternative\n",
    "\n",
    "**Task:** Show OOB can replace validation set for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: using validation set vs using OOB\n",
    "\n",
    "# Method 1: Traditional train/val/test split\n",
    "X_temp, X_test_2, y_temp, y_test_2 = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_2, X_val, y_train_2, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train_2)} samples\")\n",
    "print(f\"Val set:   {len(X_val)} samples\")\n",
    "print(f\"Test set:  {len(X_test_2)} samples\")\n",
    "print()\n",
    "\n",
    "# Find best max_depth using validation set\n",
    "max_depths = [3, 5, 10, 15, 20, None]\n",
    "val_scores = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=depth, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_2, y_train_2)\n",
    "    val_scores.append(rf.score(X_val, y_val))\n",
    "\n",
    "best_depth_val = max_depths[np.argmax(val_scores)]\n",
    "print(f\"Best max_depth (validation): {best_depth_val}\")\n",
    "\n",
    "# Method 2: Using full training set with OOB\n",
    "X_train_full, X_test_3, y_train_full, y_test_3 = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "oob_scores_depth = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=depth, oob_score=True, \n",
    "        random_state=42, n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train_full, y_train_full)\n",
    "    oob_scores_depth.append(rf.oob_score_)\n",
    "\n",
    "best_depth_oob = max_depths[np.argmax(oob_scores_depth)]\n",
    "print(f\"Best max_depth (OOB):        {best_depth_oob}\")\n",
    "\n",
    "# Compare final test performance\n",
    "rf_val = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=best_depth_val, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_val.fit(X_train_2, y_train_2)\n",
    "test_acc_val = rf_val.score(X_test_2, y_test_2)\n",
    "\n",
    "rf_oob = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=best_depth_oob, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_oob.fit(X_train_full, y_train_full)\n",
    "test_acc_oob = rf_oob.score(X_test_3, y_test_3)\n",
    "\n",
    "print()\n",
    "print(f\"Test accuracy (validation approach): {test_acc_val:.4f}\")\n",
    "print(f\"Test accuracy (OOB approach):        {test_acc_oob:.4f}\")\n",
    "print()\n",
    "print(\"Advantages of OOB:\")\n",
    "print(f\"- Uses more training data: {len(X_train_full)} vs {len(X_train_2)}\")\n",
    "print(\"- No need for separate validation set\")\n",
    "print(\"- Free validation estimate during training\")\n",
    "\n",
    "print(\"\\n\u2713 OOB successfully demonstrated as validation alternative!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Feature Importance on Wine Dataset\n",
    "\n",
    "### Background\n",
    "\n",
    "**Two Types of Feature Importance:**\n",
    "\n",
    "1. **MDI (Mean Decrease in Impurity)**: Built into Random Forest\n",
    "   - Based on reduction in Gini impurity\n",
    "   - Fast to compute\n",
    "   - Can be biased toward high-cardinality features\n",
    "\n",
    "2. **Permutation Importance**: Model-agnostic\n",
    "   - Measures performance drop when feature is shuffled\n",
    "   - More reliable but slower\n",
    "   - Not biased by feature cardinality\n",
    "\n",
    "### Exercise 7.1: Compare MDI and Permutation Importance\n",
    "\n",
    "**Task:** Calculate and compare both importance measures on Wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wine dataset\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "print(\"Wine Dataset:\")\n",
    "print(f\"Shape: {X_wine.shape}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "print(f\"Features: {len(wine.feature_names)}\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.2, random_state=42, stratify=y_wine\n",
    ")\n",
    "\n",
    "# Train Random Forest\n",
    "rf_wine = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_wine.fit(X_train_w, y_train_w)\n",
    "\n",
    "print(f\"Test Accuracy: {rf_wine.score(X_test_w, y_test_w):.4f}\")\n",
    "print()\n",
    "\n",
    "# 1. MDI (Mean Decrease in Impurity) - built-in\n",
    "mdi_importances = rf_wine.feature_importances_\n",
    "\n",
    "# 2. Permutation Importance\n",
    "perm_importance = permutation_importance(\n",
    "    rf_wine, X_test_w, y_test_w, \n",
    "    n_repeats=10, random_state=42, n_jobs=-1\n",
    ")\n",
    "perm_importances = perm_importance.importances_mean\n",
    "\n",
    "# Sort by MDI importance\n",
    "indices_mdi = np.argsort(mdi_importances)[::-1]\n",
    "indices_perm = np.argsort(perm_importances)[::-1]\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# MDI Importance\n",
    "axes[0].barh(range(len(mdi_importances)), mdi_importances[indices_mdi], alpha=0.7)\n",
    "axes[0].set_yticks(range(len(mdi_importances)))\n",
    "axes[0].set_yticklabels([wine.feature_names[i] for i in indices_mdi])\n",
    "axes[0].set_xlabel('MDI Importance')\n",
    "axes[0].set_title('Feature Importance (Mean Decrease in Impurity)')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Permutation Importance\n",
    "axes[1].barh(range(len(perm_importances)), perm_importances[indices_perm], \n",
    "            color='green', alpha=0.7)\n",
    "axes[1].set_yticks(range(len(perm_importances)))\n",
    "axes[1].set_yticklabels([wine.feature_names[i] for i in indices_perm])\n",
    "axes[1].set_xlabel('Permutation Importance')\n",
    "axes[1].set_title('Feature Importance (Permutation)')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 5 features by each method\n",
    "print(\"Top 5 Features by MDI:\")\n",
    "for i in range(5):\n",
    "    idx = indices_mdi[i]\n",
    "    print(f\"{i+1}. {wine.feature_names[idx]:25s}: {mdi_importances[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 Features by Permutation:\")\n",
    "for i in range(5):\n",
    "    idx = indices_perm[i]\n",
    "    print(f\"{i+1}. {wine.feature_names[idx]:25s}: {perm_importances[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\u2713 Feature importance successfully calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Feature Selection Using Importance\n",
    "\n",
    "**Task:** Use feature importance for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top k features based on importance\n",
    "k_values = [3, 5, 7, 10, 13]  # 13 = all features\n",
    "test_scores_mdi = []\n",
    "test_scores_perm = []\n",
    "\n",
    "for k in k_values:\n",
    "    # Using MDI importance\n",
    "    top_k_mdi = indices_mdi[:k]\n",
    "    X_train_selected_mdi = X_train_w[:, top_k_mdi]\n",
    "    X_test_selected_mdi = X_test_w[:, top_k_mdi]\n",
    "    \n",
    "    rf_mdi = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_mdi.fit(X_train_selected_mdi, y_train_w)\n",
    "    test_scores_mdi.append(rf_mdi.score(X_test_selected_mdi, y_test_w))\n",
    "    \n",
    "    # Using Permutation importance\n",
    "    top_k_perm = indices_perm[:k]\n",
    "    X_train_selected_perm = X_train_w[:, top_k_perm]\n",
    "    X_test_selected_perm = X_test_w[:, top_k_perm]\n",
    "    \n",
    "    rf_perm = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf_perm.fit(X_train_selected_perm, y_train_w)\n",
    "    test_scores_perm.append(rf_perm.score(X_test_selected_perm, y_test_w))\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, test_scores_mdi, 'o-', label='MDI Selection', \n",
    "         linewidth=2, markersize=8)\n",
    "plt.plot(k_values, test_scores_perm, 's-', label='Permutation Selection', \n",
    "         linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Top Features')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Performance vs Number of Features')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance with Different Feature Counts:\")\n",
    "print(\"k    |  MDI     |  Perm    \")\n",
    "print(\"-\" * 30)\n",
    "for k, mdi_score, perm_score in zip(k_values, test_scores_mdi, test_scores_perm):\n",
    "    print(f\"{k:2d}   |  {mdi_score:.4f}  |  {perm_score:.4f}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Small feature subset can achieve good performance\")\n",
    "print(\"- Feature importance helps identify most informative features\")\n",
    "print(\"- Can reduce dimensionality and training time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Hyperparameter Tuning\n",
    "\n",
    "### Background\n",
    "\n",
    "**Key Random Forest Hyperparameters:**\n",
    "- `n_estimators`: Number of trees (more is usually better)\n",
    "- `max_features`: Features considered at each split\n",
    "- `max_depth`: Maximum tree depth\n",
    "- `min_samples_split`: Minimum samples to split a node\n",
    "- `min_samples_leaf`: Minimum samples in a leaf\n",
    "\n",
    "### Exercise 8.1: Grid Search for Optimal Hyperparameters\n",
    "\n",
    "**Task:** Use GridSearchCV to find best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "print(\"Parameter Grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"\\nTotal combinations: {total_combinations}\")\n",
    "print(\"Running Grid Search (this may take a minute)...\\n\")\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest CV Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "test_score = best_rf.score(X_test, y_test)\n",
    "print(f\"Test Score:    {test_score:.4f}\")\n",
    "\n",
    "# Compare with default parameters\n",
    "default_rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "default_rf.fit(X_train, y_train)\n",
    "default_score = default_rf.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\nDefault RF Test Score: {default_score:.4f}\")\n",
    "print(f\"Tuned RF Test Score:   {test_score:.4f}\")\n",
    "print(f\"Improvement:           {(test_score - default_score) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n\u2713 Hyperparameter tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8.2: Visualize Hyperparameter Effects\n",
    "\n",
    "**Task:** Study the effect of individual hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study effect of n_estimators and max_depth\n",
    "n_estimators_range = [10, 25, 50, 100, 150, 200]\n",
    "max_depth_range = [5, 10, 15, 20, None]\n",
    "\n",
    "# Effect of n_estimators\n",
    "scores_n_est = []\n",
    "for n_est in n_estimators_range:\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    scores_n_est.append(rf.score(X_test, y_test))\n",
    "\n",
    "# Effect of max_depth\n",
    "scores_depth = []\n",
    "for depth in max_depth_range:\n",
    "    rf = RandomForestClassifier(max_depth=depth, n_estimators=100, \n",
    "                                random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    scores_depth.append(rf.score(X_test, y_test))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# n_estimators effect\n",
    "axes[0].plot(n_estimators_range, scores_n_est, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Estimators')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Effect of n_estimators')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# max_depth effect\n",
    "depth_labels = [str(d) if d is not None else 'None' for d in max_depth_range]\n",
    "axes[1].plot(range(len(max_depth_range)), scores_depth, 'o-', \n",
    "            linewidth=2, markersize=8, color='green')\n",
    "axes[1].set_xticks(range(len(max_depth_range)))\n",
    "axes[1].set_xticklabels(depth_labels)\n",
    "axes[1].set_xlabel('Max Depth')\n",
    "axes[1].set_ylabel('Test Accuracy')\n",
    "axes[1].set_title('Effect of max_depth')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"- n_estimators: More trees generally improve performance but with diminishing returns\")\n",
    "print(\"- max_depth: Too shallow underfits, too deep may overfit\")\n",
    "print(\"- Random Forest is relatively robust to hyperparameter choices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Random Forest Regression on California Housing\n",
    "\n",
    "### Background\n",
    "\n",
    "Random Forest works equally well for regression:\n",
    "- Each tree predicts a continuous value\n",
    "- Final prediction = average of all tree predictions\n",
    "- Uses MSE instead of Gini impurity for splits\n",
    "\n",
    "### Exercise 9.1: Apply Random Forest Regression\n",
    "\n",
    "**Task:** Predict housing prices using Random Forest Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "\n",
    "print(\"California Housing Dataset:\")\n",
    "print(f\"Shape: {X_housing.shape}\")\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "print(f\"Target: Median house value (in $100,000s)\")\n",
    "print(f\"Target range: [{y_housing.min():.2f}, {y_housing.max():.2f}]\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
    "    X_housing, y_housing, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "# Your code here\n",
    "        if self.max_features == 'sqrt':\n",
    "            return int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            return int(np.log2(n_features))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            return min(self.max_features, n_features)\n",
    "        else:\n",
    "            return n_features\n",
    "# Evaluate\n",
    "train_mse = mean_squared_error(y_train_h, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test_h, y_test_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "train_r2 = r2_score(y_train_h, y_train_pred)\n",
    "test_r2 = r2_score(y_test_h, y_test_pred)\n",
    "\n",
    "print(\"Random Forest Regressor Results:\")\n",
    "print(f\"Train RMSE: ${train_rmse * 100000:.2f}\")\n",
    "print(f\"Test RMSE:  ${test_rmse * 100000:.2f}\")\n",
    "print(f\"Train R\u00b2:   {train_r2:.4f}\")\n",
    "print(f\"Test R\u00b2:    {test_r2:.4f}\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0].scatter(y_test_h, y_test_pred, alpha=0.5, s=10)\n",
    "axes[0].plot([y_test_h.min(), y_test_h.max()], \n",
    "            [y_test_h.min(), y_test_h.max()], \n",
    "            'r--', linewidth=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Price ($100k)')\n",
    "axes[0].set_ylabel('Predicted Price ($100k)')\n",
    "axes[0].set_title('Predicted vs Actual Prices')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test_h - y_test_pred\n",
    "axes[1].scatter(y_test_pred, residuals, alpha=0.5, s=10)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Price ($100k)')\n",
    "axes[1].set_ylabel('Residuals ($100k)')\n",
    "axes[1].set_title('Residual Plot')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Random Forest Regression successfully applied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9.2: Feature Importance for Regression\n",
    "\n",
    "**Task:** Identify most important features for housing price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = rf_regressor.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(importances)), importances[indices], alpha=0.7)\n",
    "plt.yticks(range(len(importances)), [housing.feature_names[i] for i in indices])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance for Housing Price Prediction')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importances (sorted):\")\n",
    "for i in range(len(importances)):\n",
    "    idx = indices[i]\n",
    "    print(f\"{i+1}. {housing.feature_names[idx]:15s}: {importances[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- MedInc (median income) is most important predictor\")\n",
    "print(\"- Location features (latitude, longitude) also very important\")\n",
    "print(\"- These align with domain knowledge about housing prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Imbalanced Classification\n",
    "\n",
    "### Background\n",
    "\n",
    "**Handling Imbalanced Data with Random Forest:**\n",
    "- `class_weight='balanced'`: Automatically adjust weights inversely proportional to class frequencies\n",
    "- `class_weight={0: w0, 1: w1}`: Custom weights\n",
    "- Alternative: Use stratified sampling\n",
    "\n",
    "### Exercise 10.1: Create and Handle Imbalanced Dataset\n",
    "\n",
    "**Task:** Demonstrate class_weight parameter on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced dataset\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    weights=[0.9, 0.1],  # 90% class 0, 10% class 1\n",
    "    flip_y=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Imbalanced Dataset:\")\n",
    "print(f\"Total samples: {len(y_imb)}\")\n",
    "print(f\"Class 0: {np.sum(y_imb == 0)} ({np.sum(y_imb == 0) / len(y_imb) * 100:.1f}%)\")\n",
    "print(f\"Class 1: {np.sum(y_imb == 1)} ({np.sum(y_imb == 1) / len(y_imb) * 100:.1f}%)\")\n",
    "print(f\"Imbalance ratio: {np.sum(y_imb == 0) / np.sum(y_imb == 1):.1f}:1\")\n",
    "print()\n",
    "\n",
    "# Split data\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "# Model 1: Standard Random Forest (no weighting)\n",
    "rf_no_weight = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_no_weight.fit(X_train_imb, y_train_imb)\n",
    "y_pred_no_weight = rf_no_weight.predict(X_test_imb)\n",
    "\n",
    "# Model 2: Balanced class weights\n",
    "rf_balanced = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_balanced.fit(X_train_imb, y_train_imb)\n",
    "y_pred_balanced = rf_balanced.predict(X_test_imb)\n",
    "\n",
    "# Compare results\n",
    "print(\"Results Without Class Weighting:\")\n",
    "print(classification_report(y_test_imb, y_pred_no_weight, \n",
    "                          target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "print(\"\\nResults With Balanced Class Weighting:\")\n",
    "print(classification_report(y_test_imb, y_pred_balanced, \n",
    "                          target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# Visualize confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# No weighting\n",
    "cm_no_weight = confusion_matrix(y_test_imb, y_pred_no_weight)\n",
    "im1 = axes[0].imshow(cm_no_weight, cmap='Blues')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_yticks([0, 1])\n",
    "axes[0].set_xticklabels(['Class 0', 'Class 1'])\n",
    "axes[0].set_yticklabels(['Class 0', 'Class 1'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[0].text(j, i, cm_no_weight[i, j], ha=\"center\", va=\"center\", \n",
    "                    color=\"red\", fontsize=20)\n",
    "axes[0].set_title('Without Class Weighting')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Balanced weighting\n",
    "cm_balanced = confusion_matrix(y_test_imb, y_pred_balanced)\n",
    "im2 = axes[1].imshow(cm_balanced, cmap='Blues')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "axes[1].set_xticks([0, 1])\n",
    "axes[1].set_yticks([0, 1])\n",
    "axes[1].set_xticklabels(['Class 0', 'Class 1'])\n",
    "axes[1].set_yticklabels(['Class 0', 'Class 1'])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, cm_balanced[i, j], ha=\"center\", va=\"center\", \n",
    "                    color=\"red\", fontsize=20)\n",
    "axes[1].set_title('With Balanced Class Weighting')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and compare metrics for minority class\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"\\nMinority Class (Class 1) Metrics:\")\n",
    "print(\"Metric          |  No Weight  |  Balanced  |  Improvement\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "no_weight_scores = [\n",
    "    precision_score(y_test_imb, y_pred_no_weight),\n",
    "    recall_score(y_test_imb, y_pred_no_weight),\n",
    "    f1_score(y_test_imb, y_pred_no_weight)\n",
    "]\n",
    "balanced_scores = [\n",
    "    precision_score(y_test_imb, y_pred_balanced),\n",
    "    recall_score(y_test_imb, y_pred_balanced),\n",
    "    f1_score(y_test_imb, y_pred_balanced)\n",
    "]\n",
    "\n",
    "for metric, no_w, bal in zip(metrics, no_weight_scores, balanced_scores):\n",
    "    improvement = ((bal - no_w) / no_w * 100) if no_w > 0 else 0\n",
    "    print(f\"{metric:15s} |  {no_w:.4f}     |  {bal:.4f}    |  {improvement:+.1f}%\")\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Without weighting: Model ignores minority class (low recall)\")\n",
    "print(\"- With balanced weights: Much better minority class detection\")\n",
    "print(\"- Trade-off: Slightly more false positives but catches minority class\")\n",
    "\n",
    "print(\"\\n\u2713 Class weighting successfully demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Implement Random Forest Regressor from Scratch\n",
    "\n",
    "Extend the classification implementation to handle regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressorScratch:\n",
    "    \"\"\"\n",
    "    Random Forest Regressor from scratch.\n",
    "    \n",
    "    Hint: Main difference from classifier:\n",
    "    - Use DecisionTreeRegressor instead of DecisionTreeClassifier\n",
    "    - Predict by averaging tree predictions instead of voting\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, max_depth=None, \n",
    "                 max_features='sqrt', random_state=None):\n",
    "        # Your code here\n",
    "            # 1. Bootstrap sample\n",
    "            X_boot, y_boot, _ = bootstrap_sample(X, y, random_state=seed)\n",
    "        # Your code here\n",
    "        # Your code here\n",
    "print(\"Challenge 1: Implement Random Forest Regressor from scratch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Extremely Randomized Trees (Extra-Trees)\n",
    "\n",
    "Implement Extra-Trees which uses random thresholds instead of optimal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Your task: Compare Random Forest vs Extra-Trees\n",
    "# Hint: Extra-Trees are faster to train but may have slightly lower accuracy\n",
    "\n",
    "# Your code here\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Transpose to (n_samples, n_trees)\n",
    "        tree_predictions = tree_predictions.T\n",
    "        \n",
    "        predictions = []\n",
    "        for sample_preds in tree_predictions:\n",
    "            counts = np.bincount(sample_preds)\n",
    "            predictions.append(np.argmax(counts))\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "print(\"Challenge 2: Compare Random Forest with Extra-Trees!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Implement Weighted Voting\n",
    "\n",
    "Instead of simple majority voting, weight each tree's vote by its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedRandomForest:\n",
    "    \"\"\"\n",
    "    Random Forest with weighted voting based on tree accuracy.\n",
    "    \n",
    "    Hint: \n",
    "    - Store validation accuracy for each tree\n",
    "    - Use accuracy as weight in final voting\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, random_state=None):\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge 3: Implement weighted voting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Proximity Matrix\n",
    "\n",
    "Calculate and visualize the proximity matrix between samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_proximity_matrix(rf_model, X):\n",
    "    \"\"\"\n",
    "    Compute proximity matrix for Random Forest.\n",
    "    \n",
    "    Proximity between samples i and j = \n",
    "    (Number of trees where i and j end in same leaf) / (Total trees)\n",
    "    \n",
    "    Hint: Use .apply() method to get leaf indices\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Use proximity for clustering or visualization\n",
    "\n",
    "print(\"Challenge 4: Implement proximity matrix calculation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Why does bootstrap sampling create diversity in Random Forest?**\n",
    "   - What percentage of samples are OOB on average?\n",
    "   - How does this affect each tree?\n",
    "\n",
    "2. **What is the key difference between Bagging and Random Forest?**\n",
    "   - How does feature randomness help?\n",
    "   - Why use sqrt(features) instead of all features?\n",
    "\n",
    "3. **When would you prefer Random Forest over a single Decision Tree?**\n",
    "   - Consider bias-variance tradeoff\n",
    "   - What about interpretability?\n",
    "\n",
    "4. **How does OOB error estimation work?**\n",
    "   - Why is it similar to cross-validation?\n",
    "   - When is it most useful?\n",
    "\n",
    "5. **What are the pros and cons of MDI vs Permutation Importance?**\n",
    "   - Which is faster?\n",
    "   - Which is more reliable?\n",
    "\n",
    "6. **How do you handle imbalanced data with Random Forest?**\n",
    "   - What does class_weight='balanced' do?\n",
    "   - What are alternative approaches?\n",
    "\n",
    "7. **Random Forest for classification vs regression: What changes?**\n",
    "   - Split criterion?\n",
    "   - Aggregation method?\n",
    "\n",
    "8. **Why doesn't Random Forest typically overfit even with many trees?**\n",
    "   - Think about the ensemble averaging\n",
    "   - What happens as n_estimators increases?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "- **Bootstrap Sampling**: Create diverse training sets through sampling with replacement\n",
    "- **Bagging**: Reduce variance by training multiple models on bootstrap samples\n",
    "- **Feature Randomness**: De-correlate trees by using random feature subsets\n",
    "- **Random Forest**: Powerful ensemble combining bagging and feature randomness\n",
    "- **OOB Estimation**: Free validation using out-of-bag samples\n",
    "- **Feature Importance**: MDI and permutation methods for interpretation\n",
    "- **Hyperparameter Tuning**: Grid search for optimal performance\n",
    "- **RF Regression**: Apply Random Forest to continuous target variables\n",
    "- **Imbalanced Data**: Handle class imbalance with class_weight parameter\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **Random Forest Advantages:**\n",
    "   - Excellent out-of-box performance\n",
    "   - Robust to overfitting\n",
    "   - Handles high-dimensional data well\n",
    "   - Provides feature importance\n",
    "   - Works for both classification and regression\n",
    "   - Requires minimal preprocessing\n",
    "\n",
    "2. **Key Hyperparameters:**\n",
    "   - `n_estimators`: More is usually better (diminishing returns)\n",
    "   - `max_features`: sqrt(n) for classification, n/3 for regression\n",
    "   - `max_depth`: Controls individual tree complexity\n",
    "   - `min_samples_split/leaf`: Prevents overfitting\n",
    "\n",
    "3. **Best Practices:**\n",
    "   - Start with default parameters\n",
    "   - Use OOB score for quick validation\n",
    "   - Increase n_estimators if resources allow\n",
    "   - Use class_weight for imbalanced data\n",
    "   - Check feature importance for insights\n",
    "   - Consider permutation importance for reliability\n",
    "\n",
    "4. **When to Use Random Forest:**\n",
    "   - Need strong baseline quickly\n",
    "   - Interpretability less critical than performance\n",
    "   - Mixed feature types (numerical + categorical)\n",
    "   - Non-linear relationships\n",
    "   - Moderate dataset sizes (not huge)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Study Gradient Boosting (XGBoost, LightGBM, CatBoost)\n",
    "- Learn about ensemble stacking\n",
    "- Explore feature engineering for Random Forest\n",
    "- Practice on Kaggle competitions\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now have a deep understanding of Random Forest algorithms, from bootstrap sampling to production deployment. Random Forest remains one of the most reliable and widely-used machine learning algorithms for both classification and regression tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Module 3 - Exercise 2: Tree Algorithms\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module3-trees/exercise2-tree-algorithms.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "\n",
    "- Implement the ID3 algorithm from scratch for categorical data\n",
    "- Apply ID3 to the classic Play Tennis dataset\n",
    "- Calculate and compare Information Gain and Gain Ratio (C4.5 improvement)\n",
    "- Handle continuous features using discretization techniques\n",
    "- Use CART algorithm with sklearn on real datasets\n",
    "- Compare ID3 and CART algorithms\n",
    "- Handle missing values in decision trees\n",
    "- Apply tree algorithms to real-world classification problems\n",
    "- Understand trade-offs between different tree algorithms\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of entropy and information theory\n",
    "- Basic probability and statistics\n",
    "- Familiarity with classification problems\n",
    "- NumPy proficiency\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this cell first to import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: ID3 Algorithm Implementation\n",
    "\n",
    "### Background\n",
    "\n",
    "ID3 (Iterative Dichotomiser 3) is one of the earliest and most fundamental decision tree algorithms. It uses **Information Gain** to select the best attribute at each node.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "1. **Entropy**: Measures impurity/disorder in a dataset\n",
    "   $$H(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
    "   where $p_i$ is the proportion of class $i$\n",
    "\n",
    "2. **Information Gain**: Reduction in entropy after splitting\n",
    "   $$IG(S, A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v)$$\n",
    "   where $S_v$ is the subset where attribute $A$ has value $v$\n",
    "\n",
    "3. **Algorithm Steps**:\n",
    "   - Calculate entropy of the dataset\n",
    "   - For each attribute, calculate information gain\n",
    "   - Split on attribute with highest information gain\n",
    "   - Recursively build tree until stopping criterion\n",
    "\n",
    "### Exercise 1.1: Implement Entropy Function\n",
    "\n",
    "**Task:** Implement entropy calculation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    Calculate entropy of a dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Target values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Entropy value (0 = pure, higher = more impure)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Hint: Count class frequencies, calculate probabilities\n",
    "    # Handle edge case: log(0) is undefined, but 0*log(0) should be 0\n",
    "    \n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count occurrences of each class\n",
    "    counts = \n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = \n",
    "    \n",
    "    # Calculate entropy: -Σ p_i * log2(p_i)\n",
    "    ent = 0\n",
    "    for p in probabilities:\n",
    "        if p > 0:  # Avoid log(0)\n",
    "            ent += \n",
    "    \n",
    "    return ent\n",
    "\n",
    "# Test entropy function\n",
    "print(\"Testing Entropy Function:\")\n",
    "print(f\"Entropy of [0,0,0,0]: {entropy([0,0,0,0]):.4f} (should be 0.0000 - pure)\")\n",
    "print(f\"Entropy of [0,0,1,1]: {entropy([0,0,1,1]):.4f} (should be 1.0000 - max impurity)\")\n",
    "print(f\"Entropy of [0,0,0,1]: {entropy([0,0,0,1]):.4f} (should be 0.8113)\")\n",
    "print(f\"Entropy of [0,1,2]: {entropy([0,1,2]):.4f} (should be 1.5850 - 3 classes)\")\n",
    "\n",
    "assert abs(entropy([0,0,0,0]) - 0.0) < 0.001, \"Pure set should have 0 entropy\"\n",
    "assert abs(entropy([0,0,1,1]) - 1.0) < 0.001, \"50-50 split should have entropy 1\"\n",
    "print(\"\\n✓ Entropy function works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Implement Information Gain\n",
    "\n",
    "**Task:** Calculate information gain for a given attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, attribute_index):\n",
    "    \"\"\"\n",
    "    Calculate information gain for a given attribute.\n",
    "    \n",
    "    IG(S, A) = H(S) - Σ (|S_v| / |S|) * H(S_v)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame or np.ndarray\n",
    "        Features\n",
    "    y : array-like\n",
    "        Target values\n",
    "    attribute_index : int or str\n",
    "        Index or name of attribute to evaluate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Information gain\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # Calculate initial entropy\n",
    "    total_entropy = \n",
    "    \n",
    "    # Get attribute values\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        attribute_values = X.iloc[:, attribute_index] if isinstance(attribute_index, int) else X[attribute_index]\n",
    "    else:\n",
    "        attribute_values = X[:, attribute_index]\n",
    "    \n",
    "    # Calculate weighted average of entropies after split\n",
    "    weighted_entropy = 0\n",
    "    unique_values = np.unique(attribute_values)\n",
    "    \n",
    "    for value in unique_values:\n",
    "        # Get subset where attribute == value\n",
    "        mask = \n",
    "        subset_y = \n",
    "        \n",
    "        # Calculate weight and entropy for this subset\n",
    "        weight = \n",
    "        subset_entropy = \n",
    "        \n",
    "        weighted_entropy += weight * subset_entropy\n",
    "    \n",
    "    # Information gain = total entropy - weighted entropy\n",
    "    ig = \n",
    "    \n",
    "    return ig\n",
    "\n",
    "# Test with simple example\n",
    "X_test = pd.DataFrame({\n",
    "    'Weather': ['Sunny', 'Sunny', 'Rainy', 'Rainy'],\n",
    "    'Temp': ['Hot', 'Cool', 'Cool', 'Cool']\n",
    "})\n",
    "y_test = np.array([0, 0, 1, 1])\n",
    "\n",
    "ig_weather = information_gain(X_test, y_test, 'Weather')\n",
    "ig_temp = information_gain(X_test, y_test, 'Temp')\n",
    "\n",
    "print(\"Testing Information Gain:\")\n",
    "print(f\"IG(Weather): {ig_weather:.4f}\")\n",
    "print(f\"IG(Temp): {ig_temp:.4f}\")\n",
    "print(\"\\n✓ Information gain function works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Implement ID3 Algorithm\n",
    "\n",
    "**Task:** Build a complete ID3 decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3Node:\n",
    "    \"\"\"Node in an ID3 decision tree.\"\"\"\n",
    "    def __init__(self, attribute=None, label=None, branches=None):\n",
    "        self.attribute = attribute  # Attribute to split on (None for leaf)\n",
    "        self.label = label          # Class label (for leaf nodes)\n",
    "        self.branches = branches or {}  # Dictionary: value -> child node\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.label is not None\n",
    "\n",
    "\n",
    "class ID3Classifier:\n",
    "    \"\"\"\n",
    "    ID3 Decision Tree Classifier for categorical features.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        max_depth : int or None\n",
    "            Maximum depth of tree\n",
    "        min_samples_split : int\n",
    "            Minimum samples required to split\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.tree = None\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        \"\"\"\n",
    "        Build ID3 decision tree.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame or np.ndarray\n",
    "            Training features (categorical)\n",
    "        y : array-like\n",
    "            Target values\n",
    "        feature_names : list, optional\n",
    "            Names of features\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = X.columns.tolist()\n",
    "            X = X.values\n",
    "        else:\n",
    "            self.feature_names = feature_names or [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "        \n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Build tree recursively\n",
    "        available_attributes = list(range(X.shape[1]))\n",
    "        self.tree = self._build_tree(X, y, available_attributes, depth=0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _build_tree(self, X, y, available_attributes, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        ID3Node\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        \n",
    "        # Base case 1: All samples have same class\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return ID3Node(label=y[0])\n",
    "        \n",
    "        # Base case 2: No more attributes or max depth reached\n",
    "        if len(available_attributes) == 0 or (self.max_depth and depth >= self.max_depth):\n",
    "            # Return most common class\n",
    "            most_common = \n",
    "            return ID3Node(label=most_common)\n",
    "        \n",
    "        # Base case 3: Too few samples to split\n",
    "        if len(y) < self.min_samples_split:\n",
    "            most_common = \n",
    "            return ID3Node(label=most_common)\n",
    "        \n",
    "        # Find best attribute using information gain\n",
    "        best_attribute = None\n",
    "        best_gain = -1\n",
    "        \n",
    "        for attr in available_attributes:\n",
    "            gain = information_gain(X, y, attr)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_attribute = attr\n",
    "        \n",
    "        # If no information gain, return leaf\n",
    "        if best_gain == 0:\n",
    "            most_common = \n",
    "            return ID3Node(label=most_common)\n",
    "        \n",
    "        # Create node for this attribute\n",
    "        node = ID3Node(attribute=best_attribute)\n",
    "        \n",
    "        # Get unique values for this attribute\n",
    "        unique_values = np.unique(X[:, best_attribute])\n",
    "        \n",
    "        # Remove this attribute from available list\n",
    "        remaining_attributes = [a for a in available_attributes if a != best_attribute]\n",
    "        \n",
    "        # Create branches for each value\n",
    "        for value in unique_values:\n",
    "            # Get subset where attribute == value\n",
    "            mask = X[:, best_attribute] == value\n",
    "            X_subset = X[mask]\n",
    "            y_subset = y[mask]\n",
    "            \n",
    "            # Recursively build subtree\n",
    "            if len(y_subset) > 0:\n",
    "                node.branches[value] = self._build_tree(\n",
    "                    X_subset, y_subset, remaining_attributes, depth + 1\n",
    "                )\n",
    "            else:\n",
    "                # If no samples, use most common class from parent\n",
    "                most_common = \n",
    "                node.branches[value] = ID3Node(label=most_common)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame or np.ndarray\n",
    "            Test features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Predicted labels\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        return np.array([self._predict_one(x) for x in X])\n",
    "    \n",
    "    def _predict_one(self, x):\n",
    "        \"\"\"Predict label for a single sample.\"\"\"\n",
    "        node = self.tree\n",
    "        \n",
    "        while not node.is_leaf():\n",
    "            attribute_value = x[node.attribute]\n",
    "            \n",
    "            # If value not seen in training, return most common class\n",
    "            if attribute_value not in node.branches:\n",
    "                # Find a leaf and return its label\n",
    "                # (Simple fallback - could be improved)\n",
    "                break\n",
    "            \n",
    "            node = node.branches[attribute_value]\n",
    "        \n",
    "        return node.label\n",
    "    \n",
    "    def print_tree(self, node=None, depth=0, value=None):\n",
    "        \"\"\"\n",
    "        Print the decision tree in a readable format.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "        \n",
    "        indent = \"  \" * depth\n",
    "        \n",
    "        if value is not None:\n",
    "            print(f\"{indent}└─ [{value}]\")\n",
    "        \n",
    "        if node.is_leaf():\n",
    "            print(f\"{indent}   → Class: {node.label}\")\n",
    "        else:\n",
    "            attr_name = self.feature_names[node.attribute]\n",
    "            print(f\"{indent}   Split on: {attr_name}\")\n",
    "            for value, child in sorted(node.branches.items()):\n",
    "                self.print_tree(child, depth + 1, value)\n",
    "\n",
    "print(\"ID3 Classifier implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Play Tennis Dataset\n",
    "\n",
    "### Background\n",
    "\n",
    "The Play Tennis dataset is a classic example used to teach decision trees. It predicts whether someone will play tennis based on weather conditions.\n",
    "\n",
    "**Features:**\n",
    "- Outlook: Sunny, Overcast, Rainy\n",
    "- Temperature: Hot, Mild, Cool\n",
    "- Humidity: High, Normal\n",
    "- Wind: Weak, Strong\n",
    "\n",
    "**Target:** Play Tennis (Yes/No)\n",
    "\n",
    "### Exercise 2.1: Apply ID3 to Play Tennis Dataset\n",
    "\n",
    "**Task:** Build a decision tree for the Play Tennis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Play Tennis dataset\n",
    "play_tennis_data = {\n",
    "    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy', 'Rainy', 'Overcast',\n",
    "                'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy'],\n",
    "    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool',\n",
    "                    'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],\n",
    "    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal',\n",
    "                 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],\n",
    "    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong',\n",
    "             'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],\n",
    "    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes',\n",
    "             'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n",
    "}\n",
    "\n",
    "df_tennis = pd.DataFrame(play_tennis_data)\n",
    "\n",
    "print(\"Play Tennis Dataset:\")\n",
    "print(df_tennis)\n",
    "print(f\"\\nDataset shape: {df_tennis.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df_tennis['Play'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate information gain for each attribute\n",
    "X_tennis = df_tennis.drop('Play', axis=1)\n",
    "y_tennis = df_tennis['Play']\n",
    "\n",
    "print(\"Information Gain for Each Attribute:\\n\")\n",
    "for col in X_tennis.columns:\n",
    "    ig = information_gain(X_tennis, y_tennis, col)\n",
    "    print(f\"{col:15s}: {ig:.4f}\")\n",
    "\n",
    "print(\"\\nBest attribute to split on: The one with highest IG!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ID3 tree\n",
    "id3_tennis = ID3Classifier(max_depth=5)\n",
    "id3_tennis.fit(X_tennis, y_tennis)\n",
    "\n",
    "# Print the tree\n",
    "print(\"\\nID3 Decision Tree for Play Tennis:\\n\")\n",
    "print(\"=\" * 50)\n",
    "id3_tennis.print_tree()\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions on training data\n",
    "y_pred_tennis = id3_tennis.predict(X_tennis)\n",
    "accuracy_tennis = accuracy_score(y_tennis, y_pred_tennis)\n",
    "\n",
    "print(f\"\\nTraining Accuracy: {accuracy_tennis:.4f}\")\n",
    "print(\"\\nPredictions vs Actual:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_tennis,\n",
    "    'Predicted': y_pred_tennis\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Test on new examples\n",
    "print(\"\\nTesting on new examples:\")\n",
    "new_examples = pd.DataFrame({\n",
    "    'Outlook': ['Sunny', 'Rainy', 'Overcast'],\n",
    "    'Temperature': ['Cool', 'Mild', 'Hot'],\n",
    "    'Humidity': ['Normal', 'Normal', 'High'],\n",
    "    'Wind': ['Weak', 'Weak', 'Weak']\n",
    "})\n",
    "\n",
    "predictions = id3_tennis.predict(new_examples)\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Example {i+1}: {dict(new_examples.iloc[i])} → {pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Gain Ratio (C4.5 Improvement)\n",
    "\n",
    "### Background\n",
    "\n",
    "**Problem with Information Gain:** It is biased toward attributes with many values.\n",
    "\n",
    "**C4.5's Solution: Gain Ratio**\n",
    "\n",
    "$$\\text{GainRatio}(S, A) = \\frac{IG(S, A)}{\\text{SplitInfo}(S, A)}$$\n",
    "\n",
    "Where **Split Information** measures how much information is needed to specify the value of an attribute:\n",
    "\n",
    "$$\\text{SplitInfo}(S, A) = -\\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\log_2 \\frac{|S_v|}{|S|}$$\n",
    "\n",
    "This penalizes attributes that split data into many small subsets.\n",
    "\n",
    "### Exercise 3.1: Implement Gain Ratio\n",
    "\n",
    "**Task:** Implement split information and gain ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_information(X, attribute_index):\n",
    "    \"\"\"\n",
    "    Calculate split information for an attribute.\n",
    "    \n",
    "    SplitInfo(S, A) = -Σ (|S_v| / |S|) * log2(|S_v| / |S|)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame or np.ndarray\n",
    "        Features\n",
    "    attribute_index : int or str\n",
    "        Attribute to evaluate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Split information\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        attribute_values = X.iloc[:, attribute_index] if isinstance(attribute_index, int) else X[attribute_index]\n",
    "    else:\n",
    "        attribute_values = X[:, attribute_index]\n",
    "    \n",
    "    n_total = len(attribute_values)\n",
    "    value_counts = pd.Series(attribute_values).value_counts()\n",
    "    \n",
    "    split_info = 0\n",
    "    for count in value_counts:\n",
    "        proportion = count / n_total\n",
    "        if proportion > 0:\n",
    "            split_info += \n",
    "    \n",
    "    return split_info\n",
    "\n",
    "\n",
    "def gain_ratio(X, y, attribute_index):\n",
    "    \"\"\"\n",
    "    Calculate gain ratio (C4.5 criterion).\n",
    "    \n",
    "    GainRatio = InformationGain / SplitInformation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame or np.ndarray\n",
    "        Features\n",
    "    y : array-like\n",
    "        Target values\n",
    "    attribute_index : int or str\n",
    "        Attribute to evaluate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Gain ratio\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    ig = \n",
    "    si = \n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if si == 0:\n",
    "        return 0\n",
    "    \n",
    "    gr = \n",
    "    return gr\n",
    "\n",
    "print(\"Gain Ratio functions implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Compare Information Gain vs Gain Ratio\n",
    "\n",
    "**Task:** Compare both metrics on the Play Tennis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate both metrics for each attribute\n",
    "print(\"Comparison: Information Gain vs Gain Ratio\\n\")\n",
    "print(f\"{'Attribute':<15} {'Info Gain':<12} {'Split Info':<12} {'Gain Ratio':<12}\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "for col in X_tennis.columns:\n",
    "    ig = information_gain(X_tennis, y_tennis, col)\n",
    "    si = split_information(X_tennis, col)\n",
    "    gr = gain_ratio(X_tennis, y_tennis, col)\n",
    "    print(f\"{col:<15} {ig:<12.4f} {si:<12.4f} {gr:<12.4f}\")\n",
    "\n",
    "print(\"\\nObservation: Gain ratio normalizes by split information,\")\n",
    "print(\"reducing bias toward attributes with many values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Demonstrate Bias with Synthetic Example\n",
    "\n",
    "**Task:** Create an example showing information gain's bias toward many-valued attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with an attribute having unique values (like ID)\n",
    "n_samples = 20\n",
    "bias_data = pd.DataFrame({\n",
    "    'ID': list(range(n_samples)),  # Unique for each sample\n",
    "    'UsefulFeature': ['A']*10 + ['B']*10,  # Actually correlated with target\n",
    "    'RandomFeature': np.random.choice(['X', 'Y', 'Z'], n_samples),\n",
    "})\n",
    "# Target: correlated with UsefulFeature\n",
    "bias_target = np.array([0]*8 + [1]*2 + [1]*8 + [0]*2)\n",
    "\n",
    "print(\"Dataset with potential bias:\")\n",
    "print(bias_data.head(10))\n",
    "print(f\"\\nTarget: {bias_target[:10]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(f\"{'Attribute':<20} {'Info Gain':<15} {'Gain Ratio':<15}\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "for col in bias_data.columns:\n",
    "    ig = information_gain(bias_data, bias_target, col)\n",
    "    gr = gain_ratio(bias_data, bias_target, col)\n",
    "    print(f\"{col:<20} {ig:<15.4f} {gr:<15.4f}\")\n",
    "\n",
    "print(\"\\nNotice: ID has high Information Gain (splits perfectly!)\")\n",
    "print(\"But low Gain Ratio (penalty for many unique values).\")\n",
    "print(\"This shows why C4.5 uses Gain Ratio instead of Information Gain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Handling Continuous Features\n",
    "\n",
    "### Background\n",
    "\n",
    "ID3 was designed for categorical features. C4.5 extends it to handle continuous features by:\n",
    "\n",
    "1. **Discretization**: Convert continuous values into bins/categories\n",
    "2. **Binary splits**: Find best threshold to split into two groups\n",
    "\n",
    "**Methods:**\n",
    "- Equal-width binning\n",
    "- Equal-frequency binning\n",
    "- Entropy-based binning\n",
    "\n",
    "### Exercise 4.1: Discretize Continuous Features\n",
    "\n",
    "**Task:** Implement and compare discretization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with continuous features\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_cont, y_cont = make_classification(\n",
    "    n_samples=100, n_features=2, n_informative=2,\n",
    "    n_redundant=0, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Visualize continuous data\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_cont[:, 0], X_cont[:, 1], c=y_cont, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Original Continuous Data')\n",
    "plt.colorbar(label='Class')\n",
    "\n",
    "# Equal-width binning\n",
    "discretizer_width = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
    "X_width = discretizer_width.fit_transform(X_cont)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_width[:, 0], X_width[:, 1], c=y_cont, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Feature 1 (binned)')\n",
    "plt.ylabel('Feature 2 (binned)')\n",
    "plt.title('Equal-Width Binning (3 bins)')\n",
    "plt.colorbar(label='Class')\n",
    "\n",
    "# Equal-frequency binning\n",
    "discretizer_freq = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\n",
    "X_freq = discretizer_freq.fit_transform(X_cont)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_freq[:, 0], X_freq[:, 1], c=y_cont, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Feature 1 (binned)')\n",
    "plt.ylabel('Feature 2 (binned)')\n",
    "plt.title('Equal-Frequency Binning (3 bins)')\n",
    "plt.colorbar(label='Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Discretization strategies compared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to categorical and apply ID3\n",
    "X_discrete = pd.DataFrame(\n",
    "    X_width.astype(int),\n",
    "    columns=['Feature1_bin', 'Feature2_bin']\n",
    ")\n",
    "\n",
    "# Train ID3 on discretized data\n",
    "id3_discrete = ID3Classifier(max_depth=3)\n",
    "id3_discrete.fit(X_discrete, y_cont)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_discrete = id3_discrete.predict(X_discrete)\n",
    "accuracy_discrete = accuracy_score(y_cont, y_pred_discrete)\n",
    "\n",
    "print(f\"ID3 on Discretized Data:\")\n",
    "print(f\"Accuracy: {accuracy_discrete:.4f}\")\n",
    "\n",
    "print(\"\\nDecision Tree:\")\n",
    "id3_discrete.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: CART Algorithm\n",
    "\n",
    "### Background\n",
    "\n",
    "**CART (Classification and Regression Trees)** differs from ID3/C4.5:\n",
    "\n",
    "| Feature | ID3/C4.5 | CART |\n",
    "|---------|----------|------|\n",
    "| Split criterion | Information Gain / Gain Ratio | Gini Impurity |\n",
    "| Splits | Multi-way | Binary only |\n",
    "| Continuous features | Discretization (C4.5) | Native support |\n",
    "| Pruning | Post-pruning | Cost-complexity |\n",
    "| Tasks | Classification | Classification & Regression |\n",
    "\n",
    "**Gini Impurity:**\n",
    "$$\\text{Gini}(S) = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
    "\n",
    "### Exercise 5.1: Apply CART to Wine Dataset\n",
    "\n",
    "**Task:** Use sklearn's DecisionTreeClassifier (CART) on the Wine dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Wine dataset\n",
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "print(\"Wine Dataset:\")\n",
    "print(f\"Shape: {X_wine.shape}\")\n",
    "print(f\"Features: {wine.feature_names}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(pd.Series(y_wine).value_counts().sort_index())\n",
    "\n",
    "# Split data\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
    "    X_wine, y_wine, test_size=0.3, random_state=42, stratify=y_wine\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train_wine.shape}\")\n",
    "print(f\"Test set: {X_test_wine.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CART with Gini criterion\n",
    "cart_gini = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    random_state=42\n",
    ")\n",
    "cart_gini.fit(X_train_wine, y_train_wine)\n",
    "\n",
    "# Train CART with Entropy criterion\n",
    "cart_entropy = DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    random_state=42\n",
    ")\n",
    "cart_entropy.fit(X_train_wine, y_train_wine)\n",
    "\n",
    "# Evaluate both\n",
    "y_pred_gini = cart_gini.predict(X_test_wine)\n",
    "y_pred_entropy = cart_entropy.predict(X_test_wine)\n",
    "\n",
    "acc_gini = accuracy_score(y_test_wine, y_pred_gini)\n",
    "acc_entropy = accuracy_score(y_test_wine, y_pred_entropy)\n",
    "\n",
    "print(\"CART Performance on Wine Dataset:\\n\")\n",
    "print(f\"Gini Criterion:    {acc_gini:.4f}\")\n",
    "print(f\"Entropy Criterion: {acc_entropy:.4f}\")\n",
    "\n",
    "# Detailed metrics for Gini\n",
    "print(\"\\nDetailed Classification Report (Gini):\")\n",
    "print(classification_report(y_test_wine, y_pred_gini, target_names=wine.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    cart_gini,\n",
    "    feature_names=wine.feature_names,\n",
    "    class_names=wine.target_names,\n",
    "    filled=True,\n",
    "    fontsize=10,\n",
    "    rounded=True\n",
    ")\n",
    "plt.title('CART Decision Tree on Wine Dataset (Gini)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = cart_gini.feature_importances_\n",
    "indices = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importance)), feature_importance[indices])\n",
    "plt.xticks(range(len(feature_importance)), [wine.feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance in CART (Wine Dataset)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "for i in range(5):\n",
    "    idx = indices[i]\n",
    "    print(f\"{i+1}. {wine.feature_names[idx]}: {feature_importance[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Tune CART Hyperparameters\n",
    "\n",
    "**Task:** Experiment with different hyperparameters to find the best tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: Try different hyperparameters\n",
    "# Test different max_depth values\n",
    "\n",
    "depths = [2, 3, 5, 7, 10, None]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    # Your code here\n",
    "    cart = DecisionTreeClassifier(\n",
    "        max_depth=depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    cart.fit(X_train_wine, y_train_wine)\n",
    "    \n",
    "    train_acc = \n",
    "    test_acc = \n",
    "    \n",
    "    train_scores.append(train_acc)\n",
    "    test_scores.append(test_acc)\n",
    "\n",
    "# Plot results\n",
    "depth_labels = [str(d) if d is not None else 'None' for d in depths]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depth_labels, train_scores, 'o-', label='Train', linewidth=2, markersize=8)\n",
    "plt.plot(depth_labels, test_scores, 's-', label='Test', linewidth=2, markersize=8)\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CART Performance vs Tree Depth')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0.7, 1.05])\n",
    "plt.show()\n",
    "\n",
    "best_depth_idx = np.argmax(test_scores)\n",
    "print(f\"\\nBest max_depth: {depths[best_depth_idx]}\")\n",
    "print(f\"Test accuracy: {test_scores[best_depth_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Algorithm Comparison\n",
    "\n",
    "### Exercise 6.1: Compare ID3 and CART\n",
    "\n",
    "**Task:** Compare ID3 (after discretization) with CART on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Wine data for ID3 (discretize features)\n",
    "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')\n",
    "X_wine_discrete = discretizer.fit_transform(X_wine)\n",
    "\n",
    "# Split discretized data\n",
    "X_train_disc, X_test_disc, y_train_disc, y_test_disc = train_test_split(\n",
    "    X_wine_discrete, y_wine, test_size=0.3, random_state=42, stratify=y_wine\n",
    ")\n",
    "\n",
    "# Train ID3 on discretized data\n",
    "id3_wine = ID3Classifier(max_depth=5, min_samples_split=5)\n",
    "id3_wine.fit(X_train_disc.astype(int), y_train_disc, feature_names=wine.feature_names)\n",
    "\n",
    "# Predict\n",
    "y_pred_id3 = id3_wine.predict(X_test_disc.astype(int))\n",
    "acc_id3 = accuracy_score(y_test_disc, y_pred_id3)\n",
    "\n",
    "# Train CART on original continuous data\n",
    "cart_wine = DecisionTreeClassifier(max_depth=5, min_samples_split=5, random_state=42)\n",
    "cart_wine.fit(X_train_wine, y_train_wine)\n",
    "y_pred_cart = cart_wine.predict(X_test_wine)\n",
    "acc_cart = accuracy_score(y_test_wine, y_pred_cart)\n",
    "\n",
    "# Compare\n",
    "print(\"Algorithm Comparison on Wine Dataset:\\n\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Algorithm':<30} {'Accuracy':<15} {'Notes':<15}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'ID3 (discretized)':<30} {acc_id3:<15.4f} {'5 bins/feature'}\")\n",
    "print(f\"{'CART (Gini)':<30} {acc_cart:<15.4f} {'Native continuous'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"1. ID3 requires discretization for continuous features\")\n",
    "print(\"2. CART handles continuous features natively\")\n",
    "print(\"3. ID3 uses multi-way splits, CART uses binary splits\")\n",
    "print(\"4. ID3 uses Information Gain, CART uses Gini/Entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Handling Missing Values\n",
    "\n",
    "### Background\n",
    "\n",
    "Real-world data often has missing values. Decision trees can handle them through:\n",
    "\n",
    "1. **Surrogate splits**: Find backup splits that mimic the primary split\n",
    "2. **Probabilistic splits**: Send sample down multiple branches with weights\n",
    "3. **Imputation**: Fill missing values before building tree\n",
    "\n",
    "### Exercise 7.1: Handle Missing Values\n",
    "\n",
    "**Task:** Compare strategies for handling missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Wine dataset with missing values\n",
    "X_wine_missing = X_wine.copy()\n",
    "y_wine_missing = y_wine.copy()\n",
    "\n",
    "# Randomly set 10% of values to NaN\n",
    "np.random.seed(42)\n",
    "missing_mask = np.random.random(X_wine_missing.shape) < 0.1\n",
    "X_wine_missing[missing_mask] = np.nan\n",
    "\n",
    "print(f\"Created dataset with missing values\")\n",
    "print(f\"Total missing values: {np.isnan(X_wine_missing).sum()}\")\n",
    "print(f\"Percentage missing: {np.isnan(X_wine_missing).sum() / X_wine_missing.size * 100:.2f}%\")\n",
    "\n",
    "# Split\n",
    "X_train_miss, X_test_miss, y_train_miss, y_test_miss = train_test_split(\n",
    "    X_wine_missing, y_wine_missing, test_size=0.3, random_state=42, stratify=y_wine_missing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Strategy 1: Mean imputation\n",
    "imputer_mean = SimpleImputer(strategy='mean')\n",
    "X_train_mean = imputer_mean.fit_transform(X_train_miss)\n",
    "X_test_mean = imputer_mean.transform(X_test_miss)\n",
    "\n",
    "cart_mean = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "cart_mean.fit(X_train_mean, y_train_miss)\n",
    "acc_mean = cart_mean.score(X_test_mean, y_test_miss)\n",
    "\n",
    "# Strategy 2: Median imputation\n",
    "imputer_median = SimpleImputer(strategy='median')\n",
    "X_train_median = imputer_median.fit_transform(X_train_miss)\n",
    "X_test_median = imputer_median.transform(X_test_miss)\n",
    "\n",
    "cart_median = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "cart_median.fit(X_train_median, y_train_miss)\n",
    "acc_median = cart_median.score(X_test_median, y_test_miss)\n",
    "\n",
    "# Strategy 3: Most frequent (for comparison)\n",
    "imputer_freq = SimpleImputer(strategy='most_frequent')\n",
    "X_train_freq = imputer_freq.fit_transform(X_train_miss)\n",
    "X_test_freq = imputer_freq.transform(X_test_miss)\n",
    "\n",
    "cart_freq = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "cart_freq.fit(X_train_freq, y_train_miss)\n",
    "acc_freq = cart_freq.score(X_test_freq, y_test_miss)\n",
    "\n",
    "# Compare\n",
    "print(\"Missing Value Handling Strategies:\\n\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Strategy':<25} {'Test Accuracy':<15}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Mean Imputation':<25} {acc_mean:<15.4f}\")\n",
    "print(f\"{'Median Imputation':<25} {acc_median:<15.4f}\")\n",
    "print(f\"{'Most Frequent Imputation':<25} {acc_freq:<15.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nNote: sklearn's DecisionTreeClassifier doesn't natively support\")\n",
    "print(\"missing values. We must impute them before training.\")\n",
    "print(\"For native missing value support, consider XGBoost or LightGBM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Real-World Application - Breast Cancer Classification\n",
    "\n",
    "### Exercise 8.1: Full Pipeline on Breast Cancer Dataset\n",
    "\n",
    "**Task:** Apply everything you've learned to a medical diagnosis problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Breast Cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer = cancer.data\n",
    "y_cancer = cancer.target\n",
    "\n",
    "print(\"Breast Cancer Dataset:\")\n",
    "print(f\"Shape: {X_cancer.shape}\")\n",
    "print(f\"Classes: {cancer.target_names}\")\n",
    "print(f\"  0 = {cancer.target_names[0]}\")\n",
    "print(f\"  1 = {cancer.target_names[1]}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(pd.Series(y_cancer).value_counts())\n",
    "print(f\"\\nFeatures (first 10):\")\n",
    "for i, name in enumerate(cancer.feature_names[:10]):\n",
    "    print(f\"  {i}: {name}\")\n",
    "print(f\"  ... ({len(cancer.feature_names)} total features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train_cancer, X_test_cancer, y_train_cancer, y_test_cancer = train_test_split(\n",
    "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train_cancer.shape}\")\n",
    "print(f\"Test set: {X_test_cancer.shape}\")\n",
    "\n",
    "# Your turn: Build and evaluate CART model\n",
    "# Try to achieve >90% accuracy\n",
    "# Experiment with hyperparameters!\n",
    "\n",
    "# Your code here\n",
    "cart_cancer = DecisionTreeClassifier(\n",
    "    criterion=,  # Try 'gini' or 'entropy'\n",
    "    max_depth=,  # Experiment with depth\n",
    "    min_samples_split=,\n",
    "    min_samples_leaf=,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cart_cancer.fit()\n",
    "\n",
    "# Predictions\n",
    "y_pred_cancer = \n",
    "\n",
    "# Evaluate\n",
    "acc_cancer = accuracy_score(y_test_cancer, y_pred_cancer)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {acc_cancer:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cancer, y_pred_cancer, target_names=cancer.target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_cancer, y_pred_cancer)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=cancer.target_names,\n",
    "            yticklabels=cancer.target_names)\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix - Breast Cancer Classification')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Interpretation:\")\n",
    "print(f\"True Negatives (malignant correctly classified): {cm[0,0]}\")\n",
    "print(f\"False Positives (malignant predicted as benign): {cm[0,1]}\")\n",
    "print(f\"False Negatives (benign predicted as malignant): {cm[1,0]}\")\n",
    "print(f\"True Positives (benign correctly classified): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for interpretation\n",
    "feature_importance_cancer = cart_cancer.feature_importances_\n",
    "indices = np.argsort(feature_importance_cancer)[::-1][:10]  # Top 10\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(range(len(indices)), feature_importance_cancer[indices])\n",
    "plt.yticks(range(len(indices)), [cancer.feature_names[i] for i in indices])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Most Important Features for Breast Cancer Classification')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for i in range(5):\n",
    "    idx = indices[i]\n",
    "    print(f\"{i+1}. {cancer.feature_names[idx]}: {feature_importance_cancer[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nMedical Interpretation:\")\n",
    "print(\"These features are most useful for distinguishing malignant from benign tumors.\")\n",
    "print(\"Doctors can focus on these characteristics during diagnosis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for robust evaluation\n",
    "cv_scores = cross_val_score(cart_cancer, X_cancer, y_cancer, cv=5)\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(f\"Fold scores: {cv_scores}\")\n",
    "print(f\"Mean accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.boxplot([cv_scores])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-Validation Accuracy Distribution')\n",
    "plt.xticks([1], ['5-Fold CV'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Implement Gini Impurity from Scratch\n",
    "\n",
    "Implement the Gini impurity calculation and compare with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \"\"\"\n",
    "    Calculate Gini impurity.\n",
    "    \n",
    "    Gini = 1 - Σ(p_i²)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Target values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Gini impurity (0 = pure, higher = more impure)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Testing Gini Impurity:\")\n",
    "print(f\"Gini([0,0,0,0]): {gini_impurity([0,0,0,0]):.4f} (should be 0.0000)\")\n",
    "print(f\"Gini([0,0,1,1]): {gini_impurity([0,0,1,1]):.4f} (should be 0.5000)\")\n",
    "print(f\"Gini([0,1,2]): {gini_impurity([0,1,2]):.4f} (should be 0.6667)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Challenge 2: Implement Pruning\n",
    "\n",
    "Add post-pruning to your ID3 implementation to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_tree(node, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Prune a decision tree using validation data.\n",
    "    \n",
    "    Strategy: Reduced Error Pruning\n",
    "    - Try replacing each internal node with a leaf\n",
    "    - Keep the change if validation accuracy improves or stays the same\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    node : ID3Node\n",
    "        Root of tree/subtree to prune\n",
    "    X_val : validation features\n",
    "    y_val : validation labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ID3Node\n",
    "        Pruned tree\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    # This is a challenging problem!\n",
    "    pass\n",
    "\n",
    "print(\"Challenge: Implement tree pruning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Challenge 3: Implement Cost-Complexity Pruning\n",
    "\n",
    "Implement CART's cost-complexity pruning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn's built-in cost-complexity pruning\n",
    "# Your task: Understand and visualize the trade-off\n",
    "\n",
    "# Get cost-complexity pruning path\n",
    "path = cart_cancer.cost_complexity_pruning_path(X_train_cancer, y_train_cancer)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "impurities = path.impurities\n",
    "\n",
    "# Train trees with different alpha values\n",
    "trees = []\n",
    "for alpha in ccp_alphas:\n",
    "    tree = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
    "    tree.fit(X_train_cancer, y_train_cancer)\n",
    "    trees.append(tree)\n",
    "\n",
    "# Evaluate\n",
    "train_scores = [tree.score(X_train_cancer, y_train_cancer) for tree in trees]\n",
    "test_scores = [tree.score(X_test_cancer, y_test_cancer) for tree in trees]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label='Train', drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='s', label='Test', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel('Alpha (complexity parameter)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy vs Alpha for Cost-Complexity Pruning')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Find best alpha\n",
    "best_idx = np.argmax(test_scores)\n",
    "print(f\"\\nBest alpha: {ccp_alphas[best_idx]:.6f}\")\n",
    "print(f\"Test accuracy: {test_scores[best_idx]:.4f}\")\n",
    "print(f\"Tree depth: {trees[best_idx].get_depth()}\")\n",
    "print(f\"Number of leaves: {trees[best_idx].get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Challenge 4: Multi-way Split vs Binary Split Comparison\n",
    "\n",
    "Compare multi-way splits (ID3) with binary splits (CART) on a categorical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with a multi-valued categorical feature\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "\n",
    "# Feature with 5 categories\n",
    "categories = ['A', 'B', 'C', 'D', 'E']\n",
    "X_multi = np.random.choice(categories, size=(n_samples, 1))\n",
    "\n",
    "# Target correlated with categories\n",
    "# A, B → Class 0; C, D, E → Class 1\n",
    "y_multi = np.array([0 if x[0] in ['A', 'B'] else 1 for x in X_multi])\n",
    "y_multi = (y_multi + np.random.binomial(1, 0.1, n_samples)) % 2  # Add 10% noise\n",
    "\n",
    "# Convert for ID3\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X_multi_encoded = le.fit_transform(X_multi.ravel()).reshape(-1, 1)\n",
    "\n",
    "# ID3: Will create 5-way split\n",
    "id3_multi = ID3Classifier(max_depth=3)\n",
    "id3_multi.fit(X_multi_encoded, y_multi, feature_names=['Category'])\n",
    "\n",
    "print(\"ID3 Tree (Multi-way split):\")\n",
    "print(\"=\" * 40)\n",
    "id3_multi.print_tree()\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# CART: Will create binary splits\n",
    "cart_multi = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "cart_multi.fit(X_multi_encoded, y_multi)\n",
    "\n",
    "print(\"\\nCART Tree (Binary splits):\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(cart_multi, feature_names=['Category'], class_names=['Class 0', 'Class 1'],\n",
    "          filled=True, rounded=True)\n",
    "plt.title('CART with Binary Splits')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"- ID3 splits into 5 branches (one per category value)\")\n",
    "print(\"- CART creates binary splits (Category <= threshold)\")\n",
    "print(\"- For categorical data, ID3's multi-way splits can be more intuitive\")\n",
    "print(\"- CART may need multiple binary splits to achieve same result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **When should you use ID3 vs CART?**\n",
    "   - Consider data types, interpretability, and computational efficiency\n",
    "\n",
    "2. **Why does C4.5 use Gain Ratio instead of Information Gain?**\n",
    "   - Think about attributes with many unique values (like IDs)\n",
    "\n",
    "3. **What are the advantages of binary splits (CART) over multi-way splits (ID3)?**\n",
    "   - Consider tree balance, implementation simplicity\n",
    "\n",
    "4. **How does discretization affect ID3's performance?**\n",
    "   - What information is lost? What are the trade-offs?\n",
    "\n",
    "5. **When would Gini impurity give different results than Entropy?**\n",
    "   - They're similar but not identical - when does the difference matter?\n",
    "\n",
    "6. **How would you handle a dataset with both categorical and continuous features?**\n",
    "   - What preprocessing is needed for ID3 vs CART?\n",
    "\n",
    "7. **Why is pruning important?**\n",
    "   - What happens without pruning on noisy data?\n",
    "\n",
    "8. **In medical diagnosis (like breast cancer), which type of error is worse?**\n",
    "   - False positive (predict malignant when benign)?\n",
    "   - False negative (predict benign when malignant)?\n",
    "   - How would you adjust your model?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "✓ **ID3 Algorithm**: Information Gain-based tree construction for categorical data  \n",
    "✓ **C4.5 Improvements**: Gain Ratio to handle multi-valued attributes  \n",
    "✓ **CART Algorithm**: Gini-based binary splitting with native continuous feature support  \n",
    "✓ **Discretization**: Converting continuous features for categorical algorithms  \n",
    "✓ **Missing Value Handling**: Imputation strategies and their impact  \n",
    "✓ **Feature Importance**: Understanding which features drive decisions  \n",
    "✓ **Real-World Application**: Medical diagnosis with breast cancer dataset  \n",
    "✓ **Algorithm Comparison**: Trade-offs between ID3, C4.5, and CART  \n",
    "✓ **Pruning Concepts**: Reducing overfitting with cost-complexity pruning  \n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "| Algorithm | Criterion | Splits | Continuous Features | Best For |\n",
    "|-----------|-----------|--------|---------------------|----------|\n",
    "| ID3 | Information Gain | Multi-way | No (discretize) | Categorical data, interpretability |\n",
    "| C4.5 | Gain Ratio | Multi-way | Yes (discretization) | Avoiding bias, mixed data |\n",
    "| CART | Gini/Entropy | Binary | Yes (native) | General purpose, efficiency |\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Review the [Decision Trees lesson](https://jumpingsphinx.github.io/module3-trees/01-decision-trees/)\n",
    "- Review the [Tree Algorithms lesson](https://jumpingsphinx.github.io/module3-trees/02-tree-algorithms/)\n",
    "- Complete Exercise 3 on Ensemble Methods (Random Forests, Boosting)\n",
    "- Try implementing a regression tree (CART for regression)\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

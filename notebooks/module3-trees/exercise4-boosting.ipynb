{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Module 3 - Exercise 4: Boosting Algorithms\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module3-trees/exercise4-boosting.ipynb)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "\n",
    "- Understand the core concepts of boosting and sample reweighting\n",
    "- Implement AdaBoost from scratch with decision stumps\n",
    "- Apply AdaBoost using scikit-learn to classification problems\n",
    "- Understand gradient boosting through residual fitting\n",
    "- Implement gradient boosting for regression and classification\n",
    "- Analyze the effect of learning rate on boosting performance\n",
    "- Compare boosting with bagging approaches\n",
    "- Apply boosting to real-world datasets\n",
    "- Handle imbalanced classification problems with boosting\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of decision trees\n",
    "- Familiarity with ensemble methods\n",
    "- Basic machine learning evaluation metrics\n",
    "- NumPy and matplotlib proficiency\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this cell first to import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_classification, load_digits, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: AdaBoost Conceptual Understanding\n",
    "\n",
    "### Background\n",
    "\n",
    "AdaBoost (Adaptive Boosting) works by:\n",
    "1. Training a weak learner on the data\n",
    "2. Increasing weights of misclassified samples\n",
    "3. Training the next learner to focus on hard examples\n",
    "4. Combining all learners with weighted voting\n",
    "\n",
    "### Exercise 1.1: Visualize Sample Reweighting\n",
    "\n",
    "**Task:** Create a visualization showing how AdaBoost adjusts sample weights over iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple 2D dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "\n",
    "# Create two classes\n",
    "X_class0 = np.random.randn(n_samples // 2, 2) + np.array([2, 2])\n",
    "X_class1 = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])\n",
    "X_simple = np.vstack([X_class0, X_class1])\n",
    "y_simple = np.hstack([np.zeros(n_samples // 2), np.ones(n_samples // 2)])\n",
    "\n",
    "# Initialize uniform weights\n",
    "sample_weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "# Simulate 3 boosting iterations\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Initial state\n",
    "axes[0].scatter(X_simple[y_simple == 0, 0], X_simple[y_simple == 0, 1], \n",
    "                s=sample_weights[:50] * 10000, alpha=0.6, label='Class 0', c='blue')\n",
    "axes[0].scatter(X_simple[y_simple == 1, 0], X_simple[y_simple == 1, 1], \n",
    "                s=sample_weights[50:] * 10000, alpha=0.6, label='Class 1', c='red')\n",
    "axes[0].set_title('Iteration 0: Equal Weights')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Simulate boosting iterations\n",
    "for iteration in range(1, 4):\n",
    "    # Train a simple model\n",
    "    clf = DecisionTreeClassifier(max_depth=1, random_state=iteration)\n",
    "    clf.fit(X_simple, y_simple, sample_weight=sample_weights)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = clf.predict(X_simple)\n",
    "    \n",
    "    # Calculate error\n",
    "    incorrect = (predictions != y_simple)\n",
    "    error = np.sum(sample_weights * incorrect) / np.sum(sample_weights)\n",
    "    \n",
    "    # Calculate alpha (model weight)\n",
    "    alpha = 0.5 * np.log((1 - error) / max(error, 1e-10))\n",
    "    \n",
    "    # Update sample weights\n",
    "    sample_weights *= np.exp(alpha * incorrect * (2 * (predictions != y_simple) - 1))\n",
    "    sample_weights /= np.sum(sample_weights)  # Normalize\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[iteration]\n",
    "    ax.scatter(X_simple[y_simple == 0, 0], X_simple[y_simple == 0, 1], \n",
    "               s=sample_weights[:50] * 10000, alpha=0.6, label='Class 0', c='blue')\n",
    "    ax.scatter(X_simple[y_simple == 1, 0], X_simple[y_simple == 1, 1], \n",
    "               s=sample_weights[50:] * 10000, alpha=0.6, label='Class 1', c='red')\n",
    "    \n",
    "    # Highlight misclassified samples\n",
    "    if np.any(incorrect):\n",
    "        ax.scatter(X_simple[incorrect, 0], X_simple[incorrect, 1], \n",
    "                   s=200, facecolors='none', edgecolors='black', linewidths=2)\n",
    "    \n",
    "    ax.set_title(f'Iteration {iteration}: Error={error:.3f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insight: Larger circles = higher sample weights\")\n",
    "print(\"Black circles = misclassified samples that get higher weights\")\n",
    "print(\"\\nAdaBoost focuses on hard-to-classify examples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: AdaBoost from Scratch\n",
    "\n",
    "### Background\n",
    "\n",
    "AdaBoost algorithm:\n",
    "\n",
    "1. Initialize weights: $w_i = 1/N$ for all samples\n",
    "2. For each iteration $t$:\n",
    "   - Train weak learner $h_t$ with weights $w$\n",
    "   - Calculate weighted error: $\\epsilon_t = \\sum_{i: h_t(x_i) \\neq y_i} w_i$\n",
    "   - Calculate learner weight: $\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$\n",
    "   - Update sample weights: $w_i \\leftarrow w_i \\cdot \\exp(-\\alpha_t y_i h_t(x_i))$\n",
    "   - Normalize weights\n",
    "3. Final prediction: $H(x) = \\text{sign}\\left(\\sum_{t=1}^T \\alpha_t h_t(x)\\right)$\n",
    "\n",
    "### Exercise 2.1: Implement AdaBoost with Decision Stumps\n",
    "\n",
    "**Task:** Complete the AdaBoost implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostFromScratch:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        \"\"\"\n",
    "        AdaBoost classifier using decision stumps.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_estimators : int\n",
    "            Number of weak learners to train\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.weak_learners = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit AdaBoost model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Training features (n_samples, n_features)\n",
    "        y : np.ndarray\n",
    "            Target values (n_samples,) with values in {-1, 1}\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize sample weights uniformly\n",
    "        sample_weights = np.ones(n_samples) / n_samples\n",
    "        \n",
    "        for t in range(self.n_estimators):\n",
    "            # Train weak learner (decision stump)\n",
    "            weak_learner = DecisionTreeClassifier(max_depth=1, random_state=t)\n",
    "            weak_learner.fit(X, y, sample_weight=sample_weights)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = weak_learner.predict(X)\n",
    "            \n",
    "            # Calculate weighted error\n",
    "            # Your code here\n",
    "            incorrect = (predictions != y)\n",
    "            error = np.sum(sample_weights[incorrect]) / np.sum(sample_weights)\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            error = np.clip(error, 1e-10, 1 - 1e-10)\n",
    "            \n",
    "            # Calculate alpha (learner weight)\n",
    "            # Your code here\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            \n",
    "            # Update sample weights\n",
    "            # Your code here\n",
    "            # Hint: w_i = w_i * exp(-alpha * y_i * h(x_i))\n",
    "            sample_weights *= np.exp(-alpha * y * predictions)\n",
    "            \n",
    "            # Normalize weights\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "            \n",
    "            # Store learner and its weight\n",
    "            self.weak_learners.append(weak_learner)\n",
    "            self.alphas.append(alpha)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : np.ndarray\n",
    "            Features (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Predictions (n_samples,)\n",
    "        \"\"\"\n",
    "        # Aggregate weighted predictions\n",
    "        # Your code here\n",
    "        # Hint: Sum over all alpha_t * h_t(x) and take sign\n",
    "        weak_predictions = np.array([learner.predict(X) for learner in self.weak_learners])\n",
    "        weighted_sum = np.dot(self.alphas, weak_predictions)\n",
    "        \n",
    "        return np.sign(weighted_sum)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "\n",
    "print(\"AdaBoost class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Test Your Implementation\n",
    "\n",
    "**Task:** Test your AdaBoost implementation on a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "X_test_boost, y_test_boost = make_classification(\n",
    "    n_samples=500, n_features=2, n_redundant=0, n_informative=2,\n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "# Convert labels to {-1, 1}\n",
    "y_test_boost = 2 * y_test_boost - 1\n",
    "\n",
    "# Split data\n",
    "X_train_boost, X_test_boost_split, y_train_boost, y_test_boost_split = train_test_split(\n",
    "    X_test_boost, y_test_boost, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train your AdaBoost\n",
    "ada_scratch = AdaBoostFromScratch(n_estimators=50)\n",
    "ada_scratch.fit(X_train_boost, y_train_boost)\n",
    "\n",
    "# Evaluate\n",
    "train_acc = ada_scratch.score(X_train_boost, y_train_boost)\n",
    "test_acc = ada_scratch.score(X_test_boost_split, y_test_boost_split)\n",
    "\n",
    "print(\"Your AdaBoost Implementation:\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc:.4f}\")\n",
    "\n",
    "# Compare with single decision stump\n",
    "stump = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "stump.fit(X_train_boost, y_train_boost)\n",
    "stump_acc = stump.score(X_test_boost_split, y_test_boost_split)\n",
    "\n",
    "print(f\"\\nSingle Decision Stump: {stump_acc:.4f}\")\n",
    "print(f\"Improvement: {test_acc - stump_acc:.4f}\")\n",
    "\n",
    "assert test_acc > stump_acc, \"AdaBoost should outperform a single stump!\"\n",
    "print(\"\\n✓ AdaBoost implementation works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Visualize Decision Boundary Evolution\n",
    "\n",
    "**Task:** Show how the decision boundary improves with more weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model, title):\n",
    "    \"\"\"Plot decision boundary for 2D data.\"\"\"\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', marker='o', edgecolors='k', label='Class 1')\n",
    "    plt.scatter(X[y == -1, 0], X[y == -1, 1], c='blue', marker='s', edgecolors='k', label='Class -1')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Train models with different numbers of estimators\n",
    "n_estimators_list = [1, 5, 10, 50]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, n_est in enumerate(n_estimators_list):\n",
    "    plt.sca(axes[idx])\n",
    "    model = AdaBoostFromScratch(n_estimators=n_est)\n",
    "    model.fit(X_train_boost, y_train_boost)\n",
    "    acc = model.score(X_test_boost_split, y_test_boost_split)\n",
    "    plot_decision_boundary(X_test_boost_split, y_test_boost_split, model, \n",
    "                          f'{n_est} Weak Learners (Acc: {acc:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the decision boundary becomes more refined with more weak learners!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: AdaBoost with Scikit-learn\n",
    "\n",
    "### Exercise 3.1: Apply to Make_Moons Dataset\n",
    "\n",
    "**Task:** Use sklearn's AdaBoost on the make_moons dataset and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate make_moons dataset\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_moons, y_moons, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Your turn: Create and train AdaBoost classifier\n",
    "# Your code here\n",
    "ada_sklearn = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    random_state=42\n",
    ")\n",
    "ada_sklearn.fit(X_train_m, y_train_m)\n",
    "\n",
    "# Evaluate\n",
    "train_acc_m = ada_sklearn.score(X_train_m, y_train_m)\n",
    "test_acc_m = ada_sklearn.score(X_test_m, y_test_m)\n",
    "\n",
    "print(\"AdaBoost on Make_Moons:\")\n",
    "print(f\"Train Accuracy: {train_acc_m:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc_m:.4f}\")\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_decision_boundary(X_test_m, y_moons[y_moons != y_train_m], ada_sklearn, \n",
    "                       'AdaBoost on Make_Moons')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance from estimator weights\n",
    "print(\"\\nEstimator weights (first 10):\")\n",
    "print(ada_sklearn.estimator_weights_[:10])\n",
    "print(\"\\nHigher weights = more influential weak learners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Analyze Training Progression\n",
    "\n",
    "**Task:** Plot how accuracy improves with each boosting iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with varying numbers of estimators\n",
    "n_estimators_range = range(1, 101, 5)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    ada = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=n_est,\n",
    "        random_state=42\n",
    "    )\n",
    "    ada.fit(X_train_m, y_train_m)\n",
    "    train_scores.append(ada.score(X_train_m, y_train_m))\n",
    "    test_scores.append(ada.score(X_test_m, y_test_m))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_scores, label='Train Accuracy', marker='o')\n",
    "plt.plot(n_estimators_range, test_scores, label='Test Accuracy', marker='s')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('AdaBoost Performance vs Number of Weak Learners')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best Test Accuracy: {max(test_scores):.4f} at {n_estimators_range[np.argmax(test_scores)]} estimators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Gradient Boosting Intuition\n",
    "\n",
    "### Background\n",
    "\n",
    "Gradient Boosting works differently than AdaBoost:\n",
    "- Instead of reweighting samples, it fits new models to **residuals** (errors)\n",
    "- Each new tree tries to correct the mistakes of the previous ensemble\n",
    "- Predictions: $F_t(x) = F_{t-1}(x) + \\eta \\cdot h_t(x)$\n",
    "\n",
    "### Exercise 4.1: Residual Fitting Demonstration\n",
    "\n",
    "**Task:** Visualize how gradient boosting sequentially fits residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-linear data\n",
    "np.random.seed(42)\n",
    "X_residual = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_residual = np.sin(X_residual).ravel() + np.random.randn(100) * 0.2\n",
    "\n",
    "# Simulate gradient boosting manually\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Initialize prediction\n",
    "F = np.zeros(len(y_residual))\n",
    "learning_rate = 0.3\n",
    "\n",
    "for i in range(6):\n",
    "    # Calculate residuals\n",
    "    residuals = y_residual - F\n",
    "    \n",
    "    # Fit a tree to residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=2, random_state=i)\n",
    "    tree.fit(X_residual, residuals)\n",
    "    \n",
    "    # Update predictions\n",
    "    h = tree.predict(X_residual)\n",
    "    F += learning_rate * h\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i]\n",
    "    ax.scatter(X_residual, y_residual, alpha=0.3, label='True data', s=20)\n",
    "    ax.plot(X_residual, F, 'r-', linewidth=2, label='Current prediction')\n",
    "    \n",
    "    # Show residuals as vertical lines\n",
    "    if i < 5:\n",
    "        for j in range(0, len(X_residual), 10):\n",
    "            ax.plot([X_residual[j], X_residual[j]], [F[j], y_residual[j]], \n",
    "                   'g--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    mse = mean_squared_error(y_residual, F)\n",
    "    ax.set_title(f'Iteration {i + 1}: MSE={mse:.4f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green dashed lines = Residuals that the next tree will fit\")\n",
    "print(\"Red line = Current ensemble prediction\")\n",
    "print(\"\\nNotice how residuals shrink and prediction improves!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Gradient Boosting Regression\n",
    "\n",
    "### Exercise 5.1: Apply to Synthetic Data\n",
    "\n",
    "**Task:** Use GradientBoostingRegressor on synthetic data and visualize progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex non-linear data\n",
    "np.random.seed(42)\n",
    "X_gb_reg = np.linspace(0, 10, 200).reshape(-1, 1)\n",
    "y_gb_reg = np.sin(X_gb_reg).ravel() + 0.5 * np.cos(2 * X_gb_reg).ravel() + np.random.randn(200) * 0.3\n",
    "\n",
    "# Split data\n",
    "X_train_gb, X_test_gb, y_train_gb, y_test_gb = train_test_split(\n",
    "    X_gb_reg, y_gb_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Your turn: Train GradientBoostingRegressor\n",
    "# Your code here\n",
    "gb_reg = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb_reg.fit(X_train_gb, y_train_gb)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_gb = gb_reg.predict(X_train_gb)\n",
    "y_test_pred_gb = gb_reg.predict(X_test_gb)\n",
    "\n",
    "# Evaluate\n",
    "train_mse = mean_squared_error(y_train_gb, y_train_pred_gb)\n",
    "test_mse = mean_squared_error(y_test_gb, y_test_pred_gb)\n",
    "test_r2 = r2_score(y_test_gb, y_test_pred_gb)\n",
    "\n",
    "print(\"Gradient Boosting Regression:\")\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE:  {test_mse:.4f}\")\n",
    "print(f\"Test R²:   {test_r2:.4f}\")\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X_train_gb, y_train_gb, alpha=0.4, label='Training data', s=30)\n",
    "plt.scatter(X_test_gb, y_test_gb, alpha=0.6, c='red', label='Test data', s=40)\n",
    "\n",
    "# Plot smooth prediction curve\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
    "y_plot = gb_reg.predict(X_plot)\n",
    "plt.plot(X_plot, y_plot, 'g-', linewidth=2, label='GB Prediction')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Boosting Regression')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Visualize Staged Predictions\n",
    "\n",
    "**Task:** Show how predictions evolve as more trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train new model to track staged predictions\n",
    "gb_staged = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42\n",
    ")\n",
    "gb_staged.fit(X_train_gb, y_train_gb)\n",
    "\n",
    "# Get staged predictions\n",
    "X_plot = np.linspace(0, 10, 300).reshape(-1, 1)\n",
    "stages_to_plot = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Get all staged predictions\n",
    "all_predictions = list(gb_staged.staged_predict(X_plot))\n",
    "\n",
    "for idx, stage in enumerate(stages_to_plot):\n",
    "    ax = axes[idx]\n",
    "    ax.scatter(X_train_gb, y_train_gb, alpha=0.3, s=20)\n",
    "    \n",
    "    # Get prediction at this stage\n",
    "    y_pred_stage = all_predictions[stage - 1]\n",
    "    ax.plot(X_plot, y_pred_stage, 'r-', linewidth=2)\n",
    "    \n",
    "    # Calculate MSE at this stage\n",
    "    test_pred_stage = list(gb_staged.staged_predict(X_test_gb))[stage - 1]\n",
    "    mse_stage = mean_squared_error(y_test_gb, test_pred_stage)\n",
    "    \n",
    "    ax.set_title(f'After {stage} trees (Test MSE: {mse_stage:.4f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Watch how the model gradually learns the pattern!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Gradient Boosting Classification\n",
    "\n",
    "### Exercise 6.1: Apply to Digits Dataset\n",
    "\n",
    "**Task:** Use gradient boosting for multi-class classification on the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(\"Digits Dataset:\")\n",
    "print(f\"Shape: {X_digits.shape}\")\n",
    "print(f\"Classes: {np.unique(y_digits)}\")\n",
    "print(f\"Number of samples: {len(X_digits)}\")\n",
    "\n",
    "# Split data\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
    "    X_digits, y_digits, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Your turn: Train GradientBoostingClassifier\n",
    "# Your code here\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "gb_clf.fit(X_train_d, y_train_d)\n",
    "\n",
    "# Predictions\n",
    "y_pred_d = gb_clf.predict(X_test_d)\n",
    "\n",
    "# Evaluate\n",
    "train_acc_d = gb_clf.score(X_train_d, y_train_d)\n",
    "test_acc_d = gb_clf.score(X_test_d, y_test_d)\n",
    "\n",
    "print(\"\\nGradient Boosting on Digits:\")\n",
    "print(f\"Train Accuracy: {train_acc_d:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc_d:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_d, y_pred_d))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_d, y_pred_d)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Gradient Boosting on Digits')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, tick_marks)\n",
    "plt.yticks(tick_marks, tick_marks)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Feature Importance Analysis\n",
    "\n",
    "**Task:** Visualize which pixels are most important for digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "feature_importance = gb_clf.feature_importances_\n",
    "\n",
    "# Reshape to 8x8 image\n",
    "importance_image = feature_importance.reshape(8, 8)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Feature importance heatmap\n",
    "axes[0].imshow(importance_image, cmap='hot', interpolation='nearest')\n",
    "axes[0].set_title('Feature Importance Heatmap')\n",
    "axes[0].set_xlabel('Pixel Column')\n",
    "axes[0].set_ylabel('Pixel Row')\n",
    "plt.colorbar(axes[0].imshow(importance_image, cmap='hot'), ax=axes[0])\n",
    "\n",
    "# Example digit for reference\n",
    "axes[1].imshow(digits.images[0], cmap='gray')\n",
    "axes[1].set_title('Example Digit (for reference)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top 10 most important pixels\n",
    "top_10_pixels = np.argsort(feature_importance)[-10:][::-1]\n",
    "print(\"\\nTop 10 Most Important Pixels:\")\n",
    "for i, pixel_idx in enumerate(top_10_pixels, 1):\n",
    "    row = pixel_idx // 8\n",
    "    col = pixel_idx % 8\n",
    "    print(f\"{i}. Pixel ({row}, {col}): Importance = {feature_importance[pixel_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Learning Rate Effects\n",
    "\n",
    "### Exercise 7.1: Compare Different Learning Rates\n",
    "\n",
    "**Task:** Analyze how learning rate affects model performance and training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(learning_rates)))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for lr, color in zip(learning_rates, colors):\n",
    "    # Train model\n",
    "    gb_lr = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_lr.fit(X_train_gb, y_train_gb)\n",
    "    \n",
    "    # Calculate test MSE at each stage\n",
    "    test_scores = []\n",
    "    for pred in gb_lr.staged_predict(X_test_gb):\n",
    "        mse = mean_squared_error(y_test_gb, pred)\n",
    "        test_scores.append(mse)\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(range(1, 101), test_scores, label=f'LR = {lr}', color=color, linewidth=2)\n",
    "\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.title('Effect of Learning Rate on Gradient Boosting')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"- Lower learning rate: Slower learning, more trees needed, less overfitting\")\n",
    "print(\"- Higher learning rate: Faster learning, fewer trees needed, more overfitting risk\")\n",
    "print(\"- Trade-off: learning_rate × n_estimators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Optimal Learning Rate Analysis\n",
    "\n",
    "**Task:** Find the best combination of learning rate and number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid of hyperparameters\n",
    "learning_rates_grid = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "n_estimators_grid = [50, 100, 150, 200]\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates_grid:\n",
    "    for n_est in n_estimators_grid:\n",
    "        gb = GradientBoostingRegressor(\n",
    "            n_estimators=n_est,\n",
    "            learning_rate=lr,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        gb.fit(X_train_gb, y_train_gb)\n",
    "        test_mse = mean_squared_error(y_test_gb, gb.predict(X_test_gb))\n",
    "        results.append((lr, n_est, test_mse))\n",
    "\n",
    "# Convert to array for plotting\n",
    "results = np.array(results)\n",
    "\n",
    "# Create heatmap\n",
    "pivot_table = np.zeros((len(learning_rates_grid), len(n_estimators_grid)))\n",
    "for i, lr in enumerate(learning_rates_grid):\n",
    "    for j, n_est in enumerate(n_estimators_grid):\n",
    "        mask = (results[:, 0] == lr) & (results[:, 1] == n_est)\n",
    "        pivot_table[i, j] = results[mask, 2][0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pivot_table, cmap='RdYlGn_r', aspect='auto')\n",
    "plt.colorbar(label='Test MSE')\n",
    "plt.xticks(range(len(n_estimators_grid)), n_estimators_grid)\n",
    "plt.yticks(range(len(learning_rates_grid)), learning_rates_grid)\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Hyperparameter Grid Search - Test MSE')\n",
    "\n",
    "# Annotate cells with values\n",
    "for i in range(len(learning_rates_grid)):\n",
    "    for j in range(len(n_estimators_grid)):\n",
    "        plt.text(j, i, f'{pivot_table[i, j]:.3f}', \n",
    "                ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best configuration\n",
    "best_idx = np.argmin(results[:, 2])\n",
    "best_lr, best_n_est, best_mse = results[best_idx]\n",
    "print(f\"\\nBest Configuration:\")\n",
    "print(f\"Learning Rate: {best_lr}\")\n",
    "print(f\"N Estimators: {int(best_n_est)}\")\n",
    "print(f\"Test MSE: {best_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Boosting vs Bagging Comparison\n",
    "\n",
    "### Exercise 8.1: Side-by-Side Comparison\n",
    "\n",
    "**Task:** Compare AdaBoost, Gradient Boosting, and Random Forest on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison dataset\n",
    "X_comp, y_comp = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_comp, y_comp, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train multiple models\n",
    "models = {\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Single Tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results_comp = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train\n",
    "    model.fit(X_train_c, y_train_c)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = model.score(X_train_c, y_train_c)\n",
    "    test_acc = model.score(X_test_c, y_test_c)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train_c, y_train_c, cv=5)\n",
    "    \n",
    "    results_comp[name] = {\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Model':<20} {'Train Acc':<12} {'Test Acc':<12} {'CV Mean±Std'}\")\n",
    "print(\"=\" * 70)\n",
    "for name, scores in results_comp.items():\n",
    "    print(f\"{name:<20} {scores['train_acc']:<12.4f} {scores['test_acc']:<12.4f} \"\n",
    "          f\"{scores['cv_mean']:.4f}±{scores['cv_std']:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "model_names = list(results_comp.keys())\n",
    "train_accs = [results_comp[m]['train_acc'] for m in model_names]\n",
    "test_accs = [results_comp[m]['test_acc'] for m in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, train_accs, width, label='Train', alpha=0.8)\n",
    "axes[0].bar(x + width/2, test_accs, width, label='Test', alpha=0.8)\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Train vs Test Accuracy')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting analysis\n",
    "overfit = np.array(train_accs) - np.array(test_accs)\n",
    "colors = ['red' if o > 0.05 else 'green' for o in overfit]\n",
    "\n",
    "axes[1].bar(model_names, overfit, color=colors, alpha=0.7)\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Train Acc - Test Acc')\n",
    "axes[1].set_title('Overfitting Analysis')\n",
    "axes[1].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- Boosting often achieves higher accuracy than bagging\")\n",
    "print(\"- Boosting may overfit more on training data\")\n",
    "print(\"- Random Forest provides good balance and stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Real-World Regression\n",
    "\n",
    "### Exercise 9.1: Diabetes Dataset Prediction\n",
    "\n",
    "**Task:** Apply gradient boosting to predict disease progression in diabetes patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes = diabetes.data\n",
    "y_diabetes = diabetes.target\n",
    "\n",
    "print(\"Diabetes Dataset:\")\n",
    "print(f\"Shape: {X_diabetes.shape}\")\n",
    "print(f\"Features: {diabetes.feature_names}\")\n",
    "print(f\"Target: Disease progression one year after baseline\\n\")\n",
    "\n",
    "# Split data\n",
    "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Your turn: Train and compare multiple regressors\n",
    "# Your code here\n",
    "regressors = {\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results_reg = {}\n",
    "\n",
    "for name, model in regressors.items():\n",
    "    model.fit(X_train_diab, y_train_diab)\n",
    "    \n",
    "    train_pred = model.predict(X_train_diab)\n",
    "    test_pred = model.predict(X_test_diab)\n",
    "    \n",
    "    results_reg[name] = {\n",
    "        'train_mse': mean_squared_error(y_train_diab, train_pred),\n",
    "        'test_mse': mean_squared_error(y_test_diab, test_pred),\n",
    "        'train_r2': r2_score(y_train_diab, train_pred),\n",
    "        'test_r2': r2_score(y_test_diab, test_pred),\n",
    "        'test_mae': mean_absolute_error(y_test_diab, test_pred)\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"\\nRegression Model Comparison:\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Model':<20} {'Test MSE':<12} {'Test R²':<12} {'Test MAE'}\")\n",
    "print(\"=\" * 75)\n",
    "for name, scores in results_reg.items():\n",
    "    print(f\"{name:<20} {scores['test_mse']:<12.2f} {scores['test_r2']:<12.4f} {scores['test_mae']:.2f}\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "# Feature importance from best model\n",
    "best_model = regressors['Gradient Boosting']\n",
    "feature_importance_diab = best_model.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sorted_idx = np.argsort(feature_importance_diab)[::-1]\n",
    "plt.bar(range(len(sorted_idx)), feature_importance_diab[sorted_idx])\n",
    "plt.xticks(range(len(sorted_idx)), [diabetes.feature_names[i] for i in sorted_idx], rotation=45)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance - Gradient Boosting on Diabetes')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 3 Most Important Features:\")\n",
    "for i, idx in enumerate(sorted_idx[:3], 1):\n",
    "    print(f\"{i}. {diabetes.feature_names[idx]}: {feature_importance_diab[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Exercise 9.2: Prediction Analysis\n",
    "\n",
    "**Task:** Visualize predictions vs actual values and residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from best model\n",
    "gb_best = regressors['Gradient Boosting']\n",
    "y_pred_diab = gb_best.predict(X_test_diab)\n",
    "residuals_diab = y_test_diab - y_pred_diab\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predicted vs Actual\n",
    "axes[0, 0].scatter(y_test_diab, y_pred_diab, alpha=0.6)\n",
    "axes[0, 0].plot([y_test_diab.min(), y_test_diab.max()], \n",
    "                [y_test_diab.min(), y_test_diab.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Disease Progression')\n",
    "axes[0, 0].set_ylabel('Predicted Disease Progression')\n",
    "axes[0, 0].set_title('Predicted vs Actual Values')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "axes[0, 1].scatter(y_pred_diab, residuals_diab, alpha=0.6)\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[0, 1].set_xlabel('Predicted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residual Plot')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Histogram of Residuals\n",
    "axes[1, 0].hist(residuals_diab, bins=30, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Residuals')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Residuals')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Error distribution\n",
    "absolute_errors = np.abs(residuals_diab)\n",
    "axes[1, 1].boxplot([absolute_errors], labels=['Absolute Errors'])\n",
    "axes[1, 1].set_ylabel('Absolute Error')\n",
    "axes[1, 1].set_title('Error Distribution')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Absolute Error: {np.mean(absolute_errors):.2f}\")\n",
    "print(f\"Median Absolute Error: {np.median(absolute_errors):.2f}\")\n",
    "print(f\"Max Error: {np.max(absolute_errors):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Imbalanced Classification\n",
    "\n",
    "### Exercise 10.1: Create Imbalanced Dataset\n",
    "\n",
    "**Task:** Apply boosting to handle imbalanced classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced dataset (90% class 0, 10% class 1)\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=15,\n",
    "    n_redundant=5, weights=[0.9, 0.1], flip_y=0.01, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Imbalanced Dataset:\")\n",
    "print(f\"Total samples: {len(y_imb)}\")\n",
    "print(f\"Class 0: {np.sum(y_imb == 0)} ({100 * np.sum(y_imb == 0) / len(y_imb):.1f}%)\")\n",
    "print(f\"Class 1: {np.sum(y_imb == 1)} ({100 * np.sum(y_imb == 1) / len(y_imb):.1f}%)\")\n",
    "\n",
    "# Split data\n",
    "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.3, random_state=42, stratify=y_imb\n",
    ")\n",
    "\n",
    "# Train models\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve\n",
    "\n",
    "models_imb = {\n",
    "    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results_imb = {}\n",
    "\n",
    "for name, model in models_imb.items():\n",
    "    model.fit(X_train_imb, y_train_imb)\n",
    "    \n",
    "    y_pred = model.predict(X_test_imb)\n",
    "    y_pred_proba = model.predict_proba(X_test_imb)[:, 1]\n",
    "    \n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    \n",
    "    results_imb[name] = {\n",
    "        'accuracy': accuracy_score(y_test_imb, y_pred),\n",
    "        'precision': precision_score(y_test_imb, y_pred),\n",
    "        'recall': recall_score(y_test_imb, y_pred),\n",
    "        'f1': f1_score(y_test_imb, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test_imb, y_pred_proba),\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"\\nImbalanced Classification Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Model':<20} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1':<12} {'ROC-AUC'}\")\n",
    "print(\"=\" * 80)\n",
    "for name, scores in results_imb.items():\n",
    "    print(f\"{name:<20} {scores['accuracy']:<12.4f} {scores['precision']:<12.4f} \"\n",
    "          f\"{scores['recall']:<12.4f} {scores['f1']:<12.4f} {scores['roc_auc']:.4f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nNote: For imbalanced data, focus on Precision, Recall, F1, and ROC-AUC rather than Accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### Exercise 10.2: ROC and Precision-Recall Curves\n",
    "\n",
    "**Task:** Visualize model performance on imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ROC Curves\n",
    "for name, scores in results_imb.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test_imb, scores['y_pred_proba'])\n",
    "    axes[0].plot(fpr, tpr, label=f\"{name} (AUC={scores['roc_auc']:.3f})\", linewidth=2)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curves - Imbalanced Dataset')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curves\n",
    "for name, scores in results_imb.items():\n",
    "    precision, recall, _ = precision_recall_curve(y_test_imb, scores['y_pred_proba'])\n",
    "    axes[1].plot(recall, precision, label=f\"{name} (F1={scores['f1']:.3f})\", linewidth=2)\n",
    "\n",
    "# Baseline (proportion of positive class)\n",
    "baseline = np.sum(y_test_imb == 1) / len(y_test_imb)\n",
    "axes[1].axhline(y=baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curves - Imbalanced Dataset')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"For imbalanced data:\")\n",
    "print(\"- ROC curve shows overall discriminative ability\")\n",
    "print(\"- Precision-Recall curve is more informative for minority class performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Early Stopping\n",
    "\n",
    "Implement early stopping to prevent overfitting in gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn: Implement early stopping\n",
    "# Hint: Use validation_fraction and n_iter_no_change parameters\n",
    "\n",
    "# Create validation set\n",
    "X_train_es, X_val_es, y_train_es, y_val_es = train_test_split(\n",
    "    X_train_gb, y_train_gb, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Your code here\n",
    "gb_early_stop = GradientBoostingRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    validation_fraction=0.2,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train and check how many iterations it used\n",
    "print(\"Challenge: Train a model with early stopping and report the number of iterations used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Challenge 2: Custom Loss Function\n",
    "\n",
    "Explore gradient boosting with different loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different loss functions for regression\n",
    "loss_functions = ['squared_error', 'absolute_error', 'huber']\n",
    "\n",
    "# Your code here\n",
    "# Train models with different loss functions and compare performance\n",
    "# Which loss function is most robust to outliers?\n",
    "\n",
    "print(\"Challenge: Compare different loss functions and analyze robustness to outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "### Challenge 3: XGBoost Integration\n",
    "\n",
    "If available, try using XGBoost for even better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you have xgboost installed\n",
    "# import xgboost as xgb\n",
    "\n",
    "# xgb_model = xgb.XGBClassifier(\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=3,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "# Your code here\n",
    "print(\"Challenge: Install xgboost and compare with sklearn's gradient boosting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **What is the key difference between AdaBoost and Gradient Boosting?**\n",
    "   - AdaBoost: Reweights samples\n",
    "   - Gradient Boosting: Fits residuals\n",
    "\n",
    "2. **Why is learning rate important in boosting?**\n",
    "   - Controls contribution of each tree\n",
    "   - Trade-off between speed and overfitting\n",
    "   - Lower rate needs more trees but generalizes better\n",
    "\n",
    "3. **When should you use boosting vs bagging?**\n",
    "   - Boosting: When you need maximum accuracy and can afford computation\n",
    "   - Bagging: When you want stability and parallelization\n",
    "   - Boosting: More prone to overfitting\n",
    "   - Bagging: More robust to noise\n",
    "\n",
    "4. **How does boosting handle imbalanced data?**\n",
    "   - Naturally focuses on hard examples (including minority class)\n",
    "   - Can use class weights for additional emphasis\n",
    "   - Better than simple approaches like random oversampling\n",
    "\n",
    "5. **What are the main hyperparameters to tune in gradient boosting?**\n",
    "   - `n_estimators`: Number of trees\n",
    "   - `learning_rate`: Shrinkage parameter\n",
    "   - `max_depth`: Tree complexity\n",
    "   - `min_samples_split`: Minimum samples to split\n",
    "   - `subsample`: Stochastic gradient boosting\n",
    "\n",
    "6. **Why might gradient boosting overfit?**\n",
    "   - Sequential nature makes it sensitive to noise\n",
    "   - Later trees may fit idiosyncrasies of training data\n",
    "   - Solution: Lower learning rate, early stopping, regularization\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "✓ Core concepts of boosting and sample reweighting  \n",
    "✓ AdaBoost algorithm and implementation from scratch  \n",
    "✓ Gradient boosting through residual fitting  \n",
    "✓ Applying boosting to classification and regression  \n",
    "✓ Effect of learning rate on model performance  \n",
    "✓ Comparing boosting with bagging approaches  \n",
    "✓ Handling imbalanced classification problems  \n",
    "✓ Feature importance analysis with boosting  \n",
    "✓ Real-world applications and evaluation  \n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Explore XGBoost and LightGBM libraries\n",
    "- Learn about histogram-based gradient boosting\n",
    "- Study advanced regularization techniques\n",
    "- Apply boosting to your own datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 4 - Exercise 3: Backpropagation\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module4-neural-networks/exercise3-backpropagation.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Implement backpropagation from scratch\n- Understand gradient computation through the chain rule\n- Calculate gradients for different activation functions\n- Debug backpropagation using gradient checking\n- Train neural networks with gradient descent\n- Understand common training issues and solutions\n\n## Prerequisites\n\n- Completion of Exercise 2 (Feedforward Networks)\n- Understanding of calculus and chain rule\n- Familiarity with gradient descent\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Chain Rule and Computational Graphs\n",
    "\n",
    "### Background\n",
    "\n",
    "Backpropagation is simply the **chain rule** applied to neural networks.\n",
    "\n",
    "**Example:** For $f(x) = (3x + 2)^2$\n",
    "\n",
    "Let $u = 3x + 2$ and $f = u^2$\n",
    "\n",
    "Chain rule: $$\\frac{df}{dx} = \\frac{df}{du} \\cdot \\frac{du}{dx} = 2u \\cdot 3 = 6(3x + 2)$$\n",
    "\n",
    "### Exercise 1.1: Simple Chain Rule\n",
    "\n",
    "**Task:** Compute derivatives using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_simple_derivative():\n",
    "    \"\"\"\n",
    "    Compute df/dx for f(x) = (3x + 2)^2 at x = 1\n",
    "    \"\"\"\n",
    "    x = 1\n",
    "    \n",
    "    # Forward pass\n",
    "    u = 3 * x + 2\n",
    "    f = u ** 2\n",
    "    \n",
    "    # Backward pass\n",
    "    # Your code here\n",
    "    df_du = 2 * u  # derivative of u^2\n",
    "    du_dx =        # derivative of 3x + 2\n",
    "    \n",
    "    df_dx = df_du * du_dx  # chain rule\n",
    "    \n",
    "    return f, df_dx\n",
    "\n",
    "f_val, df_dx = compute_simple_derivative()\n",
    "print(f\"f(1) = {f_val}\")\n",
    "print(f\"df/dx at x=1 = {df_dx}\")\n",
    "\n",
    "# Verify with numerical gradient\n",
    "epsilon = 1e-7\n",
    "x = 1\n",
    "numerical_grad = ((3*(x+epsilon) + 2)**2 - (3*x + 2)**2) / epsilon\n",
    "print(f\"Numerical gradient: {numerical_grad:.6f}\")\n",
    "print(f\"Match: {np.isclose(df_dx, numerical_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Computational Graph for Neural Network\n",
    "\n",
    "**Task:** Draw and understand the computational graph for a simple neuron.\n",
    "\n",
    "For a single neuron: $y = \\sigma(wx + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid: σ'(z) = σ(z)(1 - σ(z))\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Forward pass for a single neuron\n",
    "x = 2.0\n",
    "w = 0.5\n",
    "b = 1.0\n",
    "\n",
    "# Compute forward\n",
    "z = w * x + b\n",
    "y = sigmoid(z)\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(f\"x = {x}\")\n",
    "print(f\"z = wx + b = {w}*{x} + {b} = {z}\")\n",
    "print(f\"y = σ(z) = {y:.4f}\")\n",
    "\n",
    "# Backward pass: compute dy/dw, dy/db, dy/dx\n",
    "# Your code here\n",
    "dy_dz = sigmoid_derivative(z)  # derivative of σ(z)\n",
    "dz_dw =                        # derivative of wx + b w.r.t. w\n",
    "dz_db =                        # derivative of wx + b w.r.t. b\n",
    "dz_dx =                        # derivative of wx + b w.r.t. x\n",
    "\n",
    "dy_dw = dy_dz * dz_dw\n",
    "dy_db = dy_dz * dz_db\n",
    "dy_dx = dy_dz * dz_dx\n",
    "\n",
    "print(\"\\nBackward Pass (Gradients):\")\n",
    "print(f\"dy/dw = {dy_dw:.4f}\")\n",
    "print(f\"dy/db = {dy_db:.4f}\")\n",
    "print(f\"dy/dx = {dy_dx:.4f}\")\n",
    "\n",
    "# Visualize computational graph\n",
    "print(\"\\nComputational Graph:\")\n",
    "print(\"x ----> [×w] ----> [+b] ----> [σ] ----> y\")\n",
    "print(\"         |          |          |\")\n",
    "print(\"         w          b          \")\n",
    "print(\"\\nBackward flow:\")\n",
    "print(\"dy/dx <-- dy/dz·w <-- dy/dz <-- dy/dy=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Manual Gradient Computation for 2-Layer Network\n",
    "\n",
    "### Background\n",
    "\n",
    "For a 2-layer network:\n",
    "\n",
    "**Forward:**\n",
    "$$\\mathbf{Z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf{X} + \\mathbf{b}^{[1]}$$\n",
    "$$\\mathbf{A}^{[1]} = \\text{ReLU}(\\mathbf{Z}^{[1]})$$\n",
    "$$\\mathbf{Z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{A}^{[1]} + \\mathbf{b}^{[2]}$$\n",
    "$$\\mathbf{A}^{[2]} = \\sigma(\\mathbf{Z}^{[2]})$$\n",
    "\n",
    "**Loss (Binary Cross-Entropy):**\n",
    "$$L = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(a^{[2](i)}) + (1-y^{(i)}) \\log(1-a^{[2](i)})]$$\n",
    "\n",
    "### Exercise 2.1: Derive Backward Pass Equations\n",
    "\n",
    "**Task:** Complete the backward pass derivations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"ReLU activation.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU: 1 if z > 0, else 0\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : ndarray, shape (1, m)\n",
    "        True labels\n",
    "    y_pred : ndarray, shape (1, m)\n",
    "        Predicted probabilities\n",
    "    \"\"\"\n",
    "    m = y_true.shape[1]\n",
    "    epsilon = 1e-15  # for numerical stability\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    loss = -1/m * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss\n",
    "\n",
    "print(\"Backward Pass Equations:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\")\n",
    "print(\"Output layer:\")\n",
    "print(\"  dL/dA² = -(Y/A² - (1-Y)/(1-A²))\")\n",
    "print(\"  dL/dZ² = dL/dA² ⊙ σ'(Z²) = A² - Y\")\n",
    "print(\"\")\n",
    "print(\"Hidden layer:\")\n",
    "print(\"  dL/dW² = (1/m) · dL/dZ² · (A¹)ᵀ\")\n",
    "print(\"  dL/db² = (1/m) · sum(dL/dZ², axis=1, keepdims=True)\")\n",
    "print(\"  dL/dA¹ = (W²)ᵀ · dL/dZ²\")\n",
    "print(\"  dL/dZ¹ = dL/dA¹ ⊙ ReLU'(Z¹)\")\n",
    "print(\"  dL/dW¹ = (1/m) · dL/dZ¹ · Xᵀ\")\n",
    "print(\"  dL/db¹ = (1/m) · sum(dL/dZ¹, axis=1, keepdims=True)\")\n",
    "print(\"\")\n",
    "print(\"Note: ⊙ denotes element-wise multiplication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Implement Backward Pass\n",
    "\n",
    "**Task:** Complete the backward propagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache, parameters):\n",
    "    \"\"\"\n",
    "    Implement backward propagation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray, shape (n_input, m)\n",
    "        Input data\n",
    "    Y : ndarray, shape (1, m)\n",
    "        True labels\n",
    "    cache : dict\n",
    "        Contains Z1, A1, Z2, A2 from forward pass\n",
    "    parameters : dict\n",
    "        Contains W1, b1, W2, b2\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    gradients : dict\n",
    "        Contains dW1, db1, dW2, db2\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Retrieve from cache\n",
    "    Z1 = cache['Z1']\n",
    "    A1 = cache['A1']\n",
    "    Z2 = cache['Z2']\n",
    "    A2 = cache['A2']\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    # Backward propagation\n",
    "    # Output layer\n",
    "    # Your code here\n",
    "    dZ2 = A2 - Y  # derivative of binary cross-entropy with sigmoid\n",
    "    dW2 = \n",
    "    db2 = \n",
    "    \n",
    "    # Hidden layer\n",
    "    # Your code here\n",
    "    dA1 = \n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = \n",
    "    db1 = \n",
    "    \n",
    "    gradients = {\n",
    "        'dW1': dW1,\n",
    "        'db1': db1,\n",
    "        'dW2': dW2,\n",
    "        'db2': db2\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "print(\"Backward propagation function implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Gradient Checking\n",
    "\n",
    "### Background\n",
    "\n",
    "Gradient checking verifies that backpropagation is correct by comparing analytical gradients with numerical gradients.\n",
    "\n",
    "Numerical gradient: $$\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta - \\epsilon)}{2\\epsilon}$$\n",
    "\n",
    "### Exercise 3.1: Implement Gradient Checking\n",
    "\n",
    "**Task:** Implement numerical gradient computation and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A2 : predictions\n",
    "    cache : intermediate values\n",
    "    \"\"\"\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache = {'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2}\n",
    "    \n",
    "    return A2, cache\n",
    "\n",
    "def gradient_check(X, Y, parameters, gradients, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Check if backpropagation gradients are correct.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : input data\n",
    "    Y : labels\n",
    "    parameters : network parameters\n",
    "    gradients : gradients from backprop\n",
    "    epsilon : small value for numerical gradient\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    difference : relative difference between gradients\n",
    "    \"\"\"\n",
    "    # Convert parameters and gradients to vectors\n",
    "    param_keys = ['W1', 'b1', 'W2', 'b2']\n",
    "    \n",
    "    # Flatten parameters\n",
    "    theta = np.concatenate([parameters[key].ravel() for key in param_keys])\n",
    "    grad_theta = np.concatenate([gradients['d' + key].ravel() for key in param_keys])\n",
    "    \n",
    "    # Compute numerical gradients\n",
    "    num_gradients = np.zeros_like(theta)\n",
    "    \n",
    "    for i in range(len(theta)):\n",
    "        # Perturb theta[i] by +epsilon\n",
    "        theta_plus = theta.copy()\n",
    "        theta_plus[i] += epsilon\n",
    "        \n",
    "        # Reconstruct parameters\n",
    "        params_plus = dict_from_vector(theta_plus, parameters)\n",
    "        A2_plus, _ = forward_propagation(X, params_plus)\n",
    "        loss_plus = binary_cross_entropy(Y, A2_plus)\n",
    "        \n",
    "        # Perturb theta[i] by -epsilon\n",
    "        theta_minus = theta.copy()\n",
    "        theta_minus[i] -= epsilon\n",
    "        \n",
    "        # Reconstruct parameters\n",
    "        params_minus = dict_from_vector(theta_minus, parameters)\n",
    "        A2_minus, _ = forward_propagation(X, params_minus)\n",
    "        loss_minus = binary_cross_entropy(Y, A2_minus)\n",
    "        \n",
    "        # Compute numerical gradient\n",
    "        # Your code here\n",
    "        num_gradients[i] = \n",
    "    \n",
    "    # Compute relative difference\n",
    "    numerator = np.linalg.norm(grad_theta - num_gradients)\n",
    "    denominator = np.linalg.norm(grad_theta) + np.linalg.norm(num_gradients)\n",
    "    difference = numerator / denominator\n",
    "    \n",
    "    return difference, num_gradients, grad_theta\n",
    "\n",
    "def dict_from_vector(theta, parameters):\n",
    "    \"\"\"Reconstruct parameter dictionary from vector.\"\"\"\n",
    "    params = {}\n",
    "    idx = 0\n",
    "    \n",
    "    for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "        shape = parameters[key].shape\n",
    "        size = np.prod(shape)\n",
    "        params[key] = theta[idx:idx+size].reshape(shape)\n",
    "        idx += size\n",
    "    \n",
    "    return params\n",
    "\n",
    "print(\"Gradient checking functions implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Test Gradient Checking\n",
    "\n",
    "**Task:** Run gradient checking on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small dataset for testing\n",
    "X_test = np.random.randn(2, 3)\n",
    "Y_test = np.array([[1, 0, 1]])\n",
    "\n",
    "# Initialize small network\n",
    "parameters_test = {\n",
    "    'W1': np.random.randn(3, 2) * 0.01,\n",
    "    'b1': np.zeros((3, 1)),\n",
    "    'W2': np.random.randn(1, 3) * 0.01,\n",
    "    'b2': np.zeros((1, 1))\n",
    "}\n",
    "\n",
    "# Forward and backward pass\n",
    "A2_test, cache_test = forward_propagation(X_test, parameters_test)\n",
    "gradients_test = backward_propagation(X_test, Y_test, cache_test, parameters_test)\n",
    "\n",
    "# Run gradient checking\n",
    "print(\"Running Gradient Checking...\")\n",
    "print(\"=\" * 70)\n",
    "difference, num_grad, ana_grad = gradient_check(X_test, Y_test, \n",
    "                                                 parameters_test, gradients_test)\n",
    "\n",
    "print(f\"\\nRelative difference: {difference:.10f}\")\n",
    "print(\"\")\n",
    "if difference < 1e-7:\n",
    "    print(\"✓ EXCELLENT! Gradient check passed with high precision.\")\n",
    "elif difference < 1e-5:\n",
    "    print(\"✓ GOOD! Gradient check passed.\")\n",
    "elif difference < 1e-3:\n",
    "    print(\"⚠ WARNING: Gradient check borderline. Check implementation.\")\n",
    "else:\n",
    "    print(\"✗ FAIL: Gradient check failed. Bug in backpropagation!\")\n",
    "\n",
    "# Show some gradient comparisons\n",
    "print(\"\\nSample Gradient Comparisons:\")\n",
    "print(f\"{'Index':<10} {'Analytical':<20} {'Numerical':<20} {'Difference'}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(min(5, len(ana_grad))):\n",
    "    diff = abs(ana_grad[i] - num_grad[i])\n",
    "    print(f\"{i:<10} {ana_grad[i]:<20.10f} {num_grad[i]:<20.10f} {diff:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Complete Training Loop\n",
    "\n",
    "### Exercise 4.1: Implement Training Function\n",
    "\n",
    "**Task:** Build a complete training loop with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_input, n_hidden, n_output):\n",
    "    \"\"\"\n",
    "    Initialize network parameters.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    parameters : dict\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    parameters = {\n",
    "        'W1': np.random.randn(n_hidden, n_input) * 0.01,\n",
    "        'b1': np.zeros((n_hidden, 1)),\n",
    "        'W2': np.random.randn(n_output, n_hidden) * 0.01,\n",
    "        'b2': np.zeros((n_output, 1))\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    parameters : dict\n",
    "        Current parameters\n",
    "    gradients : dict\n",
    "        Gradients from backprop\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    parameters : dict\n",
    "        Updated parameters\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    parameters['W1'] = parameters['W1'] - learning_rate * gradients['dW1']\n",
    "    parameters['b1'] = \n",
    "    parameters['W2'] = \n",
    "    parameters['b2'] = \n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def train_network(X, Y, n_hidden=4, learning_rate=0.01, num_iterations=10000, \n",
    "                  print_cost=True):\n",
    "    \"\"\"\n",
    "    Train a 2-layer neural network.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray, shape (n_input, m)\n",
    "        Training data\n",
    "    Y : ndarray, shape (1, m)\n",
    "        Labels\n",
    "    n_hidden : int\n",
    "        Number of hidden units\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    num_iterations : int\n",
    "        Number of training iterations\n",
    "    print_cost : bool\n",
    "        Whether to print cost\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    parameters : dict\n",
    "        Trained parameters\n",
    "    costs : list\n",
    "        Loss history\n",
    "    \"\"\"\n",
    "    n_input = X.shape[0]\n",
    "    n_output = 1\n",
    "    \n",
    "    # Initialize\n",
    "    parameters = initialize_parameters(n_input, n_hidden, n_output)\n",
    "    costs = []\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(num_iterations):\n",
    "        # Forward propagation\n",
    "        # Your code here\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = binary_cross_entropy(Y, A2)\n",
    "        \n",
    "        # Backward propagation\n",
    "        # Your code here\n",
    "        gradients = \n",
    "        \n",
    "        # Update parameters\n",
    "        # Your code here\n",
    "        parameters = \n",
    "        \n",
    "        # Record cost\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if print_cost:\n",
    "                accuracy = np.mean((A2 > 0.5) == Y)\n",
    "                print(f\"Iteration {i:5d}: Loss = {cost:.6f}, Accuracy = {accuracy:.4f}\")\n",
    "    \n",
    "    return parameters, costs\n",
    "\n",
    "print(\"Training function implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Train on XOR Problem\n",
    "\n",
    "**Task:** Train the network to solve XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0, 1, 1],\n",
    "                  [0, 1, 0, 1]])\n",
    "Y_xor = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "print(\"Training on XOR Problem\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train the network\n",
    "parameters_xor, costs_xor = train_network(X_xor, Y_xor, n_hidden=4, \n",
    "                                          learning_rate=1.0, \n",
    "                                          num_iterations=5000)\n",
    "\n",
    "# Test final predictions\n",
    "A2_final, _ = forward_propagation(X_xor, parameters_xor)\n",
    "predictions = (A2_final > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'x1':<5} {'x2':<5} {'True':<8} {'Predicted':<12} {'Probability':<15} {'Correct'}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor[:, i]\n",
    "    true_y = Y_xor[0, i]\n",
    "    pred_y = predictions[0, i]\n",
    "    prob_y = A2_final[0, i]\n",
    "    correct = '✓' if pred_y == true_y else '✗'\n",
    "    print(f\"{x1:<5.0f} {x2:<5.0f} {true_y:<8.0f} {pred_y:<12.0f} {prob_y:<15.4f} {correct}\")\n",
    "\n",
    "accuracy = np.mean(predictions == Y_xor)\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "if accuracy == 1.0:\n",
    "    print(\"✓ SUCCESS! XOR problem solved with backpropagation!\")\n",
    "else:\n",
    "    print(\"⚠ Network did not fully converge. Try adjusting hyperparameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Learning Curves and Visualization\n",
    "\n",
    "### Exercise 5.1: Plot Learning Curve\n",
    "\n",
    "**Task:** Visualize how the loss decreases during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(costs, title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    Plot the learning curve (loss over iterations).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(costs, linewidth=2)\n",
    "    plt.xlabel('Iterations (x100)', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curve(costs_xor, \"Learning Curve: XOR Problem\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(f\"  • Initial loss: {costs_xor[0]:.6f}\")\n",
    "print(f\"  • Final loss: {costs_xor[-1]:.6f}\")\n",
    "print(f\"  • Loss reduction: {(costs_xor[0] - costs_xor[-1]):.6f}\")\n",
    "print(\"\\nA good learning curve shows:\")\n",
    "print(\"  1. Rapid initial decrease\")\n",
    "print(\"  2. Gradual convergence to a low value\")\n",
    "print(\"  3. Smooth curve (not too noisy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Decision Boundary Visualization\n",
    "\n",
    "**Task:** Visualize the decision boundary learned by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(parameters, X, Y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for 2D data.\n",
    "    \"\"\"\n",
    "    # Create mesh\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    h = 0.01\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    mesh_input = np.c_[xx.ravel(), yy.ravel()].T\n",
    "    Z, _ = forward_propagation(mesh_input, parameters)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "    plt.colorbar(label='Predicted Probability')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[0, Y[0]==0], X[1, Y[0]==0], c='blue', s=200,\n",
    "               edgecolors='k', marker='o', label='Class 0', linewidths=2)\n",
    "    plt.scatter(X[0, Y[0]==1], X[1, Y[0]==1], c='red', s=200,\n",
    "               edgecolors='k', marker='s', label='Class 1', linewidths=2)\n",
    "    \n",
    "    # Decision boundary\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=3)\n",
    "    \n",
    "    plt.xlabel('$x_1$', fontsize=14)\n",
    "    plt.ylabel('$x_2$', fontsize=14)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(parameters_xor, X_xor, Y_xor, \n",
    "                      \"Decision Boundary: XOR Solved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Real Dataset - Moons\n",
    "\n",
    "### Exercise 6.1: Train on Moons Dataset\n",
    "\n",
    "**Task:** Apply backpropagation to a more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moons dataset\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "X_moons = X_moons.T\n",
    "y_moons = y_moons.reshape(1, -1)\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_moons[0, y_moons[0]==0], X_moons[1, y_moons[0]==0],\n",
    "           c='blue', alpha=0.6, edgecolors='k', label='Class 0')\n",
    "plt.scatter(X_moons[0, y_moons[0]==1], X_moons[1, y_moons[0]==1],\n",
    "           c='red', alpha=0.6, edgecolors='k', label='Class 1')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Moons Dataset', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Training on Moons Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Your code here: Train on moons dataset\n",
    "# Try different hidden layer sizes (4, 8, 16)\n",
    "parameters_moons, costs_moons = train_network(X_moons, y_moons, \n",
    "                                              n_hidden=8,\n",
    "                                              learning_rate=0.5,\n",
    "                                              num_iterations=5000)\n",
    "\n",
    "# Evaluate\n",
    "A2_moons, _ = forward_propagation(X_moons, parameters_moons)\n",
    "accuracy_moons = np.mean((A2_moons > 0.5) == y_moons)\n",
    "print(f\"\\nFinal Accuracy: {accuracy_moons:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Visualize Moons Results\n",
    "\n",
    "**Task:** Plot learning curve and decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plot_learning_curve(costs_moons, \"Learning Curve: Moons Dataset\")\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary(parameters_moons, X_moons, y_moons,\n",
    "                      \"Decision Boundary: Moons Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Implement Momentum\n",
    "\n",
    "Extend gradient descent with momentum for faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, gradients, velocity, \n",
    "                                   learning_rate, beta=0.9):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent with momentum.\n",
    "    \n",
    "    Momentum update:\n",
    "    v = beta * v + (1 - beta) * gradient\n",
    "    parameter = parameter - learning_rate * v\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "print(\"Challenge: Implement momentum optimizer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Implement Adam Optimizer\n",
    "\n",
    "Implement the Adam optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, gradients, v, s, t,\n",
    "                               learning_rate=0.001, beta1=0.9, beta2=0.999):\n",
    "    \"\"\"\n",
    "    Update parameters using Adam optimizer.\n",
    "    \n",
    "    Adam combines momentum and RMSprop:\n",
    "    v = beta1 * v + (1 - beta1) * gradient\n",
    "    s = beta2 * s + (1 - beta2) * gradient^2\n",
    "    v_corrected = v / (1 - beta1^t)\n",
    "    s_corrected = s / (1 - beta2^t)\n",
    "    parameter = parameter - learning_rate * v_corrected / (sqrt(s_corrected) + epsilon)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "print(\"Challenge: Implement Adam optimizer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Visualize Gradient Flow\n",
    "\n",
    "Create a visualization showing how gradients flow backward through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Visualize gradient magnitudes at each layer\n",
    "# Compare gradient norms for different layers\n",
    "\n",
    "print(\"Challenge: Visualize gradient flow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Why is backpropagation efficient compared to numerical gradients?**\n",
    "   - Think about computational complexity\n",
    "\n",
    "2. **What causes vanishing/exploding gradients?**\n",
    "   - How does activation function choice affect this?\n",
    "\n",
    "3. **Why is gradient checking important?**\n",
    "   - When should you use it?\n",
    "\n",
    "4. **How does learning rate affect convergence?**\n",
    "   - What happens if it's too large or too small?\n",
    "\n",
    "5. **What is the role of initialization?**\n",
    "   - Why not initialize all weights to zero?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "- The chain rule and computational graphs\n",
    "- How to manually derive and implement backpropagation\n",
    "- Gradient checking to verify correctness\n",
    "- Building a complete training loop from scratch\n",
    "- Visualizing learning curves and decision boundaries\n",
    "- Training on real datasets (XOR, Moons)\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Backpropagation = chain rule applied efficiently\n",
    "- Gradients flow backward from output to input\n",
    "- Always verify with gradient checking during development\n",
    "- Learning curves reveal training dynamics\n",
    "- Proper initialization and learning rate are crucial\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete Exercise 4 on NumPy Implementation\n",
    "- Review [Lesson 3: Backpropagation](https://jumpingsphinx.github.io/module4-neural-networks/03-backpropagation/)\n",
    "- Experiment with different optimization algorithms\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
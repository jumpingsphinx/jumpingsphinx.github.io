{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 4 - Exercise 5: PyTorch Basics\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module4-neural-networks/exercise5-pytorch-basics.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Understand PyTorch tensors and autograd\n- Build neural networks using nn.Module\n- Train models with PyTorch's built-in optimizers\n- Use DataLoader for efficient batch processing\n- Save and load trained models\n- Compare PyTorch with your NumPy implementation\n\n## Prerequisites\n\n- Completion of Exercise 4 (NumPy Implementation)\n- Basic understanding of deep learning frameworks\n- Familiarity with Python classes\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (uncomment if needed)\n",
    "# !pip install torch torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: PyTorch Tensors\n",
    "\n",
    "### Background\n",
    "\n",
    "PyTorch tensors are similar to NumPy arrays but:\n",
    "- Can run on GPU for acceleration\n",
    "- Support automatic differentiation\n",
    "- Are the foundation of PyTorch neural networks\n",
    "\n",
    "### Exercise 1.1: Create and Manipulate Tensors\n",
    "\n",
    "**Task:** Learn basic tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Tensor Basics\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create tensors\n",
    "# Your code here\n",
    "x = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"1D tensor: {x}\")\n",
    "\n",
    "# 2D tensor (matrix)\n",
    "X = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "print(f\"\\n2D tensor:\\n{X}\")\n",
    "\n",
    "# Random tensors\n",
    "rand_tensor = torch.randn(3, 4)  # Normal distribution\n",
    "print(f\"\\nRandom tensor:\\n{rand_tensor}\")\n",
    "\n",
    "# Zeros and ones\n",
    "zeros = torch.zeros(2, 3)\n",
    "ones = torch.ones(2, 3)\n",
    "print(f\"\\nZeros:\\n{zeros}\")\n",
    "print(f\"\\nOnes:\\n{ones}\")\n",
    "\n",
    "# Tensor properties\n",
    "print(\"\\nTensor Properties:\")\n",
    "print(f\"Shape: {X.shape}\")\n",
    "print(f\"Data type: {X.dtype}\")\n",
    "print(f\"Device: {X.device}\")\n",
    "\n",
    "# Convert to/from NumPy\n",
    "numpy_array = X.numpy()\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"\\nNumPy array:\\n{numpy_array}\")\n",
    "print(f\"Tensor from NumPy:\\n{tensor_from_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Tensor Operations\n",
    "\n",
    "**Task:** Perform common tensor operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors for operations\n",
    "a = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0]])\n",
    "b = torch.tensor([[5.0, 6.0],\n",
    "                  [7.0, 8.0]])\n",
    "\n",
    "print(\"Tensor Operations\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Element-wise operations\n",
    "# Your code here\n",
    "print(f\"a + b:\\n{a + b}\")\n",
    "print(f\"\\na * b (element-wise):\\n{a * b}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "print(f\"\\na @ b (matrix multiplication):\\n{a @ b}\")\n",
    "# Or use torch.mm(a, b)\n",
    "\n",
    "# Transpose\n",
    "print(f\"\\na.T (transpose):\\n{a.T}\")\n",
    "\n",
    "# Reshape\n",
    "reshaped = a.reshape(1, 4)\n",
    "print(f\"\\nReshape to (1, 4):\\n{reshaped}\")\n",
    "\n",
    "# Aggregations\n",
    "print(f\"\\nSum: {a.sum()}\")\n",
    "print(f\"Mean: {a.mean()}\")\n",
    "print(f\"Max: {a.max()}\")\n",
    "print(f\"Argmax: {a.argmax()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Automatic Differentiation (Autograd)\n",
    "\n",
    "### Background\n",
    "\n",
    "PyTorch's autograd system automatically computes gradients. This is the key feature that makes training neural networks easy!\n",
    "\n",
    "### Exercise 2.1: Compute Gradients with Autograd\n",
    "\n",
    "**Task:** Use autograd to compute derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Automatic Differentiation (Autograd)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create tensor with requires_grad=True to track computations\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"x.requires_grad = {x.requires_grad}\")\n",
    "\n",
    "# Define a function: y = x^2 + 3x + 1\n",
    "y = x**2 + 3*x + 1\n",
    "print(f\"\\ny = x^2 + 3x + 1 = {y.item()}\")\n",
    "\n",
    "# Compute gradient: dy/dx = 2x + 3\n",
    "y.backward()  # Backpropagation!\n",
    "\n",
    "print(f\"\\ndy/dx (computed by autograd) = {x.grad}\")\n",
    "print(f\"dy/dx (analytical) = 2*{x.item()} + 3 = {2*x.item() + 3}\")\n",
    "\n",
    "# More complex example\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"More Complex Example: Multi-variable\")\n",
    "\n",
    "# Your code here: compute gradients for z = x^2 + y^2\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0], requires_grad=True)\n",
    "\n",
    "z = x**2 + y**2\n",
    "print(f\"z = x^2 + y^2 = {z.item()}\")\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(f\"\\ndz/dx = {x.grad} (should be 2*x = {2*3.0})\")\n",
    "print(f\"dz/dy = {y.grad} (should be 2*y = {2*4.0})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Gradient for a Simple Neural Network Operation\n",
    "\n",
    "**Task:** Compute gradients for a simple linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate y = W @ x + b\n",
    "x = torch.randn(5, 1)  # Input\n",
    "W = torch.randn(3, 5, requires_grad=True)  # Weights\n",
    "b = torch.randn(3, 1, requires_grad=True)  # Bias\n",
    "\n",
    "# Forward pass\n",
    "y = W @ x + b\n",
    "\n",
    "# Loss (sum for simplicity)\n",
    "loss = y.sum()\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Weights shape: {W.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nGradients computed!\")\n",
    "print(f\"W.grad shape: {W.grad.shape}\")\n",
    "print(f\"b.grad shape: {b.grad.shape}\")\n",
    "print(\"\\nAutograd tracked the computational graph and computed all gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Building Neural Networks with nn.Module\n",
    "\n",
    "### Background\n",
    "\n",
    "`nn.Module` is the base class for all neural network modules in PyTorch. You define:\n",
    "1. `__init__`: Initialize layers\n",
    "2. `forward`: Define forward pass\n",
    "\n",
    "PyTorch handles the backward pass automatically!\n",
    "\n",
    "### Exercise 3.1: Create a Simple Network\n",
    "\n",
    "**Task:** Build a 2-layer neural network using nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 2-layer neural network.\n",
    "    \n",
    "    Architecture: [input_size, hidden_size, output_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        \n",
    "        # Your code here: define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor, shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : torch.Tensor, shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # Your code here: define forward computation\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleNN(input_size=10, hidden_size=20, output_size=5)\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "x_test = torch.randn(3, 10)  # Batch of 3 samples\n",
    "output = model(x_test)\n",
    "print(f\"\\nInput shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Using nn.Sequential\n",
    "\n",
    "**Task:** Build the same network using nn.Sequential for faster prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: create the same network using nn.Sequential\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5)\n",
    ")\n",
    "\n",
    "print(\"Sequential Model:\")\n",
    "print(model_sequential)\n",
    "\n",
    "# Test\n",
    "output_seq = model_sequential(x_test)\n",
    "print(f\"\\nOutput shape: {output_seq.shape}\")\n",
    "\n",
    "print(\"\\nNote: nn.Sequential is great for simple architectures!\")\n",
    "print(\"Use nn.Module for more complex models with custom forward logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Loss Functions and Optimizers\n",
    "\n",
    "### Exercise 4.1: Common Loss Functions\n",
    "\n",
    "**Task:** Learn about PyTorch loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Loss Functions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Mean Squared Error (regression)\n",
    "mse_loss = nn.MSELoss()\n",
    "predictions = torch.tensor([1.0, 2.0, 3.0])\n",
    "targets = torch.tensor([1.5, 2.5, 2.5])\n",
    "loss_mse = mse_loss(predictions, targets)\n",
    "print(f\"MSE Loss: {loss_mse.item():.4f}\")\n",
    "\n",
    "# Binary Cross-Entropy (binary classification)\n",
    "bce_loss = nn.BCEWithLogitsLoss()  # Combines sigmoid + BCE for stability\n",
    "logits = torch.tensor([0.5, -0.2, 1.0])\n",
    "targets_binary = torch.tensor([1.0, 0.0, 1.0])\n",
    "loss_bce = bce_loss(logits, targets_binary)\n",
    "print(f\"BCE Loss: {loss_bce.item():.4f}\")\n",
    "\n",
    "# Cross-Entropy (multi-class classification)\n",
    "ce_loss = nn.CrossEntropyLoss()  # Combines softmax + NLL for stability\n",
    "# Note: expects raw logits, not probabilities!\n",
    "logits_multi = torch.randn(3, 5)  # Batch of 3, 5 classes\n",
    "targets_multi = torch.tensor([1, 3, 2])  # Class indices\n",
    "loss_ce = ce_loss(logits_multi, targets_multi)\n",
    "print(f\"Cross-Entropy Loss: {loss_ce.item():.4f}\")\n",
    "\n",
    "print(\"\\nNote: PyTorch loss functions often combine activation + loss\")\n",
    "print(\"for numerical stability (e.g., BCEWithLogitsLoss, CrossEntropyLoss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Optimizers\n",
    "\n",
    "**Task:** Explore PyTorch optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PyTorch Optimizers\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a simple model\n",
    "model = SimpleNN(10, 20, 5)\n",
    "\n",
    "# Stochastic Gradient Descent (SGD)\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "print(\"SGD Optimizer:\")\n",
    "print(optimizer_sgd)\n",
    "\n",
    "# SGD with momentum\n",
    "optimizer_sgd_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "print(\"\\nSGD with Momentum:\")\n",
    "print(optimizer_sgd_momentum)\n",
    "\n",
    "# Adam (adaptive learning rate)\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "print(\"\\nAdam Optimizer:\")\n",
    "print(optimizer_adam)\n",
    "\n",
    "# RMSprop\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001)\n",
    "print(\"\\nRMSprop Optimizer:\")\n",
    "print(optimizer_rmsprop)\n",
    "\n",
    "print(\"\\nMost commonly used: Adam (good default choice)\")\n",
    "print(\"SGD with momentum: Often better for final convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Training Loop\n",
    "\n",
    "### Background\n",
    "\n",
    "A typical PyTorch training loop:\n",
    "1. Forward pass\n",
    "2. Compute loss\n",
    "3. Zero gradients\n",
    "4. Backward pass\n",
    "5. Update weights\n",
    "\n",
    "### Exercise 5.1: Simple Training Example (XOR)\n",
    "\n",
    "**Task:** Train a network on XOR using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = torch.tensor([[0.0, 0.0],\n",
    "                      [0.0, 1.0],\n",
    "                      [1.0, 0.0],\n",
    "                      [1.0, 1.0]])\n",
    "y_xor = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\n",
    "\n",
    "print(\"Training Neural Network on XOR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    # Your code here\n",
    "    predictions = model(X_xor)\n",
    "    loss = criterion(predictions, y_xor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    # Your code here\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    # Record loss\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/1000, Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Test final predictions\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    final_predictions = model(X_xor)\n",
    "    binary_preds = (final_predictions > 0.5).float()\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'x1':<5} {'x2':<5} {'True':<8} {'Predicted':<12} {'Probability':<15} {'Correct'}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor[i].numpy()\n",
    "    true_y = y_xor[i].item()\n",
    "    pred_y = binary_preds[i].item()\n",
    "    prob_y = final_predictions[i].item()\n",
    "    correct = '✓' if pred_y == true_y else '✗'\n",
    "    print(f\"{x1:<5.0f} {x2:<5.0f} {true_y:<8.0f} {pred_y:<12.0f} {prob_y:<15.4f} {correct}\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('XOR Training Loss (PyTorch)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: MNIST Classification\n",
    "\n",
    "### Exercise 6.1: Load MNIST Dataset\n",
    "\n",
    "**Task:** Download and prepare the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading MNIST Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize (mean, std)\n",
    "])\n",
    "\n",
    "# Download and load data\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, \n",
    "                              download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "\n",
    "# Visualize some samples\n",
    "examples = iter(train_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(example_data[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {example_targets[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nImage shape: {example_data[0].shape}\")\n",
    "print(\"Note: Shape is (1, 28, 28) - 1 channel, 28x28 pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.2: Build MNIST Classifier\n",
    "\n",
    "**Task:** Create a neural network for MNIST digit classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for MNIST classification.\n",
    "    \n",
    "    Architecture: Flatten -> Dense -> ReLU -> Dense -> ReLU -> Dense\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        \n",
    "        # Your code here: define layers\n",
    "        # Input: 28x28 = 784 pixels\n",
    "        # Hidden layers: 128, 64\n",
    "        # Output: 10 classes (digits 0-9)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor, shape (batch_size, 1, 28, 28)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : torch.Tensor, shape (batch_size, 10)\n",
    "            Logits for 10 classes\n",
    "        \"\"\"\n",
    "        # Your code here\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model and move to device\n",
    "model = MNISTClassifier().to(device)\n",
    "print(\"MNIST Classifier:\")\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.3: Train MNIST Classifier\n",
    "\n",
    "**Task:** Implement the training loop for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        # Your code here\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        # Your code here\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate on test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Train the model\n",
    "print(\"Training MNIST Classifier\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, train_accs = [], []\n",
    "test_losses, test_accs = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6.4: Visualize Results\n",
    "\n",
    "**Task:** Plot training curves and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "ax1.plot(test_losses, label='Test Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(train_accs, label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(test_accs, label='Test Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show some predictions\n",
    "model.eval()\n",
    "test_iter = iter(test_loader)\n",
    "images, labels = next(test_iter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# Visualize\n",
    "images = images.cpu()\n",
    "labels = labels.cpu()\n",
    "predicted = predicted.cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "    color = 'green' if predicted[i] == labels[i] else 'red'\n",
    "    ax.set_title(f\"True: {labels[i]}, Pred: {predicted[i]}\", color=color)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accs[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Saving and Loading Models\n",
    "\n",
    "### Exercise 7.1: Save and Load Model\n",
    "\n",
    "**Task:** Learn to save and load trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving and Loading Models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save model\n",
    "model_path = 'mnist_model.pth'\n",
    "\n",
    "# Method 1: Save entire model\n",
    "torch.save(model, model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Method 2: Save only state dict (recommended)\n",
    "state_dict_path = 'mnist_state_dict.pth'\n",
    "torch.save(model.state_dict(), state_dict_path)\n",
    "print(f\"State dict saved to {state_dict_path}\")\n",
    "\n",
    "# Save checkpoint with optimizer state\n",
    "checkpoint_path = 'mnist_checkpoint.pth'\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': train_losses[-1],\n",
    "    'test_loss': test_losses[-1],\n",
    "    'test_accuracy': test_accs[-1]\n",
    "}, checkpoint_path)\n",
    "print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Load model\n",
    "# Method 1: Load entire model\n",
    "loaded_model = torch.load(model_path)\n",
    "loaded_model.eval()\n",
    "print(\"Loaded entire model\")\n",
    "\n",
    "# Method 2: Load state dict\n",
    "new_model = MNISTClassifier().to(device)\n",
    "new_model.load_state_dict(torch.load(state_dict_path))\n",
    "new_model.eval()\n",
    "print(\"Loaded model from state dict\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "checkpoint_model = MNISTClassifier().to(device)\n",
    "checkpoint_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "checkpoint_model.eval()\n",
    "print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Test accuracy: {checkpoint['test_accuracy']:.2f}%\")\n",
    "\n",
    "# Verify loaded model works\n",
    "test_loss, test_acc = evaluate(new_model, test_loader, criterion, device)\n",
    "print(f\"\\nVerification - Test Accuracy: {test_acc:.2f}%\")\n",
    "print(\"✓ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Add Dropout Regularization\n",
    "\n",
    "Modify the MNIST classifier to include dropout layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Add dropout to the network\n",
    "# Hint: Use nn.Dropout(p=0.5)\n",
    "\n",
    "print(\"Challenge: Add dropout regularization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Learning Rate Scheduling\n",
    "\n",
    "Implement learning rate decay during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Use torch.optim.lr_scheduler\n",
    "# Try StepLR or ReduceLROnPlateau\n",
    "\n",
    "print(\"Challenge: Implement learning rate scheduling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Early Stopping\n",
    "\n",
    "Implement early stopping to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Stop training when validation loss stops improving\n",
    "\n",
    "print(\"Challenge: Implement early stopping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **How does PyTorch autograd simplify neural network training?**\n",
    "   - Compare to manual backpropagation\n",
    "\n",
    "2. **What are the advantages of nn.Module over manual implementation?**\n",
    "   - Think about code organization and reusability\n",
    "\n",
    "3. **Why use DataLoader instead of feeding entire dataset?**\n",
    "   - Consider memory and training efficiency\n",
    "\n",
    "4. **What's the difference between model.train() and model.eval()?**\n",
    "   - How does this affect layers like dropout and batch norm?\n",
    "\n",
    "5. **When should you save the entire model vs just state_dict?**\n",
    "   - What are the trade-offs?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "- PyTorch tensor operations and automatic differentiation\n",
    "- Building neural networks with nn.Module and nn.Sequential\n",
    "- Using loss functions and optimizers\n",
    "- Implementing training loops with proper gradient handling\n",
    "- Training on MNIST digit classification\n",
    "- Model evaluation and visualization\n",
    "- Saving and loading trained models\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- PyTorch autograd eliminates manual gradient computation\n",
    "- nn.Module provides a clean interface for model building\n",
    "- DataLoader handles batching and shuffling efficiently\n",
    "- Always use `optimizer.zero_grad()` before backward pass\n",
    "- Switch between train/eval modes appropriately\n",
    "- Save checkpoints during training for recovery\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete Exercise 6 on Advanced PyTorch\n",
    "- Review [Lesson 5: Introduction to PyTorch](https://jumpingsphinx.github.io/module4-neural-networks/05-pytorch-intro/)\n",
    "- Experiment with different architectures and hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
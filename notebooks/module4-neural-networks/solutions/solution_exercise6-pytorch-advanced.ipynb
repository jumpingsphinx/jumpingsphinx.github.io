{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Exercise 6: PyTorch Advanced Topics\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module4-neural-networks/exercise6-pytorch-advanced.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this exercise, you will be able to:\n",
    "\n",
    "- Implement custom layers and loss functions\n",
    "- Use learning rate schedulers and advanced optimizers\n",
    "- Apply data augmentation techniques\n",
    "- Implement early stopping and model checkpointing\n",
    "- Use TensorBoard for visualization\n",
    "- Build production-ready training pipelines\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Exercise 5 (PyTorch Basics)\n",
    "- Understanding of advanced deep learning concepts\n",
    "- Familiarity with PyTorch ecosystem\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this cell first to import required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Custom Layers and Modules\n",
    "\n",
    "### Background\n",
    "\n",
    "Sometimes you need to create custom layers with specialized behavior. PyTorch makes this easy by subclassing nn.Module.\n",
    "\n",
    "### Exercise 1.1: Create a Custom Residual Block\n",
    "\n",
    "**Task:** Implement a residual connection block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with skip connection.\n",
    "    \n",
    "    out = F(x) + x\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Main path\n",
    "        self.fc1 = nn.Linear(in_features, out_features)\n",
    "        self.bn1 = nn.BatchNorm1d(out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(out_features, out_features)\n",
    "        self.bn2 = nn.BatchNorm1d(out_features)\n",
    "        \n",
    "        # Skip connection (if dimensions don't match)\n",
    "        self.skip = None\n",
    "        if in_features != out_features:\n",
    "            self.skip = nn.Linear(in_features, out_features)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with residual connection.\n",
    "        \"\"\"\n",
    "        identity = x\n",
    "        \n",
    "        # Main path\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Skip connection\n",
    "        if self.skip is not None:\n",
    "            identity = self.skip(identity)\n",
    "        \n",
    "        # Add and activate\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "        return out\n",
    "\n",
    "# Test residual block\n",
    "print(\"Testing Residual Block\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "res_block = ResidualBlock(64, 128)\n",
    "x_test = torch.randn(32, 64)  # Batch of 32\n",
    "output = res_block(x_test)\n",
    "\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"✓ Residual block working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Create a Custom Attention Layer\n",
    "\n",
    "**Task:** Implement a simple self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple self-attention layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # Create query, key, value projections\n",
    "        self.query = nn.Linear(in_features, in_features)\n",
    "        self.key = nn.Linear(in_features, in_features)\n",
    "        self.value = nn.Linear(in_features, in_features)\n",
    "        \n",
    "        self.scale = np.sqrt(in_features)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply self-attention.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor, shape (batch_size, in_features)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : torch.Tensor, shape (batch_size, in_features)\n",
    "        \"\"\"\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.T) / self.scale\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention_weights, V)\n",
    "        return out\n",
    "        return out\n",
    "\n",
    "# Test attention\n",
    "attention = SelfAttention(64)\n",
    "x_test = torch.randn(32, 64)\n",
    "output = attention(x_test)\n",
    "print(f\"\\nAttention input: {x_test.shape}\")\n",
    "print(f\"Attention output: {output.shape}\")\n",
    "print(\"✓ Self-attention working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Learning Rate Schedulers\n",
    "\n",
    "### Background\n",
    "\n",
    "Learning rate scheduling adjusts the learning rate during training for better convergence.\n",
    "\n",
    "### Exercise 2.1: Compare Different Schedulers\n",
    "\n",
    "**Task:** Test various learning rate schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learning Rate Schedulers\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a dummy model and optimizer\n",
    "model = nn.Linear(10, 1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Different schedulers\n",
    "schedulers = {\n",
    "    'StepLR': StepLR(optimizer, step_size=10, gamma=0.5),\n",
    "    'ReduceLROnPlateau': ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5),\n",
    "    'CosineAnnealing': CosineAnnealingLR(optimizer, T_max=50, eta_min=0.001)\n",
    "}\n",
    "\n",
    "# Simulate training and track learning rates\n",
    "epochs = 50\n",
    "lr_history = {name: [] for name in schedulers.keys()}\n",
    "\n",
    "for name, scheduler in schedulers.items():\n",
    "    # Reset optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    if name == 'StepLR':\n",
    "        scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    elif name == 'ReduceLROnPlateau':\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    else:\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Simulate validation loss (decreasing with noise)\n",
    "        val_loss = 1.0 / (epoch + 1) + np.random.randn() * 0.05\n",
    "        \n",
    "        # Record learning rate\n",
    "        lr_history[name].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Step scheduler\n",
    "        if name == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "# Plot learning rate schedules\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, lrs in lr_history.items():\n",
    "    plt.plot(lrs, label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Learning Rate Schedulers Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScheduler Descriptions:\")\n",
    "print(\"  • StepLR: Reduces LR by factor every N epochs\")\n",
    "print(\"  • ReduceLROnPlateau: Reduces LR when metric stops improving\")\n",
    "print(\"  • CosineAnnealing: Smooth cosine decay to minimum LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Regularization Techniques\n",
    "\n",
    "### Exercise 3.1: Implement Network with Dropout and Batch Normalization\n",
    "\n",
    "**Task:** Create a well-regularized network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP with dropout and batch normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.5):\n",
    "        super(RegularizedMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create regularized model\n",
    "model_reg = RegularizedMLP(input_size=784, \n",
    "                          hidden_sizes=[256, 128, 64], \n",
    "                          output_size=10,\n",
    "                          dropout_rate=0.3)\n",
    "\n",
    "print(\"Regularized MLP Architecture:\")\n",
    "print(model_reg)\n",
    "\n",
    "total_params = sum(p.numel() for p in model_reg.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Train with L2 Regularization (Weight Decay)\n",
    "\n",
    "**Task:** Compare training with and without weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "# Split train into train/val\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001, weight_decay=0):\n",
    "    \"\"\"\n",
    "    Train model and track metrics.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data = data.view(data.size(0), -1).to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train without weight decay\n",
    "print(\"Training WITHOUT weight decay\")\n",
    "print(\"=\" * 70)\n",
    "model_no_wd = RegularizedMLP(784, [256, 128, 64], 10, dropout_rate=0.3)\n",
    "history_no_wd = train_model(model_no_wd, train_loader, val_loader, epochs=20, weight_decay=0)\n",
    "\n",
    "# Train with weight decay\n",
    "print(\"\\nTraining WITH weight decay\")\n",
    "print(\"=\" * 70)\n",
    "model_wd = RegularizedMLP(784, [256, 128, 64], 10, dropout_rate=0.3)\n",
    "history_wd = train_model(model_wd, train_loader, val_loader, epochs=20, weight_decay=0.001)\n",
    "\n",
    "# Compare\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history_no_wd['train_loss'], label='No WD - Train', linewidth=2)\n",
    "ax1.plot(history_no_wd['val_loss'], label='No WD - Val', linewidth=2)\n",
    "ax1.plot(history_wd['train_loss'], label='WD - Train', linewidth=2, linestyle='--')\n",
    "ax1.plot(history_wd['val_loss'], label='WD - Val', linewidth=2, linestyle='--')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(history_no_wd['val_acc'], label='No Weight Decay', linewidth=2)\n",
    "ax2.plot(history_wd['val_acc'], label='With Weight Decay', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Validation Accuracy (%)')\n",
    "ax2.set_title('Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Val Acc (No WD): {history_no_wd['val_acc'][-1]:.2f}%\")\n",
    "print(f\"Final Val Acc (With WD): {history_wd['val_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Data Augmentation\n",
    "\n",
    "### Exercise 4.1: Apply Augmentation Transforms\n",
    "\n",
    "**Task:** Create augmented datasets for better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Augmentation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define augmentation transforms\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),  # Rotate by ±10 degrees\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Shift\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Load datasets with augmentation\n",
    "    train_dataset_aug = datasets.MNIST(root='./data', train=True, \n",
    "                                       download=True, transform=train_transform)\n",
    "    test_dataset_aug = datasets.MNIST(root='./data', train=False, \n",
    "                                      download=True, transform=test_transform)\n",
    "print(\"Data augmentation applies random transformations during training.\")\n",
    "print(\"This helps the model generalize better to variations in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Model Checkpointing\n",
    "\n",
    "### Exercise 5.1: Implement Checkpoint Saving\n",
    "\n",
    "**Task:** Save best model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping to stop training when validation loss doesn't improve.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=5, min_delta=0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "\n",
    "def train_with_checkpointing(model, train_loader, val_loader, epochs=50, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train with checkpointing and early stopping.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    early_stopping = EarlyStopping(patience=7, verbose=True)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data = data.view(data.size(0), -1).to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100. * correct / total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"✓ Saved new best model (Val Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            model.load_state_dict(early_stopping.best_model)\n",
    "            break\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train with checkpointing\n",
    "print(\"Training with Checkpointing and Early Stopping\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_checkpoint = RegularizedMLP(784, [256, 128, 64], 10, dropout_rate=0.3)\n",
    "history_checkpoint = train_with_checkpointing(model_checkpoint, train_loader, val_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Transfer Learning Concepts\n",
    "\n",
    "### Exercise 6.1: Use Pre-trained Features\n",
    "\n",
    "**Task:** Demonstrate feature extraction with a pre-trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transfer Learning Concepts\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load pre-trained ResNet (example - won't train on MNIST)\n",
    "# This demonstrates the concept\n",
    "pretrained_model = models.resnet18(pretrained=True)\n",
    "\n",
    "print(\"Pre-trained ResNet18:\")\n",
    "print(pretrained_model)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer\n",
    "num_features = pretrained_model.fc.in_features\n",
    "pretrained_model.fc = nn.Linear(num_features, 10)  # 10 classes for MNIST\n",
    "\n",
    "print(\"\\nModified for MNIST (last layer):\")\n",
    "print(pretrained_model.fc)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in pretrained_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in pretrained_model.parameters())\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "print(\"\\nTransfer Learning Benefits:\")\n",
    "print(\"  • Faster training (fewer parameters to update)\")\n",
    "print(\"  • Better performance with less data\")\n",
    "print(\"  • Leverage knowledge from large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Advanced Architecture\n",
    "\n",
    "### Exercise 7.1: Build a Multi-Path Network\n",
    "\n",
    "**Task:** Create a network with parallel paths (Inception-style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPathBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-path block with parallel processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features):\n",
    "        super(MultiPathBlock, self).__init__()\n",
    "        \n",
    "        # Path 1: Direct\n",
    "        self.path1 = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Path 2: Deeper\n",
    "        self.path2 = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features // 2, in_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Combine paths\n",
    "        self.combine = nn.Linear(in_features * 2, in_features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Process through parallel paths\n",
    "        out1 = self.path1(x)\n",
    "        out2 = self.path2(x)\n",
    "        \n",
    "        # Concatenate and combine\n",
    "        combined = torch.cat([out1, out2], dim=1)\n",
    "        output = self.combine(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class AdvancedNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Advanced network with multiple techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(AdvancedNetwork, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.multi_path = MultiPathBlock(256)\n",
    "        \n",
    "        self.residual = ResidualBlock(256, 128)\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.multi_path(x)\n",
    "        x = self.residual(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "# Create advanced network\n",
    "advanced_model = AdvancedNetwork(784, 10)\n",
    "print(\"Advanced Network Architecture:\")\n",
    "print(advanced_model)\n",
    "\n",
    "total_params = sum(p.numel() for p in advanced_model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Implement Gradient Clipping\n",
    "\n",
    "Add gradient clipping to prevent exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Use torch.nn.utils.clip_grad_norm_()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "print(\"Challenge: Implement gradient clipping!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Mixed Precision Training\n",
    "\n",
    "Use automatic mixed precision for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Use torch.cuda.amp.autocast and GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # In training loop:\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    #     output = model(data)\n",
    "    #     loss = criterion(output, target)\n",
    "    # scaler.scale(loss).backward()\n",
    "    # scaler.step(optimizer)\n",
    "    # scaler.update()\n",
    "print(\"Challenge: Implement mixed precision training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Custom Loss Function\n",
    "\n",
    "Create a custom loss function combining multiple objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weight_decay=0.0):\n",
    "        super().__init__()\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "    def forward(self, output, target, model):\n",
    "        loss = self.ce(output, target)\n",
    "        l1_reg = torch.tensor(0.).to(output.device)\n",
    "        for param in model.parameters():\n",
    "            l1_reg += torch.norm(param, 1)\n",
    "        return loss + self.weight_decay * l1_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **When should you use residual connections?**\n",
    "   - Think about very deep networks\n",
    "\n",
    "2. **How does batch normalization help training?**\n",
    "   - Consider internal covariate shift\n",
    "\n",
    "3. **What's the difference between dropout and weight decay?**\n",
    "   - How do they prevent overfitting differently?\n",
    "\n",
    "4. **Why is early stopping important?**\n",
    "   - What problems does it solve?\n",
    "\n",
    "5. **When is transfer learning most effective?**\n",
    "   - Think about dataset size and similarity\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "- Creating custom layers (residual blocks, attention)\n",
    "- Using learning rate schedulers for adaptive learning\n",
    "- Applying regularization (dropout, batch norm, weight decay)\n",
    "- Data augmentation for better generalization\n",
    "- Model checkpointing and early stopping\n",
    "- Transfer learning concepts\n",
    "- Building advanced architectures with multiple techniques\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Custom modules enable flexible architecture design\n",
    "- Learning rate scheduling improves convergence\n",
    "- Multiple regularization techniques work together\n",
    "- Data augmentation is crucial for small datasets\n",
    "- Checkpointing prevents loss of progress\n",
    "- Transfer learning leverages pre-trained knowledge\n",
    "- Combining techniques creates robust models\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Explore convolutional neural networks (CNNs)\n",
    "- Study recurrent neural networks (RNNs)\n",
    "- Learn about attention mechanisms and transformers\n",
    "- Practice on real-world datasets (ImageNet, COCO, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You've completed the Neural Networks module. You now have a solid foundation in both the theory and practice of deep learning with PyTorch.\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

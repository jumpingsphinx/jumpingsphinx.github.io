{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 4 - Exercise 4: Complete NumPy Neural Network\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module4-neural-networks/exercise4-numpy-implementation.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Build a complete neural network library using only NumPy\n- Implement modular components (layers, activations, losses)\n- Train networks on real datasets (MNIST, Fashion-MNIST)\n- Add regularization and optimization improvements\n- Evaluate and visualize network performance\n- Understand the full training pipeline\n\n## Prerequisites\n\n- Completion of Exercise 3 (Backpropagation)\n- Strong NumPy proficiency\n- Understanding of software design patterns\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles, load_digits, load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Layer Base Class\n",
    "\n",
    "### Background\n",
    "\n",
    "We'll design our library using object-oriented principles. Each layer will:\n",
    "\n",
    "- Have a `forward(input)` method for forward propagation\n",
    "- Have a `backward(grad_output)` method for backpropagation\n",
    "- Store parameters and gradients internally\n",
    "\n",
    "### Exercise 1.1: Create Base Layer Class\n",
    "\n",
    "**Task:** Implement the base Layer class that all layers will inherit from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Base class for all layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input : ndarray\n",
    "            Input to the layer\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : ndarray\n",
    "            Output of the layer\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad_output : ndarray\n",
    "            Gradient of loss w.r.t. layer output\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        grad_input : ndarray\n",
    "            Gradient of loss w.r.t. layer input\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(\"Base Layer class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Dense (Fully Connected) Layer\n",
    "\n",
    "### Background\n",
    "\n",
    "A dense layer performs: $$\\mathbf{Y} = \\mathbf{X} \\mathbf{W}^T + \\mathbf{b}$$\n",
    "\n",
    "**Forward:** $\\mathbf{Y} = \\mathbf{X} \\mathbf{W}^T + \\mathbf{b}$\n",
    "\n",
    "**Backward:**\n",
    "- $\\frac{\\partial L}{\\partial \\mathbf{W}} = (\\frac{\\partial L}{\\partial \\mathbf{Y}})^T \\mathbf{X}$\n",
    "- $\\frac{\\partial L}{\\partial \\mathbf{b}} = \\sum \\frac{\\partial L}{\\partial \\mathbf{Y}}$\n",
    "- $\\frac{\\partial L}{\\partial \\mathbf{X}} = \\frac{\\partial L}{\\partial \\mathbf{Y}} \\mathbf{W}$\n",
    "\n",
    "### Exercise 2.1: Implement Dense Layer\n",
    "\n",
    "**Task:** Complete the Dense layer implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    Fully connected (dense) layer.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_size : int\n",
    "        Number of input features\n",
    "    output_size : int\n",
    "        Number of output features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize weights with Xavier/He initialization\n",
    "        self.weights = np.random.randn(output_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass: Y = X @ W.T + b\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input : ndarray, shape (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : ndarray, shape (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.output = np.dot(input, self.weights.T) + self.bias\n",
    "        return self.output\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad_output : ndarray, shape (batch_size, output_size)\n",
    "            Gradient of loss w.r.t. output\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        grad_input : ndarray, shape (batch_size, input_size)\n",
    "            Gradient of loss w.r.t. input\n",
    "        \"\"\"\n",
    "        self.grad_weights = np.dot(grad_output.T, self.input)\n",
    "        self.grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        grad_input = np.dot(grad_output, self.weights)\n",
    "        return grad_input\n",
    "\n",
    "# Test Dense layer\n",
    "print(\"Testing Dense Layer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dense = Dense(3, 2)\n",
    "x_test = np.random.randn(5, 3)  # batch of 5 samples, 3 features\n",
    "y_test = dense.forward(x_test)\n",
    "\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {y_test.shape}\")\n",
    "print(f\"Weights shape: {dense.weights.shape}\")\n",
    "print(f\"Bias shape: {dense.bias.shape}\")\n",
    "\n",
    "# Test backward\n",
    "grad_out = np.random.randn(5, 2)\n",
    "grad_in = dense.backward(grad_out)\n",
    "print(f\"\\nGradient output shape: {grad_out.shape}\")\n",
    "print(f\"Gradient input shape: {grad_in.shape}\")\n",
    "print(f\"Gradient weights shape: {dense.grad_weights.shape}\")\n",
    "print(f\"Gradient bias shape: {dense.grad_bias.shape}\")\n",
    "print(\"\\n\u2713 Dense layer implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Activation Layers\n",
    "\n",
    "### Exercise 3.1: Implement ReLU Layer\n",
    "\n",
    "**Task:** Create a ReLU activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \"\"\"\n",
    "    ReLU activation layer.\n",
    "    \n",
    "    Forward: f(x) = max(0, x)\n",
    "    Backward: f'(x) = 1 if x > 0 else 0\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.maximum(0, input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for ReLU.\n",
    "        \n",
    "        grad_input = grad_output * (input > 0)\n",
    "        \"\"\"\n",
    "        grad_input = grad_output * (self.input > 0)\n",
    "        return grad_input\n",
    "\n",
    "# Test ReLU\n",
    "relu = ReLU()\n",
    "x = np.array([[-2, -1, 0, 1, 2]])\n",
    "y = relu.forward(x)\n",
    "print(f\"ReLU forward: {x} -> {y}\")\n",
    "\n",
    "grad_out = np.ones_like(y)\n",
    "grad_in = relu.backward(grad_out)\n",
    "print(f\"ReLU backward: {grad_in}\")\n",
    "print(\"\u2713 ReLU layer implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Implement Sigmoid Layer\n",
    "\n",
    "**Task:** Create a Sigmoid activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    \"\"\"\n",
    "    Sigmoid activation layer.\n",
    "    \n",
    "    Forward: f(x) = 1 / (1 + exp(-x))\n",
    "    Backward: f'(x) = f(x) * (1 - f(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = 1 / (1 + np.exp(-np.clip(input, -500, 500)))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for Sigmoid.\n",
    "        \"\"\"\n",
    "        grad_input = grad_output * self.output * (1 - self.output)\n",
    "        return grad_input\n",
    "\n",
    "# Test Sigmoid\n",
    "sigmoid = Sigmoid()\n",
    "x = np.array([[-2, -1, 0, 1, 2]])\n",
    "y = sigmoid.forward(x)\n",
    "print(f\"Sigmoid forward: {x} -> {y}\")\n",
    "print(\"\u2713 Sigmoid layer implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Implement Softmax Layer\n",
    "\n",
    "**Task:** Create a Softmax activation layer for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    \"\"\"\n",
    "    Softmax activation layer.\n",
    "    \n",
    "    Forward: softmax(x)_i = exp(x_i) / sum(exp(x_j))\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        \n",
    "        # Subtract max for numerical stability\n",
    "        exp_values = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass for Softmax.\n",
    "        Note: Usually combined with cross-entropy loss for efficiency.\n",
    "        \"\"\"\n",
    "        # Simplified: we'll handle this in the loss function\n",
    "        return grad_output\n",
    "\n",
    "# Test Softmax\n",
    "softmax = Softmax()\n",
    "x = np.array([[1, 2, 3], [1, 2, 3]])\n",
    "y = softmax.forward(x)\n",
    "print(f\"Softmax forward:\\n{x}\\n->\\n{y}\")\n",
    "print(f\"Sum of probabilities: {np.sum(y, axis=1)}\")\n",
    "print(\"\u2713 Softmax layer implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Loss Functions\n",
    "\n",
    "### Exercise 4.1: Implement Mean Squared Error (MSE)\n",
    "\n",
    "**Task:** Create MSE loss for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss.\n",
    "    \n",
    "    L = (1/m) * sum((y_pred - y_true)^2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute MSE loss.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_pred : ndarray, shape (batch_size, output_size)\n",
    "        y_true : ndarray, shape (batch_size, output_size)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "        \"\"\"\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        loss = np.mean((y_pred - y_true) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Compute gradient of MSE.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        grad : ndarray\n",
    "            Gradient w.r.t. predictions\n",
    "        \"\"\"\n",
    "        grad = 2 * (self.y_pred - self.y_true) / m\n",
    "        return grad\n",
    "\n",
    "# Test MSE\n",
    "mse = MSELoss()\n",
    "y_pred = np.array([[1.0], [2.0], [3.0]])\n",
    "y_true = np.array([[1.5], [2.5], [2.5]])\n",
    "loss = mse.forward(y_pred, y_true)\n",
    "grad = mse.backward()\n",
    "print(f\"MSE Loss: {loss:.4f}\")\n",
    "print(f\"MSE Gradient: {grad.ravel()}\")\n",
    "print(\"\u2713 MSE loss implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Implement Binary Cross-Entropy\n",
    "\n",
    "**Task:** Create binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss.\n",
    "    \n",
    "    L = -(1/m) * sum(y*log(p) + (1-y)*log(1-p))\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute binary cross-entropy.\n",
    "        \"\"\"\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        # Clip for numerical stability\n",
    "        epsilon = 1e-15\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -np.mean(y_true * np.log(y_pred_clipped) + \n",
    "                       (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Gradient for binary cross-entropy with sigmoid.\n",
    "        Simplified: y_pred - y_true (when using sigmoid)\n",
    "        \"\"\"\n",
    "        # Use the stable gradient formula for Sigmoid + BCE\n",
    "        # dL/da = (p - y) / (p * (1 - p))\n",
    "        # Added stability epsilon\n",
    "        grad = (self.y_pred - self.y_true) / (self.y_pred * (1 - self.y_pred) + 1e-15)\n",
    "        return grad\n",
    "\n",
    "# Test BCE\n",
    "bce = BinaryCrossEntropyLoss()\n",
    "y_pred = np.array([[0.9], [0.2], [0.8]])\n",
    "y_true = np.array([[1.0], [0.0], [1.0]])\n",
    "loss = bce.forward(y_pred, y_true)\n",
    "print(f\"BCE Loss: {loss:.4f}\")\n",
    "print(\"\u2713 Binary cross-entropy loss implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Implement Categorical Cross-Entropy\n",
    "\n",
    "**Task:** Create categorical cross-entropy for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss:\n",
    "    \"\"\"\n",
    "    Categorical cross-entropy loss.\n",
    "    \n",
    "    For use with softmax output and one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute categorical cross-entropy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y_pred : ndarray, shape (batch_size, num_classes)\n",
    "            Predicted probabilities (after softmax)\n",
    "        y_true : ndarray, shape (batch_size, num_classes)\n",
    "            One-hot encoded labels\n",
    "        \"\"\"\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        \n",
    "        # Clip for stability\n",
    "        epsilon = 1e-15\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred_clipped), axis=1))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Gradient for categorical cross-entropy with softmax.\n",
    "        Simplified: y_pred - y_true\n",
    "        \"\"\"\n",
    "        grad = self.y_pred - self.y_true\n",
    "        return grad\n",
    "\n",
    "# Test categorical cross-entropy\n",
    "cce = CategoricalCrossEntropyLoss()\n",
    "y_pred = np.array([[0.7, 0.2, 0.1],\n",
    "                   [0.1, 0.8, 0.1]])\n",
    "y_true = np.array([[1, 0, 0],\n",
    "                   [0, 1, 0]])\n",
    "loss = cce.forward(y_pred, y_true)\n",
    "print(f\"Categorical Cross-Entropy Loss: {loss:.4f}\")\n",
    "print(\"\u2713 Categorical cross-entropy loss implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Neural Network Class\n",
    "\n",
    "### Exercise 5.1: Create Network Class\n",
    "\n",
    "**Task:** Build a class to manage the entire network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    Modular neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss_function = None\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"Add a layer to the network.\"\"\"\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def set_loss(self, loss_function):\n",
    "        \"\"\"Set the loss function.\"\"\"\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass through all layers.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : ndarray\n",
    "            Network output\n",
    "        \"\"\"\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"\n",
    "        Backward pass through all layers.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad : ndarray\n",
    "            Gradient from loss\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dense):\n",
    "                layer.weights -= learning_rate * layer.grad_weights\n",
    "                layer.bias -= learning_rate * layer.grad_bias\n",
    "    def train_step(self, X, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "        \"\"\"\n",
    "        # Forward\n",
    "        predictions = self.forward(X)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss_function.forward(predictions, y)\n",
    "        \n",
    "        # Backward\n",
    "        grad = self.loss_function.backward()\n",
    "        self.backward(grad)\n",
    "        \n",
    "        # Update\n",
    "        self.update_parameters(learning_rate)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None,\n",
    "           epochs=100, learning_rate=0.01, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : ndarray\n",
    "            Training data\n",
    "        y_train : ndarray\n",
    "            Training labels\n",
    "        X_val : ndarray, optional\n",
    "            Validation data\n",
    "        y_val : ndarray, optional\n",
    "            Validation labels\n",
    "        epochs : int\n",
    "            Number of epochs\n",
    "        learning_rate : float\n",
    "            Learning rate\n",
    "        batch_size : int\n",
    "            Batch size\n",
    "        verbose : bool\n",
    "            Print progress\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        history : dict\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        history = {'train_loss': [], 'val_loss': []}\n",
    "        n_samples = X_train.shape[0]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                \n",
    "                loss = self.train_step(X_batch, y_batch, learning_rate)\n",
    "                epoch_loss += loss\n",
    "                n_batches += 1\n",
    "            \n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            history['train_loss'].append(avg_loss)\n",
    "            \n",
    "            # Validation\n",
    "            if X_val is not None:\n",
    "                val_pred = self.forward(X_val)\n",
    "                val_loss = self.loss_function.forward(val_pred, y_val)\n",
    "                history['val_loss'].append(val_loss)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 10 == 0:\n",
    "                if X_val is not None:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f} - Val Loss: {val_loss:.6f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "print(\"Neural Network class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Test on XOR Problem\n",
    "\n",
    "### Exercise 6.1: Build and Train Network for XOR\n",
    "\n",
    "**Task:** Use your library to solve XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(\"Solving XOR with Custom Neural Network Library\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build network\n",
    "    model = NeuralNetwork()\n",
    "    model.add(Dense(2, 4))\n",
    "    model.add(ReLU())\n",
    "    model.add(Dense(4, 1))\n",
    "    model.add(Sigmoid())\n",
    "    model.set_loss(BinaryCrossEntropyLoss())\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(X_xor, y_xor, epochs=1000, learning_rate=0.5, \n",
    "                       batch_size=4, verbose=True)\n",
    "# Test\n",
    "predictions = model.predict(X_xor)\n",
    "binary_preds = (predictions > 0.5).astype(int)\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'x1':<5} {'x2':<5} {'True':<8} {'Predicted':<12} {'Probability':<15} {'Correct'}\")\n",
    "print(\"-\" * 70)\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor[i]\n",
    "    true_y = y_xor[i, 0]\n",
    "    pred_y = binary_preds[i, 0]\n",
    "    prob_y = predictions[i, 0]\n",
    "    correct = '\u2713' if pred_y == true_y else '\u2717'\n",
    "    print(f\"{x1:<5.0f} {x2:<5.0f} {true_y:<8.0f} {pred_y:<12.0f} {prob_y:<15.4f} {correct}\")\n",
    "\n",
    "accuracy = np.mean(binary_preds == y_xor)\n",
    "print(f\"\\nAccuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('XOR Training Loss', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Multi-Class Classification - Digits Dataset\n",
    "\n",
    "### Exercise 7.1: Load and Prepare Digits Dataset\n",
    "\n",
    "**Task:** Prepare the 8x8 digits dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(\"Digits Dataset Information:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Samples: {X_digits.shape[0]}\")\n",
    "print(f\"Features: {X_digits.shape[1]} (8x8 pixel images)\")\n",
    "print(f\"Classes: {len(np.unique(y_digits))} (digits 0-9)\")\n",
    "\n",
    "# Visualize some digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_digits[i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f\"Label: {y_digits[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare data\n",
    "    model_digits = NeuralNetwork()\n",
    "    model_digits.add(Dense(64, 32))\n",
    "    model_digits.add(ReLU())\n",
    "    model_digits.add(Dense(32, 16))\n",
    "    model_digits.add(ReLU())\n",
    "    model_digits.add(Dense(16, 10))\n",
    "    model_digits.add(Softmax())\n",
    "    model_digits.set_loss(CategoricalCrossEntropyLoss())\n",
    "    \n",
    "    # Train\n",
    "    history = model_digits.fit(X_train, y_train_onehot, \n",
    "                          X_val=X_test, y_val=y_test_onehot,\n",
    "                          epochs=100, learning_rate=0.1, \n",
    "                          batch_size=32, verbose=True)\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((y.shape[0], num_classes))\n",
    "    one_hot[np.arange(y.shape[0]), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_onehot = one_hot_encode(y_train, 10)\n",
    "y_test_onehot = one_hot_encode(y_test, 10)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"One-hot labels shape: {y_train_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.2: Build and Train Multi-Class Network\n",
    "\n",
    "**Task:** Create a network for 10-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Multi-Class Classification Network\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build network\n",
    "# Your code here: create architecture [64, 32, 16, 10]\n",
    "model_digits = NeuralNetwork()\n",
    "model_digits.add(Dense(64, 32))\n",
    "model_digits.add(ReLU())\n",
    "model_digits.add(Dense(32, 16))\n",
    "model_digits.add(ReLU())\n",
    "model_digits.add(Dense(16, 10))\n",
    "model_digits.add(Softmax())\n",
    "model_digits.set_loss(CategoricalCrossEntropyLoss())\n",
    "\n",
    "# Train\n",
    "history = model_digits.fit(X_train, y_train_onehot, \n",
    "                          X_val=X_test, y_val=y_test_onehot,\n",
    "                          epochs=100, learning_rate=0.1, \n",
    "                          batch_size=32, verbose=True)\n",
    "\n",
    "# Evaluate\n",
    "test_pred = model_digits.predict(X_test)\n",
    "test_pred_classes = np.argmax(test_pred, axis=1)\n",
    "test_accuracy = accuracy_score(y_test, test_pred_classes)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.2%}\")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Digits Classification Training', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7.3: Confusion Matrix\n",
    "\n",
    "**Task:** Visualize the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, test_pred_classes)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix: Digits Classification', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Show some predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    image = scaler.inverse_transform(X_test[idx:idx+1]).reshape(8, 8)\n",
    "    pred = test_pred_classes[idx]\n",
    "    true = y_test[idx]\n",
    "    \n",
    "    ax.imshow(image, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f\"True: {true}, Pred: {pred}\", color=color)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Add Dropout Layer\n",
    "\n",
    "Implement dropout regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    \"\"\"\n",
    "    Dropout layer for regularization.\n",
    "    \n",
    "    During training: randomly set a fraction of inputs to 0.\n",
    "    During inference: use all inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.training = True\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Your code here\n",
    "        pass\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge: Implement Dropout layer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Add Batch Normalization\n",
    "\n",
    "Implement batch normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(Layer):\n",
    "    \"\"\"\n",
    "    Batch normalization layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, momentum=0.9, epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge: Implement Batch Normalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Implement Adam Optimizer\n",
    "\n",
    "Add an optimizer class to replace simple gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    \"\"\"\n",
    "    Adam optimizer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "print(\"Challenge: Implement Adam optimizer!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Why use a modular design for neural networks?**\n",
    "   - Think about extensibility and maintenance\n",
    "\n",
    "2. **How does batch processing improve training?**\n",
    "   - Consider computational efficiency and gradient stability\n",
    "\n",
    "3. **What are the trade-offs between library complexity and flexibility?**\n",
    "   - Compare with frameworks like PyTorch/TensorFlow\n",
    "\n",
    "4. **Why is proper weight initialization important?**\n",
    "   - What happens with zero or very large initial weights?\n",
    "\n",
    "5. **How would you extend this library for convolutional networks?**\n",
    "   - What new layer types would you need?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "- How to design a modular neural network library\n",
    "- Implementing layer classes with forward/backward methods\n",
    "- Creating various activation functions and loss functions\n",
    "- Building a flexible network architecture system\n",
    "- Training on real datasets (XOR, digits classification)\n",
    "- Object-oriented programming for deep learning\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Modular design makes neural networks easier to understand and extend\n",
    "- Each component (layer, activation, loss) has clear responsibilities\n",
    "- Proper abstraction enables code reuse\n",
    "- Understanding internals helps debug and optimize models\n",
    "- Modern frameworks (PyTorch, TensorFlow) follow similar patterns\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete Exercise 5 on PyTorch Basics\n",
    "- Review [Lesson 4: Training Neural Networks](https://jumpingsphinx.github.io/module4-neural-networks/04-training/)\n",
    "- Try implementing more advanced features (dropout, batch norm, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
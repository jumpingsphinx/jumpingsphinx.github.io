{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 4 - Exercise 1: The Perceptron\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module4-neural-networks/exercise1-perceptron.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Implement a single perceptron from scratch\n- Understand activation functions (sigmoid, tanh, ReLU)\n- Train with the perceptron learning algorithm\n- Visualize decision boundaries\n- Recognize linear separability limitations\n- Understand the XOR problem\n\n## Prerequisites\n\n- Completion of Modules 1-3\n- Understanding of linear algebra and optimization\n- Familiarity with classification concepts\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_iris, make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron as SklearnPerceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Activation Functions\n",
    "\n",
    "### Background\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks. The perceptron uses:\n",
    "\n",
    "$$\\hat{y} = f(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "Common activation functions:\n",
    "- **Step**: $f(z) = 1$ if $z \\geq 0$ else $0$\n",
    "- **Sigmoid**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "- **Tanh**: $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "- **ReLU**: $\\text{ReLU}(z) = \\max(0, z)$\n",
    "\n",
    "### Exercise 1.1: Implement Activation Functions\n",
    "\n",
    "**Task:** Complete the activation functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_function(z):\n",
    "    \"\"\"\n",
    "    Step activation function.\n",
    "    \n",
    "    Returns 1 if z >= 0, else 0\n",
    "    \"\"\"\n",
    "    if z >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    \u03c3(z) = 1 / (1 + e^(-z))\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function.\n",
    "    \"\"\"\n",
    "    return np.tanh(z)\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit) activation function.\n",
    "    \n",
    "    ReLU(z) = max(0, z)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "# Test activations\n",
    "z_test = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Input:\", z_test)\n",
    "print(\"Step:\", step_function(z_test))\n",
    "print(\"Sigmoid:\", sigmoid(z_test))\n",
    "print(\"Tanh:\", tanh(z_test))\n",
    "print(\"ReLU:\", relu(z_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Visualize Activation Functions\n",
    "\n",
    "**Task:** Create a visualization comparing all activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input range\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "axes[0, 1].plot(z, sigmoid(z), 'r-', linewidth=2)\n",
    "axes[0, 1].set_title('Sigmoid Function', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('z')\n",
    "axes[0, 1].set_ylabel('\u03c3(z)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[0, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[1, 0].plot(z, tanh(z), 'g-', linewidth=2)\n",
    "axes[1, 0].set_title('Tanh Function', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('z')\n",
    "axes[1, 0].set_ylabel('tanh(z)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1, 0].axvline(x=0, color='k', linewidth=0.5)\n",
    "axes[1, 1].plot(z, relu(z), 'm-', linewidth=2)\n",
    "axes[1, 1].set_title('ReLU Function', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('z')\n",
    "axes[1, 1].set_ylabel('ReLU(z)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].axhline(y=0, color='k', linewidth=0.5)\n",
    "axes[1, 1].axvline(x=0, color='k', linewidth=0.5)\n",
    "        # 1. Compute z = X @ weights + bias\n",
    "        z = np.dot(X, self.weights) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implement Perceptron Class\n",
    "\n",
    "### Background\n",
    "\n",
    "A perceptron computes:\n",
    "1. Linear combination: $z = \\mathbf{w}^T \\mathbf{x} + b$\n",
    "2. Apply activation: $\\hat{y} = f(z)$\n",
    "\n",
    "Training uses the perceptron learning rule:\n",
    "- $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (y - \\hat{y}) \\mathbf{x}$\n",
    "- $b \\leftarrow b + \\eta (y - \\hat{y})$\n",
    "\n",
    "### Exercise 2.1: Complete the Perceptron Class\n",
    "\n",
    "**Task:** Implement the missing methods in the Perceptron class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Perceptron classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_inputs : int\n",
    "        Number of input features\n",
    "    activation : str\n",
    "        Activation function ('step', 'sigmoid', 'tanh', 'relu')\n",
    "    learning_rate : float\n",
    "        Learning rate for weight updates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, activation='sigmoid', learning_rate=0.01):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights = np.random.randn(n_inputs) * 0.01\n",
    "        self.bias = 0.0\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Set activation function\n",
    "        self.activation_name = activation\n",
    "        if activation == 'step':\n",
    "            self.activation = step_function\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = relu\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for input data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : array, shape (n_samples,)\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "            # weights: w = w + learning_rate * X^T @ errors\n",
    "            # Need to handle dimensions carefully for single sample vs batch\n",
    "            # But Perceptron usually updates sample by sample or batch\n",
    "            # efficient vectorization:\n",
    "            update = self.learning_rate * np.dot(errors, X)\n",
    "            self.weights += update\n",
    "        return self.activation(z)\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the perceptron.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Target values (0 or 1)\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        verbose : bool\n",
    "            Print progress\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Make predictions\n",
    "            predictions = self.predict(X)\n",
    "            \n",
    "            # Compute errors\n",
    "            errors = y - predictions\n",
    "            \n",
    "            self.bias += self.learning_rate * np.sum(errors)\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate accuracy.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : features\n",
    "        y : true labels\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        accuracy : float\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        predictions = self.predict(X)\n",
    "        binary_preds = (predictions > 0.5).astype(int)\n",
    "        return np.mean(binary_preds == y)\n",
    "print(\"Perceptron class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Logic Gates - Linearly Separable Problems\n",
    "\n",
    "### Exercise 3.1: Learn the AND Gate\n",
    "\n",
    "**Task:** Train a perceptron to learn the AND logic gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND gate truth table\n",
    "X_and = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "\n",
    "print(\"AND Gate Truth Table:\")\n",
    "print(\"x1  x2  | AND\")\n",
    "print(\"-\" * 15)\n",
    "for i in range(len(X_and)):\n",
    "    print(f\"{X_and[i, 0]}   {X_and[i, 1]}   | {y_and[i]}\")\n",
    "\n",
    "# Your code here: Create and train perceptron\n",
    "perceptron_and = Perceptron(n_inputs=2, activation='sigmoid', learning_rate=0.1)\n",
    "perceptron_and.fit(X_and, y_and, epochs=100)\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nFinal Predictions:\")\n",
    "predictions = perceptron_and.predict(X_and)\n",
    "for i in range(len(X_and)):\n",
    "    print(f\"Input: {X_and[i]}, True: {y_and[i]}, Predicted: {predictions[i]:.4f}, Class: {int(predictions[i] > 0.5)}\")\n",
    "\n",
    "accuracy = perceptron_and.score(X_and, y_and)\n",
    "print(f\"\\nFinal Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "assert accuracy == 1.0, \"Perceptron should learn AND gate perfectly\"\n",
    "print(\"\u2713 AND gate learned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Learn the OR Gate\n",
    "\n",
    "**Task:** Train a perceptron to learn the OR logic gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Create OR gate dataset\n",
    "X_or = np.array([[0, 0],\n",
    "                 [0, 1],\n",
    "                 [1, 0],\n",
    "                 [1, 1]])\n",
    "y_or = # Your code here\n",
    "\n",
    "# Train perceptron\n",
    "# Your code here\n",
    "\n",
    "# Test and evaluate\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Visualize Decision Boundaries\n",
    "\n",
    "**Task:** Create a function to visualize the perceptron's decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(perceptron, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for 2D data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    perceptron : Perceptron\n",
    "        Trained perceptron\n",
    "    X : array, shape (n_samples, 2)\n",
    "        Input data\n",
    "    y : array, shape (n_samples,)\n",
    "        Labels\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    # Create mesh\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Your code here: Make predictions on mesh\n",
    "    Z = perceptron.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.8)\n",
    "    plt.colorbar(label='Prediction')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=100,\n",
    "                edgecolors='k', label='Class 0', marker='o')\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', s=100,\n",
    "                edgecolors='k', label='Class 1', marker='s')\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    plt.xlabel('$x_1$', fontsize=12)\n",
    "    plt.ylabel('$x_2$', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize AND gate\n",
    "plot_decision_boundary(perceptron_and, X_and, y_and, \"Perceptron Decision Boundary: AND Gate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: The XOR Problem - Non-Linearly Separable\n",
    "\n",
    "### Exercise 4.1: Attempt to Learn XOR\n",
    "\n",
    "**Task:** Try to train a perceptron on the XOR problem and observe the failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR gate truth table\n",
    "X_xor = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])  # XOR output\n",
    "\n",
    "print(\"XOR Gate Truth Table:\")\n",
    "print(\"x1  x2  | XOR\")\n",
    "print(\"-\" * 15)\n",
    "for i in range(len(X_xor)):\n",
    "    print(f\"{X_xor[i, 0]}   {X_xor[i, 1]}   | {y_xor[i]}\")\n",
    "\n",
    "# Visualize XOR problem first\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_xor[y_xor==0, 0], X_xor[y_xor==0, 1], c='blue', s=200,\n",
    "            edgecolors='k', label='Class 0', marker='o')\n",
    "plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='red', s=200,\n",
    "            edgecolors='k', label='Class 1', marker='s')\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('XOR Problem: Not Linearly Separable!', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCan you draw a single straight line to separate blue from red?\")\n",
    "print(\"NO! This is why a single perceptron fails on XOR.\\n\")\n",
    "\n",
    "# Train perceptron on XOR (it will fail!)\n",
    "perceptron_xor = Perceptron(n_inputs=2, activation='sigmoid', learning_rate=0.1)\n",
    "perceptron_xor.fit(X_xor, y_xor, epochs=200)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_xor = perceptron_xor.score(X_xor, y_xor)\n",
    "print(f\"\\nFinal Accuracy on XOR: {accuracy_xor:.2%}\")\n",
    "print(\"Notice: The perceptron cannot achieve 100% accuracy!\")\n",
    "\n",
    "# Visualize the failed attempt\n",
    "plot_decision_boundary(perceptron_xor, X_xor, y_xor, \"Perceptron Fails on XOR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Real Dataset - Iris Classification\n",
    "\n",
    "### Exercise 5.1: Binary Classification on Iris\n",
    "\n",
    "**Task:** Apply perceptron to classify two species of Iris flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset (use only 2 classes for binary classification)\n",
    "iris = load_iris()\n",
    "mask = iris.target != 2  # Remove third class\n",
    "X_iris = iris.data[mask, :2]  # Use only first 2 features\n",
    "y_iris = iris.target[mask]\n",
    "\n",
    "print(\"Iris Dataset (Binary Classification):\")\n",
    "print(f\"Samples: {len(X_iris)}\")\n",
    "print(f\"Features: {iris.feature_names[:2]}\")\n",
    "print(f\"Classes: Setosa (0) vs Versicolor (1)\\n\")\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Feature scaling is important!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Your code here: Train perceptron\n",
    "perceptron_iris = \n",
    "\n",
    "# Evaluate\n",
    "train_acc = perceptron_iris.score(X_train_scaled, y_train)\n",
    "test_acc = perceptron_iris.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.2%}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2%}\")\n",
    "\n",
    "# Compare with sklearn\n",
    "sklearn_perceptron = SklearnPerceptron(max_iter=100, eta0=0.1, random_state=42)\n",
    "sklearn_perceptron.fit(X_train_scaled, y_train)\n",
    "sklearn_acc = sklearn_perceptron.score(X_test_scaled, y_test)\n",
    "print(f\"\\nSklearn Perceptron Accuracy: {sklearn_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Visualize Iris Decision Boundary\n",
    "\n",
    "**Task:** Modify the plotting function to work with the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Create visualization showing decision boundary on Iris data\n",
    "# Hint: You'll need to transform the mesh grid using the scaler\n",
    "\n",
    "def plot_iris_boundary(perceptron, X, y, scaler, title):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Scale the mesh grid\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    mesh_scaled = scaler.transform(mesh_points)\n",
    "    Z = perceptron.predict(mesh_scaled)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.6)\n",
    "    plt.colorbar(label='Prediction')\n",
    "    \n",
    "    # Plot data\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=60,\n",
    "                edgecolors='k', label='Setosa', alpha=0.7)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', s=60,\n",
    "                edgecolors='k', label='Versicolor', alpha=0.7)\n",
    "    \n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2.5)\n",
    "    \n",
    "    plt.xlabel(iris.feature_names[0], fontsize=12)\n",
    "    plt.ylabel(iris.feature_names[1], fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_iris_boundary(perceptron_iris, X_train, y_train, scaler, \n",
    "                   \"Perceptron: Iris Classification (Setosa vs Versicolor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Limitations of the Perceptron\n",
    "\n",
    "### Exercise 6.1: Non-Linearly Separable Datasets\n",
    "\n",
    "**Task:** Test the perceptron on datasets that are not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-linearly separable datasets\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
    "\n",
    "# Visualize datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='blue', alpha=0.7)\n",
    "axes[0].scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='red', alpha=0.7)\n",
    "axes[0].set_title('Moons Dataset (Non-linearly Separable)', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], c='blue', alpha=0.7)\n",
    "axes[1].scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], c='red', alpha=0.7)\n",
    "axes[1].set_title('Circles Dataset (Non-linearly Separable)', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Your code here: Train perceptron on both datasets and report accuracy\n",
    "# The perceptron should perform poorly on both\n",
    "\n",
    "# Moons\n",
    "perceptron_moons = \n",
    "accuracy_moons = \n",
    "\n",
    "# Circles\n",
    "perceptron_circles = \n",
    "accuracy_circles = \n",
    "\n",
    "print(f\"Moons Accuracy: {accuracy_moons:.2%}\")\n",
    "print(f\"Circles Accuracy: {accuracy_circles:.2%}\")\n",
    "print(\"\\nNote: Single perceptrons struggle with non-linearly separable data!\")\n",
    "print(\"Solution: Multi-layer neural networks (next lesson)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Implement Derivatives\n",
    "\n",
    "Implement the derivatives of activation functions (needed for backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid: \u03c3'(z) = \u03c3(z)(1 - \u03c3(z))\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    s = sigmoid(z)\n",
    "    return \n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of tanh: tanh'(z) = 1 - tanh\u00b2(z)\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return \n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU: 1 if z > 0, else 0\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    return \n",
    "\n",
    "# Test\n",
    "z_test = np.linspace(-3, 3, 100)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(z_test, sigmoid_derivative(z_test), linewidth=2)\n",
    "plt.title(\"Sigmoid Derivative\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(z_test, tanh_derivative(z_test), linewidth=2)\n",
    "plt.title(\"Tanh Derivative\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(z_test, relu_derivative(z_test), linewidth=2)\n",
    "plt.title(\"ReLU Derivative\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Multi-Class Perceptron\n",
    "\n",
    "Extend the perceptron to handle more than 2 classes using one-vs-rest strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassPerceptron:\n",
    "    \"\"\"\n",
    "    Multi-class perceptron using one-vs-rest strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "        self.perceptrons = []\n",
    "    \n",
    "    def fit(self, X, y, epochs=100):\n",
    "        # Your code here\n",
    "        # Train one perceptron for each class\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Your code here\n",
    "        # Choose class with highest confidence\n",
    "        pass\n",
    "\n",
    "print(\"Challenge: Implement multi-class perceptron!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Implement Pocket Algorithm\n",
    "\n",
    "The Pocket algorithm improves the perceptron by keeping track of the best weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PocketPerceptron(Perceptron):\n",
    "    \"\"\"\n",
    "    Pocket Perceptron: keeps best weights seen during training.\n",
    "    Useful for non-separable data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit(self, X, y, epochs=100, verbose=True):\n",
    "        # Your code here\n",
    "        # Track best weights and their accuracy\n",
    "        # Update pocket when current weights are better\n",
    "        pass\n",
    "\n",
    "print(\"Challenge: Implement pocket algorithm!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Why does the perceptron fail on XOR?**\n",
    "   - Think about linear separability\n",
    "\n",
    "2. **When would you use sigmoid vs ReLU activation?**\n",
    "   - Consider output interpretation and gradient flow\n",
    "\n",
    "3. **Why is feature scaling important for perceptrons?**\n",
    "   - How do different feature scales affect the dot product?\n",
    "\n",
    "4. **What does the perceptron learning rule do geometrically?**\n",
    "   - How does it adjust the decision boundary?\n",
    "\n",
    "5. **How is the perceptron related to logistic regression?**\n",
    "   - Compare the update rules and loss functions\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "- How to implement a perceptron from scratch\n",
    "- Different activation functions and their properties\n",
    "- The perceptron learning algorithm\n",
    "- How to visualize decision boundaries\n",
    "- The limitation of perceptrons: cannot learn non-linearly separable patterns\n",
    "- Why we need multi-layer networks (next lesson!)\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Perceptron = linear classifier with activation function\n",
    "- Can learn AND, OR but not XOR\n",
    "- Decision boundary is a hyperplane\n",
    "- Foundation for understanding neural networks\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete Exercise 2 on Feedforward Networks\n",
    "- Review [Lesson 1: The Perceptron](https://jumpingsphinx.github.io/module4-neural-networks/01-perceptron/)\n",
    "- Experiment with different activation functions and learning rates\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Module 4 - Exercise 2: Feedforward Neural Networks\n\n<a href=\"https://colab.research.google.com/github/jumpingsphinx/jumpingsphinx.github.io/blob/main/notebooks/module4-neural-networks/exercise2-feedforward-networks.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n\n## Learning Objectives\n\nBy the end of this exercise, you will be able to:\n\n- Build multi-layer feedforward networks\n- Understand forward propagation through layers\n- Implement different activation functions\n- Solve non-linearly separable problems (like XOR)\n- Visualize network architectures and activations\n- Initialize weights properly\n\n## Prerequisites\n\n- Completion of Exercise 1 (Perceptron)\n- Understanding of matrix operations\n- Familiarity with activation functions\n\n## Setup\n\nRun this cell first to import required libraries:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Building a 2-Layer Neural Network\n",
    "\n",
    "### Background\n",
    "\n",
    "A 2-layer neural network (1 hidden layer) consists of:\n",
    "\n",
    "**Layer 1 (Input \u2192 Hidden)**:\n",
    "$$\\mathbf{z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b}^{[1]}$$\n",
    "$$\\mathbf{a}^{[1]} = f^{[1]}(\\mathbf{z}^{[1]})$$\n",
    "\n",
    "**Layer 2 (Hidden \u2192 Output)**:\n",
    "$$\\mathbf{z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}$$\n",
    "$$\\mathbf{a}^{[2]} = f^{[2]}(\\mathbf{z}^{[2]})$$\n",
    "\n",
    "### Exercise 1.1: Implement a 2-Layer Network\n",
    "\n",
    "**Task:** Complete the NeuralNetwork class with forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"Tanh activation function.\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    2-layer neural network for binary classification.\n",
    "    \n",
    "    Architecture: [n_input, n_hidden, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, hidden_activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize network parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_input : int\n",
    "            Number of input features\n",
    "        n_hidden : int\n",
    "            Number of neurons in hidden layer\n",
    "        hidden_activation : str\n",
    "            Activation function for hidden layer ('relu', 'sigmoid', 'tanh')\n",
    "        \"\"\"\n",
    "        # Initialize weights with small random values\n",
    "        self.b1 = np.zeros((n_hidden, 1))\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray, shape (n_input, m)\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        A2 : ndarray, shape (1, m)\n",
    "            Output predictions\n",
    "        cache : dict\n",
    "            Cached values for visualization\n",
    "        \"\"\"\n",
    "        # Layer 1: Input \u2192 Hidden\n",
    "        A1 = self.hidden_activation(Z1)\n",
    "        Z2 = np.dot(self.W2, A1) + self.b2\n",
    "        return A2, cache\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make binary predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray\n",
    "            Input data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : ndarray\n",
    "            Binary predictions (0 or 1)\n",
    "        \"\"\"\n",
    "        A2, _ = self.forward(X)\n",
    "        return (A2 > 0.5).astype(int)\n",
    "# Test the network\n",
    "print(\"Testing 2-Layer Neural Network\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "nn = NeuralNetwork(n_input=2, n_hidden=4, hidden_activation='relu')\n",
    "\n",
    "# Test with random data\n",
    "X_test = np.random.randn(2, 5)\n",
    "A2, cache = nn.forward(X_test)\n",
    "\n",
    "print(f\"Input shape: {X_test.shape}\")\n",
    "print(f\"Hidden activations shape: {cache['A1'].shape}\")\n",
    "print(f\"Output shape: {A2.shape}\")\n",
    "print(f\"\\nOutput probabilities:\\n{A2}\")\n",
    "print(f\"\\nBinary predictions:\\n{nn.predict(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Solving XOR with a 2-Layer Network\n",
    "\n",
    "### Background\n",
    "\n",
    "Recall that a single perceptron cannot solve XOR. A 2-layer network can!\n",
    "\n",
    "The key insight: the hidden layer learns a new representation where XOR becomes linearly separable.\n",
    "\n",
    "### Exercise 2.1: Manually Set Weights for XOR\n",
    "\n",
    "**Task:** Set weights manually to solve XOR (to understand how it works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0, 1, 1],\n",
    "                  [0, 1, 0, 1]])\n",
    "y_xor = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "print(\"XOR Truth Table:\")\n",
    "print(\"x1  x2  | XOR\")\n",
    "print(\"-\" * 15)\n",
    "for i in range(4):\n",
    "    print(f\"{X_xor[0, i]}   {X_xor[1, i]}   | {y_xor[0, i]}\")\n",
    "\n",
    "# Visualize XOR problem\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_xor[0, y_xor[0]==0], X_xor[1, y_xor[0]==0], \n",
    "           c='blue', s=200, edgecolors='k', marker='o', label='Class 0')\n",
    "plt.scatter(X_xor[0, y_xor[0]==1], X_xor[1, y_xor[0]==1], \n",
    "           c='red', s=200, edgecolors='k', marker='s', label='Class 1')\n",
    "plt.xlabel('$x_1$', fontsize=12)\n",
    "plt.ylabel('$x_2$', fontsize=12)\n",
    "plt.title('XOR Problem: Not Linearly Separable!', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.show()\n",
    "\n",
    "# Create network and manually set weights\n",
    "nn_xor = NeuralNetwork(n_input=2, n_hidden=2, hidden_activation='relu')\n",
    "\n",
    "# Manually set weights to implement XOR\n",
    "# Hidden layer neurons: h1 \u2248 AND(x1, x2), h2 \u2248 OR(x1, x2)\n",
    "# Output: y \u2248 AND(NOT h1, h2) = XOR(x1, x2)\n",
    "nn_xor.W1 = np.array([[20, 20],   # First hidden neuron (AND-like)\n",
    "                      [20, 20]])   # Second hidden neuron (OR-like)\n",
    "nn_xor.b1 = np.array([[-30],       # High threshold for AND\n",
    "                      [-10]])      # Low threshold for OR\n",
    "\n",
    "nn_xor.W2 = np.array([[-20, 20]])  # Negative first, positive second\n",
    "nn_xor.b2 = np.array([[-10]])\n",
    "\n",
    "# Test the network\n",
    "predictions, cache = nn_xor.forward(X_xor)\n",
    "\n",
    "print(\"\\nXOR Solution with Manual Weights:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'x1':<5} {'x2':<5} {'True':<8} {'Predicted':<12} {'Correct'}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor[:, i]\n",
    "    true_y = y_xor[0, i]\n",
    "    pred_y = predictions[0, i]\n",
    "    correct = '\u2713' if (pred_y > 0.5) == true_y else '\u2717'\n",
    "    print(f\"{x1:<5.0f} {x2:<5.0f} {true_y:<8.0f} {pred_y:<12.4f} {correct}\")\n",
    "\n",
    "accuracy = np.mean((predictions > 0.5) == y_xor)\n",
    "print(f\"\\nAccuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Show hidden layer transformations\n",
    "print(\"\\nHidden Layer Activations:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"(Shows how the network transforms the input)\")\n",
    "print(f\"{'x1':<5} {'x2':<5} {'h1 (AND-like)':<15} {'h2 (OR-like)'}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor[:, i]\n",
    "    h1, h2 = cache['A1'][:, i]\n",
    "    print(f\"{x1:<5.0f} {x2:<5.0f} {h1:<15.4f} {h2:<15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Visualize Hidden Layer Transformation\n",
    "\n",
    "**Task:** Visualize how the hidden layer makes XOR linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hidden layer activations\n",
    "predictions, cache = nn_xor.forward(X_xor)\n",
    "H = cache['A1']  # Hidden activations (2 x 4)\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original space (not linearly separable)\n",
    "ax1.scatter(X_xor[0, y_xor[0]==0], X_xor[1, y_xor[0]==0],\n",
    "           c='blue', s=200, edgecolors='k', marker='o', label='Class 0')\n",
    "ax1.scatter(X_xor[0, y_xor[0]==1], X_xor[1, y_xor[0]==1],\n",
    "           c='red', s=200, edgecolors='k', marker='s', label='Class 1')\n",
    "ax1.set_xlabel('$x_1$', fontsize=14)\n",
    "ax1.set_ylabel('$x_2$', fontsize=14)\n",
    "ax1.set_title('Original Space\\n(Not Linearly Separable)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(-0.5, 1.5)\n",
    "ax1.set_ylim(-0.5, 1.5)\n",
    "\n",
    "# Hidden space (linearly separable!)\n",
    "ax2.scatter(H[0, y_xor[0]==0], H[1, y_xor[0]==0],\n",
    "           c='blue', s=200, edgecolors='k', marker='o', label='Class 0')\n",
    "ax2.scatter(H[0, y_xor[0]==1], H[1, y_xor[0]==1],\n",
    "           c='red', s=200, edgecolors='k', marker='s', label='Class 1')\n",
    "\n",
    "# Draw decision boundary in hidden space\n",
    "h_vals = np.linspace(-0.5, 1.5, 100)\n",
    "# W2[0] * h1 + W2[1] * h2 + b2 = 0\n",
    "# h2 = -(W2[0] * h1 + b2) / W2[1]\n",
    "boundary_h2 = -(nn_xor.W2[0, 0] * h_vals + nn_xor.b2[0, 0]) / nn_xor.W2[0, 1]\n",
    "ax2.plot(h_vals, boundary_h2, 'k-', linewidth=2, label='Decision Boundary')\n",
    "\n",
    "ax2.set_xlabel('$h_1$ (AND-like)', fontsize=14)\n",
    "ax2.set_ylabel('$h_2$ (OR-like)', fontsize=14)\n",
    "ax2.set_title('Hidden Layer Space\\n(Linearly Separable!)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(-0.5, 1.5)\n",
    "ax2.set_ylim(-0.5, 1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The Magic of Hidden Layers:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"In the original space (left), XOR is not linearly separable.\")\n",
    "print(\"But in the hidden layer space (right), a simple line separates the classes!\")\n",
    "print(\"\\nThis is why neural networks are so powerful:\")\n",
    "print(\"Hidden layers learn representations that make problems easier to solve.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Network Architecture - Depth vs Width\n",
    "\n",
    "### Exercise 3.1: Experiment with Network Width\n",
    "\n",
    "**Task:** Test different numbers of hidden neurons on XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Create networks with different hidden layer sizes\n",
    "# Test sizes: 2, 4, 8, 16 neurons\n",
    "# For each, count the total number of parameters\n",
    "\n",
    "hidden_sizes = [2, 4, 8, 16]\n",
    "\n",
    "print(\"Network Width Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Hidden Size':<15} {'Parameters':<15} {'Architecture'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n_hidden in hidden_sizes:\n",
    "    nn = NeuralNetwork(n_input=2, n_hidden=n_hidden)\n",
    "    \n",
    "    # Count parameters\n",
    "    # Your code here\n",
    "    params_layer1 = n_hidden * 2 + n_hidden  # W1 + b1\n",
    "    params_layer2 = \n",
    "    total_params = params_layer1 + params_layer2\n",
    "    \n",
    "    print(f\"{n_hidden:<15} {total_params:<15} [2, {n_hidden}, 1]\")\n",
    "\n",
    "print(\"\\nObservation: More neurons = more parameters = more capacity\")\n",
    "print(\"But: Also more risk of overfitting and slower training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Build a 3-Layer Network (2 Hidden Layers)\n",
    "\n",
    "**Task:** Extend the network to have 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNetwork:\n",
    "    \"\"\"\n",
    "    3-layer neural network (2 hidden layers) for binary classification.\n",
    "    \n",
    "    Architecture: [n_input, n_hidden1, n_hidden2, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2):\n",
    "        # Your code here: Initialize 3 layers of weights and biases\n",
    "        # Layer 1: input -> hidden1\n",
    "        self.W1 = np.random.randn(n_hidden1, n_input) * 0.01\n",
    "        self.b1 = np.zeros((n_hidden1, 1))\n",
    "        \n",
    "        # Layer 2: hidden1 -> hidden2\n",
    "        # Your code here\n",
    "        self.W2 = \n",
    "        self.b2 = \n",
    "        \n",
    "        # Layer 3: hidden2 -> output\n",
    "        # Your code here\n",
    "        self.W3 = \n",
    "        self.b3 = \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through 3 layers.\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        Z1 = np.dot(self.W1, X) + self.b1\n",
    "        A1 = relu(Z1)\n",
    "        \n",
    "        # Layer 2\n",
    "        # Your code here\n",
    "        Z2 = \n",
    "        A2 = relu(Z2)\n",
    "        \n",
    "        # Layer 3\n",
    "        # Your code here\n",
    "        Z3 = \n",
    "        A3 = sigmoid(Z3)\n",
    "        \n",
    "        cache = {'A1': A1, 'A2': A2, 'A3': A3}\n",
    "        return A3, cache\n",
    "    \n",
    "    def predict(self, X):\n",
    "        A3, _ = self.forward(X)\n",
    "        return (A3 > 0.5).astype(int)\n",
    "\n",
    "# Test the deep network\n",
    "print(\"Testing 3-Layer Neural Network\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "deep_nn = DeepNeuralNetwork(n_input=2, n_hidden1=4, n_hidden2=3)\n",
    "\n",
    "# Test on XOR\n",
    "predictions, cache = deep_nn.forward(X_xor)\n",
    "\n",
    "print(f\"Architecture: [2, 4, 3, 1]\")\n",
    "print(f\"Hidden layer 1 activations shape: {cache['A1'].shape}\")\n",
    "print(f\"Hidden layer 2 activations shape: {cache['A2'].shape}\")\n",
    "print(f\"Output shape: {predictions.shape}\")\n",
    "print(f\"\\nPredictions (untrained):\\n{predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Testing on Non-Linearly Separable Datasets\n",
    "\n",
    "### Exercise 4.1: Moons Dataset\n",
    "\n",
    "**Task:** Test the network on the moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moons dataset\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "X_moons = X_moons.T  # Shape: (2, 200)\n",
    "y_moons = y_moons.reshape(1, -1)  # Shape: (1, 200)\n",
    "\n",
    "# Visualize dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_moons[0, y_moons[0]==0], X_moons[1, y_moons[0]==0],\n",
    "           c='blue', alpha=0.7, edgecolors='k', label='Class 0')\n",
    "plt.scatter(X_moons[0, y_moons[0]==1], X_moons[1, y_moons[0]==1],\n",
    "           c='red', alpha=0.7, edgecolors='k', label='Class 1')\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Moons Dataset (Non-linearly Separable)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Shape: {X_moons.shape}\")\n",
    "print(f\"Labels: {np.unique(y_moons)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_moons[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Circles Dataset\n",
    "\n",
    "**Task:** Test on the circles dataset (even harder!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Generate and visualize circles dataset\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.5, random_state=42)\n",
    "X_circles = X_circles.T\n",
    "y_circles = y_circles.reshape(1, -1)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Your code here\n",
    "\n",
    "print(\"Circles Dataset - Even More Challenging!\")\n",
    "print(\"Requires more complex decision boundaries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Comparing with sklearn's MLPClassifier\n",
    "\n",
    "### Exercise 5.1: Train MLPClassifier on XOR\n",
    "\n",
    "**Task:** Use sklearn's MLP to solve XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Prepare data for sklearn (needs transposed format)\n",
    "X_xor_sklearn = X_xor.T  # Shape: (4, 2)\n",
    "y_xor_sklearn = y_xor.ravel()  # Shape: (4,)\n",
    "\n",
    "# Create MLP classifier\n",
    "# Your code here: Create MLPClassifier with architecture (4,) - one hidden layer with 4 neurons\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(4,), \n",
    "                   activation='relu',\n",
    "                   max_iter=2000,\n",
    "                   learning_rate_init=0.1,\n",
    "                   random_state=42)\n",
    "\n",
    "# Train\n",
    "mlp.fit(X_xor_sklearn, y_xor_sklearn)\n",
    "\n",
    "# Predict\n",
    "predictions_sklearn = mlp.predict(X_xor_sklearn)\n",
    "\n",
    "print(\"sklearn MLPClassifier on XOR:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'x1':<5} {'x2':<5} {'True':<8} {'Predicted':<12} {'Correct'}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(4):\n",
    "    x1, x2 = X_xor_sklearn[i]\n",
    "    true_y = y_xor_sklearn[i]\n",
    "    pred_y = predictions_sklearn[i]\n",
    "    correct = '\u2713' if pred_y == true_y else '\u2717'\n",
    "    print(f\"{x1:<5.0f} {x2:<5.0f} {true_y:<8.0f} {pred_y:<12.0f} {correct}\")\n",
    "\n",
    "accuracy_sklearn = accuracy_score(y_xor_sklearn, predictions_sklearn)\n",
    "print(f\"\\nAccuracy: {accuracy_sklearn:.2%}\")\n",
    "\n",
    "print(f\"\\nNetwork architecture: {[2] + list(mlp.hidden_layer_sizes) + [1]}\")\n",
    "print(f\"Number of iterations: {mlp.n_iter_}\")\n",
    "print(f\"Loss: {mlp.loss_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2: Compare on Moons Dataset\n",
    "\n",
    "**Task:** Train and compare different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_moons_sklearn = X_moons.T\n",
    "y_moons_sklearn = y_moons.ravel()\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_moons_sklearn, y_moons_sklearn, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Test different architectures\n",
    "architectures = [\n",
    "    (4,),\n",
    "    (8,),\n",
    "    (16,),\n",
    "    (8, 4),\n",
    "    (16, 8)\n",
    "]\n",
    "\n",
    "print(\"Comparing MLP Architectures on Moons Dataset:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Architecture':<20} {'Train Acc':<15} {'Test Acc':<15} {'Iterations'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for arch in architectures:\n",
    "    # Your code here: Train MLP with this architecture\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=arch,\n",
    "                       activation='relu',\n",
    "                       max_iter=1000,\n",
    "                       random_state=42)\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = mlp.score(X_train, y_train)\n",
    "    test_acc = mlp.score(X_test, y_test)\n",
    "    \n",
    "    arch_str = str([2] + list(arch) + [1])\n",
    "    print(f\"{arch_str:<20} {train_acc:<15.4f} {test_acc:<15.4f} {mlp.n_iter_}\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"  \u2022 Deeper/wider networks may achieve higher training accuracy\")\n",
    "print(\"  \u2022 But watch out for overfitting (train acc >> test acc)\")\n",
    "print(\"  \u2022 Balance between capacity and generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Visualizing Decision Boundaries\n",
    "\n",
    "### Exercise 6.1: Plot Decision Boundary\n",
    "\n",
    "**Task:** Visualize what the network learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_mlp(mlp, X, y, title):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for sklearn MLP.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mlp : MLPClassifier\n",
    "        Trained MLP\n",
    "    X : ndarray, shape (m, 2)\n",
    "        Input data\n",
    "    y : ndarray, shape (m,)\n",
    "        Labels\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    # Create mesh\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdYlBu', alpha=0.6)\n",
    "    plt.colorbar(label='Prediction')\n",
    "    \n",
    "    # Plot data\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', s=60,\n",
    "               edgecolors='k', label='Class 0', alpha=0.7)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='red', s=60,\n",
    "               edgecolors='k', label='Class 1', alpha=0.7)\n",
    "    \n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2.5)\n",
    "    \n",
    "    plt.xlabel('Feature 1', fontsize=12)\n",
    "    plt.ylabel('Feature 2', fontsize=12)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Train on moons and visualize\n",
    "mlp_moons = MLPClassifier(hidden_layer_sizes=(8, 4), \n",
    "                         activation='relu',\n",
    "                         max_iter=1000,\n",
    "                         random_state=42)\n",
    "mlp_moons.fit(X_moons_sklearn, y_moons_sklearn)\n",
    "\n",
    "plot_decision_boundary_mlp(mlp_moons, X_moons_sklearn, y_moons_sklearn,\n",
    "                          \"MLP Decision Boundary: Moons Dataset\")\n",
    "\n",
    "print(f\"Test accuracy: {mlp_moons.score(X_moons_sklearn, y_moons_sklearn):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challenge Problems (Optional)\n",
    "\n",
    "### Challenge 1: Implement 4-Layer Network\n",
    "\n",
    "Extend the network to have 3 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Build a 4-layer network\n",
    "# Architecture: [n_input, n_hidden1, n_hidden2, n_hidden3, 1]\n",
    "\n",
    "print(\"Challenge: Implement 4-layer neural network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Activation Function Comparison\n",
    "\n",
    "Compare ReLU, Sigmoid, and Tanh activations on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Train MLPs with different activation functions\n",
    "# Compare performance\n",
    "\n",
    "activations = ['relu', 'logistic', 'tanh']\n",
    "\n",
    "# Test each activation on moons dataset\n",
    "# Report train/test accuracy for each\n",
    "\n",
    "print(\"Challenge: Compare activation functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Universal Approximation\n",
    "\n",
    "Approximate a complex function with a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Create a complex 1D function\n",
    "# Use MLP to approximate it\n",
    "# Plot original vs approximation\n",
    "\n",
    "def complex_function(x):\n",
    "    return np.sin(x) + 0.5 * np.cos(3*x) + 0.2 * np.sin(5*x)\n",
    "\n",
    "# Generate data\n",
    "X_func = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "y_func = complex_function(X_func.ravel())\n",
    "\n",
    "# Train MLP regressor to approximate this function\n",
    "# Compare with different network sizes\n",
    "\n",
    "print(\"Challenge: Universal approximation theorem in action!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection Questions\n",
    "\n",
    "1. **Why do we need hidden layers?**\n",
    "   - Think about what problems can't be solved with a single layer\n",
    "\n",
    "2. **What's the difference between network depth (layers) and width (neurons)?**\n",
    "   - When would you increase depth vs width?\n",
    "\n",
    "3. **How does the hidden layer transform the input space?**\n",
    "   - What did you observe in the XOR visualization?\n",
    "\n",
    "4. **Why does the network need non-linear activation functions?**\n",
    "   - What would happen with only linear activations?\n",
    "\n",
    "5. **How do you choose the network architecture?**\n",
    "   - What factors influence the number of layers and neurons?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "- How to build multi-layer perceptrons (feedforward networks)\n",
    "- Why hidden layers are crucial for solving non-linear problems\n",
    "- How hidden layers transform the input space\n",
    "- The XOR problem and its solution with 2-layer networks\n",
    "- Network architecture design (depth vs width)\n",
    "- How to use sklearn's MLPClassifier\n",
    "- Visualizing decision boundaries\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Single perceptrons are limited to linear boundaries\n",
    "- Hidden layers learn new representations of the data\n",
    "- Non-linear activations are essential\n",
    "- Deeper/wider networks have more capacity but risk overfitting\n",
    "- Neural networks can approximate any continuous function (Universal Approximation Theorem)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Complete Exercise 3 on Backpropagation\n",
    "- Review [Lesson 2: Feedforward Networks](https://jumpingsphinx.github.io/module4-neural-networks/02-feedforward-networks/)\n",
    "- Experiment with different architectures on real datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Need help?** Check the solution notebook or open an issue on [GitHub](https://github.com/jumpingsphinx/jumpingsphinx.github.io/issues)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}